<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
    <meta charset="utf-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="keywords" content="机器学习, 有监督学习, 线性回归, 局部加权回归, 概率解释, ">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content=""/>
<meta property="og:title" content="机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释 : nanshu.wang"/>
<meta property="og:site_name" content="nanshu wang blog"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="http://nanshu.wang/post/2015-02-11">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2015-02-11"/>
<meta property="article:modified_time" content="2015-02-11"/>



<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="有监督学习">
<meta property="article:tag" content="线性回归">
<meta property="article:tag" content="局部加权回归">
<meta property="article:tag" content="概率解释">





    <base href="http://nanshu.wang">
    <title> 机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释 - nanshu.wang </title>
    <link rel="canonical" href="http://nanshu.wang/post/2015-02-11">
    

    
<link rel="stylesheet" href="/static/css/style.css">

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
    <script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?a79535ca63291dc820e3ecefa615aad1";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

</head>

<body lang="en" itemscope itemtype="http://schema.org/Article">
<header id="header">
	<nav id="nav">
	<div id="title"><a href="/">Nanshu Wang</a></div>
    <div><a href=mailto: nanshu.wang@gmail.com target="_blank" class="mailto"> </span> <span class="icon-mail"></span>gardenia22</a></div>
	</nav>
    <nav id="nav">
    	        <ul id="mainnav">
            <li>
                <a href="/蘅芜/">
                <span class="icon"> <i aria-hidden="true" class="icon-pencil"></i></span>
                <span> 蘅芜 </span>
            </a>
            </li>
            <li>
            <a href="/潇湘/">
                <span class="icon"> <i aria-hidden="true" class="icon-quill"></i></span>
                <span> 潇湘 </span>
            </a>
            </li>
            <li>
            <a href="/观叹/">
                <span class="icon"> <i aria-hidden="true" class="icon-leaf"></i></span>
                <span> 观叹 </span>
            </a>
            </li>
            <li>
            <a href="/about">
                <span class="icon"> <i aria-hidden="true" class="icon-heart"></i></span>
                <span> 关于 </span>
            </a>
            </li>
        </ul>

    </nav>
    <nav id="nav">
       	        <ul id="social">
            
            <li id="share">
                <span class="title"> 友链 </span>
                <div class="dropdown share">
                    <ul class="social">
                      
                      <li> <a href="http://spf13.com" target="_blank" title="spf13 is Steve Francis" class="facebook">spf13</a> </li>
                      <li> <a href="http://libaier.net" target="_blank" title="Libaier" class="rss">Libaier</a> </li>
                      <li> <a href="http://read.douban.com/column/195295/" target="_blank" title="100个故事" class="douban">一百个故事</a></li>
                      <li> <a href="http://zhangwenli.com/blog/" target="_blank" title="羡辙杂俎" class="yangzhe">羡辙杂俎</a></li>
                      <li> <a href="http://tianwang.gift/" target="_blank" title="Tian" class="tian">Tian</a></li>
                      <li> <a href="https://nathaniel.blog/" target="_blank" title="Nathaniel's blog" class="nat">Nat</a></li>
                    </ul>
                <span class="icon icon-bubbles"> </span> <span class="subcount"></span> </div>
            </li>
    
            <li id="follow">
                <span class="title"> 驻留地 </span>
                <div class="dropdown follow">
                    <ul class="social">
                        

                        <li> <a href="http://weibo.com/gardenia22/" target="_blank" title="微博" class="weibo">微博</a> </li>
                        <li> <a href="http://www.douban.com/people/gardenia22" target="_blank"
                        title="豆瓣" class="douban">豆瓣</a> </li>
                        <li> <a href="http://www.zhihu.com/people/gardenia" target="_blank" title="知乎" class="zhihu">知乎</a> </li>
                        <li> <a href="http://github.com/gardenia22" target="_blank" title="GitHub" class="github">GitHub</a> </li>                         
                        <li> <a href="http://www.facebook.com/gardenia.nanshu.wang" target="_blank" title="Facebook" class="facebook"> Facebook</a></li>
                        <li> <a href="http://instagram.com/ainedrag22" target="_blank" title="Instagram" class="instagram">Instagram</a> </li>
                                                                       
                        
                    </ul>
                <span class="icon icon-rocket"> </span> <span class="subcount"></span> </div>
            </li>

        </ul>

	</nav>
</header>



<section id="main">
  <h1 itemprop="name" >机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释</h1>
  

<aside id="meta">

    <div>
        <section id="datecount">
          <h4 id="date"> Wed Feb 11, 2015 </h4>
          
        </section>
        
        <ul id="tags">
          
            <li> <a href="http://nanshu.wang/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a> </li>
          
            <li> <a href="http://nanshu.wang/tags/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">有监督学习</a> </li>
          
            <li> <a href="http://nanshu.wang/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">线性回归</a> </li>
          
            <li> <a href="http://nanshu.wang/tags/%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92">局部加权回归</a> </li>
          
            <li> <a href="http://nanshu.wang/tags/%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A">概率解释</a> </li>
          
        </ul>
    </div>

</aside>

<meta itemprop="wordCount" content="79">
<meta itemprop="datePublished" content="2015-02-11">
<meta itemprop="url" content="http://nanshu.wang/post/2015-02-11">


  <div>
        <article itemprop="articleBody" id="content">
           

<p>Andrew Ng cs229 Machine Learning 笔记</p>

<h1 id="有监督学习:a41c550b5852f048f913986db76cdf89">有监督学习</h1>

<h2 id="局部加权线性回归-locally-weighted-linear-regression:a41c550b5852f048f913986db76cdf89">局部加权线性回归(Locally weighted linear regression)</h2>

<p>参数学习算法(parametric learning algorithm)：参数个数固定</p>

<p>非参数学习算法(non-parametric learning algorithm)：参数个数随样本增加</p>

<p>特征选择对参数学习算法非常重要，否则会出现下面的问题：</p>

<ul>
<li>欠拟合(underfitting)：特征过少，模型过于简单，高偏差(high bias)，不能很好拟合训练集</li>
<li>过拟合(overfitting)：特征过多，模型过于复杂，高方差(high variance)，过于拟合训练集，不能很好预测新样本</li>
</ul>

<p>对于非参数学习算法来说，并不需要进行精心的特征选择，局部加权线性回归就是这样。</p>

<p>局部加权回归又叫做Loess，其成本函数为：</p>

<div>
$$\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$$
</div>

<p>$w^{(i)}$是非负权重，通常定义$w^{(i)}$如下：</p>

<div>
$$w^{(i)}=exp(- \frac{(x^{(i)}-x)^2}{2\tau^2})$$
</div>

<p>权值取决于用于预测的输入变量$x$的值：离$x$越近的样本，权值越接近1；越远的样本，越接近0。$\tau$被称为带宽(bandwidth)参数，决定了以$x$为中心，样本权重递减的速度。$\tau$越大，递减速度越慢；$\tau$越小，递减速度越快。</p>

<p>参数学习算法的参数是固定的，一经学习得到不再改变。而非参数学习算法的参数并不固定，每次预测都要重新学习一组新的参数，并且要一直保留完整的训练样本集。当样本集很大时，局部加权回归的计算开销会很大，Andrew Moore提出的KD-tree方法可以在大数据集上的计算更高效。</p>

<h2 id="回归模型的概率解释:a41c550b5852f048f913986db76cdf89">回归模型的概率解释</h2>

<p>遇到一个回归问题，为什么采用线性回归，又为什么采用最小二乘作为成本函数？其实最小二乘回归中蕴含了非常自然的概率假设。假设目标值和输入值之间满足以下关系：</p>

<div>
$$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$$
</div>

<p>其中，$\epsilon^{(i)}$是误差项，包含了模型未考虑到的影响目标值的因素和随机噪声。假设误差项独立同分布，服从均值为0，方差为$\sigma^2$的高斯分布（正态分布），即$\epsilon^{(i)}\sim N(0,\sigma^2)$。$\epsilon^{(i)}$的密度函数为：</p>

<div>
$$p(\epsilon^{(i)})=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$
</div>

<p>也就是说：</p>

<div>
$$p(y^{(i)}|x^{(i)};\theta)=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
</div>

<p>或者也可以这样写：$(y^{(i)}|x^{(i)};\theta) \sim N(\theta^Tx^{(i)},\sigma^2)$。</p>

<p>为什么采用正态分布？一种原因是数学上处理起来比较便利，二是因为根据中心极限定理，独立的随机变量的和，即多种随机误差的累积，其总的影响是接近正态分布的。</p>

<p>$p(y^{(i)}|x^{(i)};\theta)$的含义是给定条件$x^{(i)}$，参数设定为$\theta$时，$y^{(i)}$的分布值。不能将$\theta$看作是概率条件，因为$\theta$不是随机变量，而是实际存在的真实值，虽然我们不知道真实值到底是多少。因此在以上的式子中，用了分号而不是逗号来区分$x^{(i)}$和$\theta$。</p>

<p>定义$\theta$的似然(likelihood)函数：</p>

<div>
$$\begin{equation}
\begin{split}
L(\theta)=& p(y|X;\theta)\\
=& \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=& \Pi_{i=1}^m \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{split}
\end{equation}$$
</div>

<p>似然(likelihood)和概率(probability)实际上是一个东西，但是似然函数是对参数$\theta$定义的，为了加以区分，使用了似然这一术语。我们可以说参数的似然，数据的概率，但不能说数据的似然，参数的概率。</p>

<p>极大似然估计的含义是选择参数$\theta$，使参数的似然函数最大化，也就是说选择参数使得已有样本数据出现的概率最大。</p>

<p>为方便求解，再定义函数$l(\theta)$：</p>

<div>
$$\begin{equation}
\begin{split}
l(\theta) =& \log L(\theta) \\
=& \sum_{i=1}^m \log \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\
=& m\log (\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2)
\end{split}
\end{equation}$$
</div>

<p>可得，要最大化似然函数$L(\theta)$，也就是最大化$l(\theta)$，也就是最小化：</p>

<div>
$$\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$$
</div>

<p>这个式子刚好是最小二乘法中定义的成本函数$J(\theta)$。总结一下，最小二乘回归模型刚好就是在假设了误差独立同服从正态分布后，得到的最大似然估计。注意到，正态分布中的方差$\sigma^2$的取值对模型并没有影响。</p>

        </article>
  </div>
</section>

<aside id=comments>
    <div><h2> Comments </h2></div>
    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'nanshuwang';
    var disqus_identifier = 'http:\/\/nanshu.wang\/post\/2015-02-11';
    var disqus_title = '机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释';
    var disqus_url = 'http:\/\/nanshu.wang\/post\/2015-02-11';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</aside>

<footer>
  <div>
    <p>
    &copy; 2015 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Nanshu Wang.</span></span>
        Powered by <a href="http://hugo.spf13.com">Hugo</a>.
        Theme by <a href="http://spf13.com">Steve Francia</a>.
    </p>
  </div>
</footer>
<script type="text/javascript">
(function(){var j=function(a,b){return window.getComputedStyle?getComputedStyle(a).getPropertyValue(b):a.currentStyle[b]};var k=function(a,b,c){if(a.addEventListener)a.addEventListener(b,c,false);else a.attachEvent('on'+b,c)};var l=function(a,b){for(key in b)if(b.hasOwnProperty(key))a[key]=b[key];return a};window.fitText=function(d,e,f){var g=l({'minFontSize':-1/0,'maxFontSize':1/0},f);var h=function(a){var b=e||1;var c=function(){a.style.fontSize=Math.max(Math.min(a.clientWidth/(b*10),parseFloat(g.maxFontSize)),parseFloat(g.minFontSize))+'px'};c();k(window,'resize',c)};if(d.length)for(var i=0;i<d.length;i++)h(d[i]);else h(d);return d}})();
fitText(document.getElementById('title'), 1)
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</body>
</html>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</body>