<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title> on Nanshu&#39;s blog </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://localhost:1313/tags/</link>
    <language>en-us</language>
    <author>Nanshu Wang</author>
    <copyright>Copyright (c) 2015, Nanshu Wang; all rights reserved.</copyright>
    <updated>Tue, 19 May 2015 00:00:00 UTC</updated>
    
    <item>
      <title>机器学习中的最优化问题：大规模学习问题的trade-off</title>
      <link>http://localhost:1313/post/2015-05-19</link>
      <pubDate>Tue, 19 May 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://localhost:1313/post/2015-05-19</guid>
      <description>

&lt;p&gt;最优化问题是机器学习中很重要的一部分，事实上机器学习中的学习算法的本质就是一个最优化问题。比如我们都熟悉的线性回归问题，使用最小二乘法来构建目标函数，参数学习的过程就是最优化这个目标函数的过程。而机器学习的实际问题中，数据的维度通常很高，目标函数中的参数很多，每一步迭代的计算量也较大，因此机器学习问题中对于数据量较大的问题通常采用单次迭代计算量较小的算法。比如Stochastic回归，就是降低了单次迭代的计算量，从而可以处理大规模的数据学习问题。因此，和最优化问题不同的是，机器学习中最优化目标函数根本不是其最终目标。机器学习的定义是：&amp;rdquo;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&amp;ldquo;(Tom Mitchell)。 从这个定义中可以知道，机器学习的目标是提高经验E，因此，机器学习需要考虑到模型泛化，训练时间等等问题，这和单纯地求解最优化问题实际上构成了一个trade-off的问题。《Optimization for Machine Learning》一书中的第十三章The tradeoffs of Large-Scale Learning讨论了大规模数据的机器学习问题中的一些tradeoffs，本文是阅读这部分内容后的一个总结。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;这篇文章的作者是Léon Bottou和Olivier Bousquet。Léon Bottou提出了Stochastic gradient descent这个最基本的机器学习算法，这个算法本身就是一种近似最优化算法，他也是DjVu图像压缩技术的发明者之一。Olivier Bousquet任职于Google，从事机器学习的相关研究。&lt;/p&gt;

&lt;p&gt;这篇文章主要提出了一种理论框架，来衡量机器学习中近似最优化方法的效果。分析结论发现，对于小规模数据和大规模数据，近似最优化算法的tradeoff是不相同的。&lt;/p&gt;

&lt;p&gt;机器学习算法的计算复杂度很长一段时间被忽略了，但随着训练集大小的增长，计算复杂度成为了一个瓶颈。作者给出了两个例子：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;数据挖掘的竞争优势在于分析描述我们这个可计算社会的大规模数据。每一台电脑都产生数据，因此数据量是和现有的计算能力成正比的。因此我们需要时间复杂度和数据总量增长成正比的一种机器学习方法。&lt;/li&gt;
&lt;li&gt;人工智能试图模仿人类的认知能力。我们的生理大脑只需消耗有限的糖分和水分，就能从我们的六个感官产生的数据中有效地学习。这个发现意味着存在一种学习算法，其计算时间和数据总量也是线性关系。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于以上两点，很有可能近似最优化算法对于机器学习问题已经足够用了。文章第一部分提出了一种分解测试误差的方法，增加了一项代表近似最优化误差的项。在小规模学习问题中，这种分解也就导致了近似误差和估计误差之间的一个trade-off。在大规模学习问题中，需要trade-off的更加复杂，因为要考虑到计算复杂度的问题。第二部分探讨了大规模学习问题对于各种学习算法原型的渐进性质。这部分显示了最优的最优化算法对于得到最优的机器学习结果可能并不需要。&lt;/p&gt;

&lt;h1 id=&#34;一-近似最优化:3461546ae96d9376984b841eaed7fa7f&#34;&gt;一、近似最优化&lt;/h1&gt;

&lt;h2 id=&#34;1-过量误差分解:3461546ae96d9376984b841eaed7fa7f&#34;&gt;1. 过量误差分解&lt;/h2&gt;

&lt;p&gt;考虑训练集由输入输出对$(x,y)\in X \times Y$组成，并且满足分布$P(x,y)$。其中$P(y|x)$表示输入$x$和输出$y$之间的未知概率关系。用损失函数$l(\hat{y},y)$表示预测值$\hat{y}$和真实输出$y$之间的差异。那么我们的衡量基准是能够最小化期望的损失的函数$f^*$：&lt;/p&gt;

&lt;p&gt;$$E(f) = \int l(\hat{y},y)dP(x,y) = \mathbb{E}[l(f(x),y)]$$&lt;/p&gt;

&lt;p&gt;$$f^*(x) = \underset{x}{\operatorname{argmin}} \mathbb{E}[l(\hat{y},y)]$$&lt;/p&gt;

&lt;p&gt;虽然分布$P(x,y)$未知，但我们可以认为每个样本的出现都是独立等概率的，那么定义经验损失：&lt;/p&gt;

&lt;div&gt;
$$E_n(f) = \frac1n \sum_{i=1}^n l(f(x_i),y_i) = \mathbb{E}_n[l(f(x),y)]$$
&lt;/div&gt;

&lt;p&gt;设F是预测函数的集合，定义$f_n = argmin_{f \in F}E_n(f)$，即$f_n$是使经验损失最小的那个预测函数。定义$f^*_F = argmin_{f \in F}E(f)$，即$f^*_F$是F中使期望损失最小的预测函数。注意$f^*_F$和$f^*$是不同的，因为$f^*$并不属于函数集合F。那么可以将过量误差进行一下分解：&lt;/p&gt;

&lt;div&gt;
$$\mathbb{E}[E(f_n)-E(f^*)] = \mathbb{E}[E(f^*_F)-E(f^*)]+\mathbb{E}[E(f_n)-E(f^*_F)] = \epsilon_{app} + \epsilon_{est}$$
&lt;/div&gt;

&lt;p&gt;其中$\epsilon_{app}$表示近似误差(approximation error)，用来估计F集合的函数和最优化函数$f^*$的近似程度。$\epsilon_{est}$表示估计误差(estimation error)，用来表示假设样本独立等概率出现带来的误差。&lt;/p&gt;

&lt;h2 id=&#34;2-增加最优化误差项:3461546ae96d9376984b841eaed7fa7f&#34;&gt;2. 增加最优化误差项&lt;/h2&gt;

&lt;p&gt;在实际中，通过最优化$E_n(f)$来找到$f_n$的计算量很大。实际中很可能并没有真正求解出$f_n$，而是得到一个近似解$\check{f}_n$。比如，可以在最优化求解时，在目标函数收敛之前就停止计算。&lt;/p&gt;

&lt;div&gt;
$$E_n(\check(f)_n) &lt; E_n(f_n) + \rho$$
&lt;/div&gt;

&lt;p&gt;这里$\rho$是一个提前定义的容忍度(最优化精度)。那么，可以增加一项最优化误差$\epsilon_{opt} = \mathbb{E}[E(\check(f)_n)-E(f_n)]$。那么从$\check{f}_n$到$f^*$的误差可以分解成三部分：&lt;/p&gt;

&lt;div&gt;
$$\epsilon = \mathbb{E}[E(f^*_F)-E(f^*)]+\mathbb{E}[E(f_n)-E(f*_F)] + \mathbb{E}[E(\check(f)_n)-E(f_n)]= \epsilon_{app} + \epsilon_{est} + \epsilon_{opt}$$
&lt;/div&gt;

&lt;p&gt;最优化误差受近似最优化求解的影响，和$\rho$的大小有关。&lt;/p&gt;

&lt;h2 id=&#34;3-近似误差-估计误差和最优化误差的trade-off:3461546ae96d9376984b841eaed7fa7f&#34;&gt;3. 近似误差，估计误差和最优化误差的trade-off&lt;/h2&gt;

&lt;p&gt;以上对误差的分解导致了更为复杂的trade-off情形。其中包括了三个变量和两个约束。三个变量分别是函数集合$F$的大小，最优化精度$\rho$和训练样本数$n$；两个约束是训练集样本数的最大值和训练时间的最大值。&lt;/p&gt;

&lt;p&gt;这里训练样本数$n$也是一个其中一个变量，因为在实际中我们可以只选择一部分样本进行训练，从而可以再分配的时间内解决最优化问题。下表总结了三个变量和三种不同误差之间的数量关系。&lt;/p&gt;

&lt;p&gt;注意这个最优化问题必须考虑到最大样本数和最大时间的约束。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于小规模训练问题，其约束在于最大样本数。因为训练时间没有显示，因此最优化问题也容易求解，因此最优化误差会很小。那么过量误差主要是由近似误差和估计误差造成的。&lt;/li&gt;
&lt;li&gt;对于大规模训练问题，其约束在于最大训练时间。在近似最优化的过程中，选择一个恰当的最优化精度$\rho&amp;gt;0$，不在精确求解最优化问题上浪费过多的时间。这样可能会使得到的训练模型有更好地泛化能力，因为这样可以在分配的时间内处理更多地训练样本。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;二-大规模学习的渐进性质:3461546ae96d9376984b841eaed7fa7f&#34;&gt;二、大规模学习的渐进性质&lt;/h1&gt;

&lt;h2 id=&#34;1-估计误差和最优化误差的收敛性质:3461546ae96d9376984b841eaed7fa7f&#34;&gt;1. 估计误差和最优化误差的收敛性质&lt;/h2&gt;

&lt;p&gt;这里假设$F$中的预测函数是参数$w \in \mathbb{R}^d$。那么&lt;/p&gt;

&lt;div&gt;
$$\mathbb{E}[sup_{f \in F}|E(f)-E_n(f)|] \leq c \sqrt{\frac dn}$$
&lt;/div&gt;

&lt;p&gt;这里的期望值是针对训练集的随机选择。那么对于估计误差，其上界为：&lt;/p&gt;

&lt;div
$$\Epsilon_{est} \leq 2 \mathbb{E}[sup_{f \in F}|E(f)-E_n(f)|] \leq c \sqrt{\frac dn}$$
&lt;/div&gt;

&lt;p&gt;对于估计误差和最优化误差同样也有一个上界：&lt;/p&gt;

&lt;div&gt;
$$\epsilon_{est} + \epsilon_{opt} \leq c \sqrt{\frac dn} + \rho + 0 + c \sqrt{\frac dn} = c(\rho + \sqrt{\frac dn})$$
&lt;/div&gt;

&lt;p&gt;当损失函数$l(\hat y,y)$在对任意$\tau &amp;gt;0$时，有$1-E^{-\tau}$的概率为正时，有&lt;/p&gt;

&lt;div&gt;
$$sup_{f \in F} \frac {E(f)-E_n(f)}{\sqrt{E(f)}} \leq c\sqrt{\frac dn \log {\frac nd} + \frac {\tau} n}$$
&lt;/div&gt;

&lt;p&gt;那么，可以得到估计误差和最优化误差的一个更好地上界估计&lt;/p&gt;

&lt;div&gt;
$$\epsilon_{est}+\epsilon_{opt} \leq \mathbb{E}[E(\check(f)_n)] = c(\rho + \frac dn \log \frac nd+ \rho)$$
&lt;/div&gt;

&lt;h2 id=&#34;2-梯度最优化算法:3461546ae96d9376984b841eaed7fa7f&#34;&gt;2. 梯度最优化算法&lt;/h2&gt;

&lt;p&gt;假设损失函数为凸函数并且二阶连续可微。凸函数的性质可以保证所求目标函数只有一个最优解。&lt;/p&gt;

&lt;p&gt;这篇文章的讨论根本成本函数的梯度迭代更新参数$w(t)$。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;梯度下降法 Gradient Descent (GD)&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;
$$w(t+1) = w(t)-\eta \frac{\delta C}{\delta w}(w(t)) = w(t) - \eta \frac 1n \sum_{i=1}^{n} \frac{\delta}{\delta w}l(f_{w(t)(x_i),y_i})$$
&lt;/div&gt;

&lt;p&gt;其中$\eta&amp;gt;0$，梯度下降收敛速度是线性的。当$\eta = 1/\lambda_{max}$时，算法需要迭代$O(k \log (1/\rho))$次达到精度$\rho$。精确的迭代次数取决于初始参数的设定。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;二阶梯度下降法Second Order Gradient Descent (2GD)&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;
$$w(t+1) = w(t)- H^{-1} \frac {\delta C}{\delta w}(w(t)) = w(t) - \frac 1n H^{-1} \sum_{i=1}^{n} \frac{\delta}{\delta w}l(f_{w(t)(x_i),y_i})$$
&lt;/div&gt;

&lt;p&gt;其中，$H^{-1}$是Hessian矩阵的逆矩阵。二次梯度下降法比牛顿法要好，因为我们并不计算每次迭代时的Hessian矩阵，而是简单假设我们提前知道了最有点处的Hessian矩阵。二次梯度下降法是一种超线性的最优化算法，但拥有二次收敛速度。当成本函数是二次时，只需要一次迭代就能到达最优解。更普遍的情况下，需要$O(\log \log (1/\rho))$次迭代就能达到精度$\rho$。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随机梯度下降法Stochastic Gradient Descent (SGD)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;随机梯度下降法每次迭代选择一个随机的样本，仅仅根据这一个样本更新参数$w$。&lt;/p&gt;

&lt;div&gt;
$$w(t+1) = w(t) - \frac {\eta}t \frac {\delta}{\delta w} l(f_{w(t)(x_t),y_t})$$
&lt;/div&gt;

&lt;p&gt;随机梯度下降法需要迭代$vk^2/\rho + o(1/\rho)$次达到精度$\rho$。随机梯度下降的收敛性受每次迭代的随机样本选择所产生的噪声的限制。参数的初始值和训练及样本数都不能影响迭代次数的上界。当训练集很大时，也有可能没有遍历完所有的训练集就可以达到精度$\rho$。这实际上是一种泛化的上界。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;二阶随机梯度下降法Second Order Stochastic Gradient Descent (2SGD)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;二阶随机梯度下降法将$\eta$替换为Hessian矩阵的逆矩阵。&lt;/p&gt;

&lt;div&gt;
$$w(t+1) = w(t) - \frac 1t H^{-1} \frac {\delta}{\delta w} l(f_{w(t)(x_t),y_t})$$
&lt;/div&gt;

&lt;p&gt;和标准的梯度下降法不一样，使用二阶信息并没有影响$\rho$对收敛性的影响，不过优化了收敛的常数项。达到精度$\rho$的迭代次数为$v/p + p(1/p)$&lt;/p&gt;

&lt;p&gt;下表比较了四种算法的渐进性质。其中$n$表示训练集样本数，$d$表示参数的维度。&lt;/p&gt;

&lt;p&gt;表中的四列分别表示了这四种算法的单步迭代时间，到达精度$\rho$的迭代次数，到达精度$\rho$的时间，使过量误差$\epsilon$小于$c(\epsilon_{app}+\epsilon)$。&lt;/p&gt;

&lt;p&gt;结果表示，对于大规模学习系统的泛化性能，取决于估计的统计性质和最优化算法的计算性质。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SGD和2SGD算法与估计速率$\alpha$无关。当估计速率$\alpha$的值并不好时，没必要对最优化问题精确求解。那么可以节约下来时间去处理更多的训练集，而这已经被认为是一种提升模型泛化能力的方法。&lt;/li&gt;
&lt;li&gt;二阶算法对$\epsilon$的提升作用很少。虽然超线性2GD算法优化了对数项，但四种算法都主要受$(1/\epsilon)$的影响。&lt;/li&gt;
&lt;li&gt;随机算法(SGD,2SGD)虽然是最差的优化算法，但却有最好的泛化能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;三-结论:3461546ae96d9376984b841eaed7fa7f&#34;&gt;三、结论&lt;/h1&gt;

&lt;p&gt;将训练集数量和计算时间纳入考虑，发现了小规模学习系统和大规模学习系统的泛化能力之间存在着质的不同。大规模学习系统的泛化性质同时取决于估计的统计性质和最优化算法的计算性质。&lt;/p&gt;

&lt;h1 id=&#34;四-读后感:3461546ae96d9376984b841eaed7fa7f&#34;&gt;四、读后感&lt;/h1&gt;

&lt;p&gt;这篇论文介绍了在大规模机器学习的问题中，非精确的最优化算法反而比精确的最优化算法更加有效，因为其一可以提升模型的泛化能力，其二可以使问题在更快的时间内求解。这也说明最优化算法在实际应用中的形式不是固定的，需要根据问题的特殊性来找到最优的解法。机器学习问题虽然最终形式是作为最优化问题呈现，但其最终目标并不是求解最优化问题，而是面向一个现实问题，需要考虑到问题的规模、复杂性、时间成本等等因素。因此，不能简单地将最优化问题和机器学习问题等同起来，特别是面对在实际问题，要综合考虑各种因素。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考资料：&lt;/strong&gt;
[1]. Bousquet O, Bottou L. The tradeoffs of large scale learning. Advances in neural information processing systems. 2008: 161-168.
[2]. Chiyuan Zhang. Machine Learning and Optimization. Retrieved from &lt;a href=&#34;http://freemind.pluskid.org/series/mlopt/&#34;&gt;http://freemind.pluskid.org/series/mlopt/&lt;/a&gt;
[3]. Andrew Ng. Machine Learning:Introduction. Retrieved from &lt;a href=&#34;https://share.coursera.org/wiki/index.php/ML:Introduction&#34;&gt;https://share.coursera.org/wiki/index.php/ML:Introduction&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
