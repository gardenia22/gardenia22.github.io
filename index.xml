<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title> on Nanshu&#39;s blog </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://nanshu.wang/</link>
    <language>en-us</language>
    <author>Nanshu Wang</author>
    <copyright>Copyright (c) 2015, Nanshu Wang; all rights reserved.</copyright>
    <updated>Mon, 25 Jan 2016 00:00:00 UTC</updated>
    
    <item>
      <title>How to write shortest code with Python</title>
      <link>http://nanshu.wang/post/2016-01-25</link>
      <pubDate>Mon, 25 Jan 2016 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2016-01-25</guid>
      <description>&lt;p&gt;I was so enthralled by the &lt;a href=&#34;https://codefights.com&#34;&gt;Codefights&lt;/a&gt; Challenges of writing shortest code last weekend. After several attempts, I finally managed to rank #1 in this &lt;a href=&#34;https://codefights.com/challenge/vjxo2WFyex6a85BrH&#34;&gt;MatchingParentheses&lt;/a&gt; problem with one-line solution of only 77 chars(excludes whitespace).&lt;/p&gt;

&lt;p&gt;Curtailing chars of solution is fun with little tricks. In the following I will show the tricks I used in this problem.&lt;/p&gt;

&lt;p&gt;Here is the problem description:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Given a string para, consisting of symbols &#39;(&#39;, &#39;[&#39;, &#39;{&#39;, &#39;)&#39;, &#39;]&#39;, &#39;}&#39; and &#39; &#39;, find out if it is a correct bracket sequence (CBS in short) with occasional whitespace (&#39; &#39;) characters. A CBS can be defined as follows:
empty string (&amp;quot;&amp;quot;) is a CBS;
if S is a CBS, then (S), [S], {S} are CBSs;
if S1, S2 are CBS, then S1S2 is a CBS.

Example:
MatchingParentheses(&amp;quot;( )(( )){([( )])}&amp;quot;) = true
MatchingParentheses(&amp;quot;)(&amp;quot;) = false
[input] string para
A string of symbols &#39;(&#39;, &#39;[&#39;, &#39;{&#39;, &#39;)&#39;, &#39;]&#39;, &#39;}&#39; and &#39; &#39;.
[output] boolean
true if the given string is CBS, false otherwise.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let me put my solution here. It is short and hard to understand. I will explain step by step how I get there.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MatchingParentheses = f = lambda p,d=1:d and f(*re.subn(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p)) or not p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First of my thought, stack would be a perfect way to solve this problem: push the element when left bracket appears and pop last element when matching right bracket appears. If the stack is empty at last, return True, otherwise False.&lt;/p&gt;

&lt;p&gt;Before translating this procedure into code, think twice if there is simpler way. The answer is YES.&lt;/p&gt;

&lt;p&gt;Instead of pushing and popping the element, can we directly delete adjacent matching parentheses? We delete repeatedly until the string is empty or we can&amp;rsquo;t find parentheses to delete anymore. This way is quite straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def MatchingParentheses(p):
    # d function deletes adjacent matching parentheses and whitespace
    def d(p):
        return re.sub(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p)
    while p and p!=d(p):
        p = d(p)
    if p==&#39;&#39;:
        return True
    else:
        return False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This solution is easy to understand with 125 chars. Then let&amp;rsquo;s cut down the chars.&lt;/p&gt;

&lt;p&gt;##Lessen the return statement&lt;/p&gt;

&lt;p&gt;We can absolutely lose the if-else statement and write just:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return p==&#39;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In Python, empty string has False value in bool type. This works too:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return not p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;##use subn() instead of sub&lt;/p&gt;

&lt;p&gt;subn() return 2 arguments, first is the modified string and second is the times of modification. Then this solution becomes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def MatchingParentheses(p):
    # d function deletes adjacent matching parentheses and whitespace
    def d(p):
        return re.subn(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p)
    n = 1
    while n:
        p,n= d(p)
    return not p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since we only use d function once, there is no need to define it separately.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def MatchingParentheses(p):
    # d function deletes adjacent matching parentheses and whitespace
    n = 1
    while n:
        p,n= re.subn(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p)
    return not p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have reduce this solution to 79 chars, but we still have room to go even further.&lt;/p&gt;

&lt;p&gt;##Lambda function&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s translate the whole function into lambda expression. Because lambda can only have one-line, we need to use recursion as an alternative to while loop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MatchingParentheses = lambda p,n=1:MatchingParentheses(re.subn(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p)) if n else not p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We write MatchingParentheses twice, and that&amp;rsquo;s a lot of chars! There is a little trick to save chars:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MatchingParentheses = f = lambda p,n=1:f(*re.subn(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p)) if n else not p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have 78 chars, with only 1 char reduced after this big transformation&amp;hellip;(So we really shouldn&amp;rsquo;t write code like this if we are not in this shortest code competition.)&lt;/p&gt;

&lt;p&gt;And can we reduce even more chars?&lt;/p&gt;

&lt;p&gt;Answer is still YES!&lt;/p&gt;

&lt;p&gt;##Ternary in Python&lt;/p&gt;

&lt;p&gt;There is no ternary expression in Python like &lt;code&gt;(condition)? v1:v2&lt;/code&gt;, instead we have &lt;code&gt;v1 if condition else v2&lt;/code&gt;. But there is a &lt;strong&gt;dangerous&lt;/strong&gt; way to use bool operation as a proxy of ternary expression: &lt;code&gt;condition and v1 or v2&lt;/code&gt;. This is dangerous because this proxy is wrong when &lt;code&gt;v1&lt;/code&gt; has possible &lt;code&gt;0&lt;/code&gt; value. The right bool expression is: &lt;code&gt;(condition and [v1] or [v2])[0]&lt;/code&gt;. In this problem, &lt;code&gt;v1 = f(*re.subn(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p))&lt;/code&gt; is a tuple of 2 values, so there is no chance &lt;code&gt;v1&lt;/code&gt; equals to &lt;code&gt;0&lt;/code&gt;. So we take a risk here and get our &lt;strong&gt;final solution&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MatchingParentheses = f = lambda p,n=1:n and f(*re.subn(&#39;\(\)|{}|\[\]| &#39;,&#39;&#39;,p)) or not p
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fortunately, this trick saves one more char for us. The final solution has 77 chars in total.&lt;/p&gt;

&lt;p&gt;##Some takeaway&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Never pursue writing shortest code in real practice.&lt;/li&gt;
&lt;li&gt;Never overuse tricks.&lt;/li&gt;
&lt;li&gt;Keep code readable and simple.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you can write even shorter solution for this problem, welcome to leave a comment here. :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python代码风格指南（三）命名约定(PEP8中文翻译)</title>
      <link>http://nanshu.wang/post/2015-09-14</link>
      <pubDate>Mon, 31 Aug 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-09-14</guid>
      <description>

&lt;h1 id=&#34;命名约定-naming-conventions:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;命名约定（Naming Conventions）&lt;/h1&gt;

&lt;p&gt;Python标准库的命名约定有一些混乱，因此我们永远都无法保持一致。但如今仍然存在一些推荐的命名标准。新的模块和包（包括第三方框架）应该采用这些标准，但若是已经存在的包有另一套风格的话，还是应当与原有的风格保持内部一致。&lt;/p&gt;

&lt;h2 id=&#34;重写原则-overriding-principle:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;重写原则（Overriding Principle）&lt;/h2&gt;

&lt;p&gt;对于用户可见的公共部分API，其命名应当表达出功能用途而不是其具体的实现细节。&lt;/p&gt;

&lt;h2 id=&#34;描述性-命名风格-descriptive-naming-styles:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;描述性：命名风格（Descriptive: Naming Styles）&lt;/h2&gt;

&lt;p&gt;存在很多不同的命名风格，最好能够独立地从命名对象的用途认出采用了哪种命名风格。&lt;/p&gt;

&lt;p&gt;以下是常用于区分的命名风格：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- ``b`` (单个小写字母)
- ``B`` (单个大写字母)
- ``lowercase``(小写)
- ``lower_case_with_underscores``(带下划线小写)
- ``UPPERCASE``(带下划线大写)
- ``UPPER_CASE_WITH_UNDERSCORES``(带下划线大写)
- ``CapitalizedWords`` (也叫做CapWords或者CamelCase -- 因为单词首字母大写看起来很像驼峰)。也被称作StudlyCaps。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：当CapWords里包含缩写时，将缩写部分的字母都大写。HTTPServerError比HttpServerError要好。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- ``mixedCase`` (注意：和CapitalizedWords不同在于其首字母小写！)
- ``Capitalized_Words_With_Underscores`` (这种风格超丑！)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也有风格使用简短唯一的前缀来表示一组相关的命名。这在Python中并不常见，但为了完整起见这里也捎带提一下。比如，&lt;code&gt;os.stat()&lt;/code&gt;函数返回一个tuple，其中的元素名原本为&lt;code&gt;st_mode&lt;/code&gt;,&lt;code&gt;st-size&lt;/code&gt;,&lt;code&gt;st_mtime&lt;/code&gt;等等。（这样做是为了强调和POSIX系统调用结构之间的关系，可以帮助程序员更好熟悉。）&lt;/p&gt;

&lt;p&gt;X11库中的公共函数名都以X开头。在Python中这样的风格一般被认为是不必要的，因为属性和方法名之前已经有了对象名的前缀，而函数名前也有了模块名的前缀。&lt;/p&gt;

&lt;p&gt;此外，要区别以下划线开始或结尾的特殊形式（可以和其它的规则结合起来）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- ``_single_leading_underscore``: 以单个下划线开头是&amp;quot;内部使用&amp;quot;的弱标志。
  E.g. ``from M import *``不会import下划线开头的对象。

- ``single_trailing_underscore_``: 以单个下划线结尾用来避免和Python关键词产生冲突，例如:

      Tkinter.Toplevel(master, class_=&#39;ClassName&#39;)

- ``__double_leading_underscore``: 以双下划线开头的风格命名类属性表示触发命名修饰（在FooBar类中，``__boo``命名会被修饰成``_FooBar__boo``; 见下）。

- ``__double_leading_and_trailing_underscore__``: 以双下划线开头和结尾的命名风格表示生存在用户控制的命名空间里“魔法”对象或属性。
  E.g. ``__init__``, ``__import__`` 或 ``__file__``。请依照文档描述来使用这些命名，千万不要自己发明。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;规范性-命名约定-prescriptive-naming-conventions:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;规范性：命名约定(Prescriptive: Naming Conventions)&lt;/h2&gt;

&lt;h3 id=&#34;需要避免的命名-names-to-avoid:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;需要避免的命名(Names to Avoid)&lt;/h3&gt;

&lt;p&gt;不要使用字符&amp;rsquo;l&amp;rsquo;（小写的字母el），&amp;rsquo;O&amp;rsquo;（大写的字母oh），或者&amp;rsquo;I&amp;rsquo;（大写的字母eye）来作为单个字符的变量名。&lt;/p&gt;

&lt;p&gt;在一些字体中，这些字符和数字1和0无法区别开来。当想使用&amp;rsquo;l&amp;rsquo;时，使用&amp;rsquo;L&amp;rsquo;代替。&lt;/p&gt;

&lt;h3 id=&#34;包和模块命名-package-and-module-names:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;包和模块命名(Package and Module Names)&lt;/h3&gt;

&lt;p&gt;模块命名应短小，且为全小写。若下划线能提高可读性，也可以在模块名中使用。Python包命名也应该短小，且为全小写，但不应使用下划线。&lt;/p&gt;

&lt;p&gt;模块名是对应到文件名的，一些文件系统会区分大小写并且会将长的文件名截断。因此模块名应该尽量短小，这个问题在Unix系统上是不存在的，但把代码移植到较旧的Mac，Windows版本或DOS系统上时，可能会出现问题。&lt;/p&gt;

&lt;p&gt;当使用C或C++写的扩展模块有相应的Python模块提供更高级的接口时（e.g. 更加面向对象），C/C++模块名以下划线开头（e.g. &lt;code&gt;_sociket&lt;/code&gt;）。&lt;/p&gt;

&lt;h3 id=&#34;类命名-class-names:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;类命名(Class Names)&lt;/h3&gt;

&lt;p&gt;类命名应该使用单词字母大写（CapWords）的命名约定。&lt;/p&gt;

&lt;p&gt;当接口已有文档说明且主要是被用作调用时，也可以使用函数的命名约定。&lt;/p&gt;

&lt;p&gt;注意对于内建命名(builtin names)有一个特殊的约定：大部分内建名都是一个单词（或者两个一起使用的单词），单词首字母大写(CapWords)的约定只对异常命名和内建常量使用。&lt;/p&gt;

&lt;h3 id=&#34;异常命名-exception-names:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;异常命名(Exception Names)&lt;/h3&gt;

&lt;p&gt;由于异常实际上也是类，因此类命名约定也适用与异常。不同的是，如果异常实际上是抛出错误的话，异常名前应该加上&amp;rdquo;Error&amp;rdquo;的前缀。&lt;/p&gt;

&lt;h3 id=&#34;全局变量命名-global-variable-names:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;全局变量命名(Global Variable Names)&lt;/h3&gt;

&lt;p&gt;（在此之前，我们先假定这些变量都仅在同一个模块内使用。）这些约定同样也适用于函数命名。&lt;/p&gt;

&lt;p&gt;对于引用方式设计为&lt;code&gt;from M import *&lt;/code&gt;的模块，应该使用&lt;code&gt;__all__&lt;/code&gt;机制来避免import全局变量，或者采用下划线前缀的旧约定来命名全局变量，从而表明这些变量是“模块非公开的”。&lt;/p&gt;

&lt;h3 id=&#34;函数命名-function-names:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;函数命名(Function Names)&lt;/h3&gt;

&lt;p&gt;函数命名应该都是小写，必要时使用下划线来提高可读性。&lt;/p&gt;

&lt;p&gt;只有当已有代码风格已经是混合大小写时（比如threading.py），为了保留向后兼容性才使用混合大小写。&lt;/p&gt;

&lt;h3 id=&#34;函数和方法参数-function-and-method-arguments:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;函数和方法参数(Function and method arguments)&lt;/h3&gt;

&lt;p&gt;实例方法的第一参数永远都是&lt;code&gt;self&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;类方法的第一个参数永远都是&lt;code&gt;cls&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在函数参数名和保留关键字冲突时，相对于使用缩写或拼写简化，使用以下划线结尾的命名一般更好。比如，&lt;code&gt;class_&lt;/code&gt;比&lt;code&gt;clss&lt;/code&gt;更好。（或许使用同义词避免这样的冲突是更好的方式。）&lt;/p&gt;

&lt;h3 id=&#34;方法命名和实例变量-method-names-and-instance-variables:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;方法命名和实例变量(Method Names and Instance Variables)&lt;/h3&gt;

&lt;p&gt;使用函数命名的规则：小写单词，必要时使用下划线分开以提高可读性。&lt;/p&gt;

&lt;p&gt;仅对于非公开方法和变量命名在开头使用一个下划线。&lt;/p&gt;

&lt;p&gt;避免和子类的命名冲突，使用两个下划线开头来触发Python的命名修饰机制。&lt;/p&gt;

&lt;p&gt;Python类名的命名修饰规则：如果类Foo有一个属性叫&lt;code&gt;__a&lt;/code&gt;，不能使用&lt;code&gt;Foo.__a&lt;/code&gt;的方式访问该变量。（有用户可能仍然坚持使用&lt;code&gt;Foo._Foo__a&lt;/code&gt;的方法访问。）一般来说，两个下划线开头的命名方法只应该用来避免设计为子类的属性中的命名冲突。&lt;/p&gt;

&lt;p&gt;注意：关于__names的使用也有一些争论（见下）。&lt;/p&gt;

&lt;h3 id=&#34;常量-constants:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;常量(Constants)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;MAX_OVERFLOW&lt;/code&gt; and &lt;code&gt;TOTAL&lt;/code&gt;.
常量通常是在模块级别定义的，使用全部大写并用下划线将单词分开。例如：&lt;code&gt;MAX_OVERFLOW&lt;/code&gt;和&lt;code&gt;TOTAL&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;继承的设计-designing-for-inheritance:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;继承的设计(Designing for inheritance)&lt;/h3&gt;

&lt;p&gt;记得永远区别类的方法和实例变量（属性）应该是公开的还是非公开的。如果有疑虑的话，请选择非公开的；因为之后将非公开属性变为公开属性要容易些。&lt;/p&gt;

&lt;p&gt;公开属性是那些和你希望和你定义的类无关的客户来使用的，并且确保不会出现向后不兼容的问题。非公开属性是那些不希望被第三方使用的部分，你可以不用保证非公开属性不会变化或被移除。&lt;/p&gt;

&lt;p&gt;我们在这里没有使用“私有（private）”这个词，因为在Python里没有什么属性是真正私有的（这样设计省略了大量不必要的工作）。&lt;/p&gt;

&lt;p&gt;另一类属性属于子类API的一部分（在其他语言中经常被称为&amp;rdquo;protected&amp;rdquo;）。一些类是为继承设计的，要么扩展要么修改类行为的部分。当设计这样的类时，需要谨慎明确地决定哪些属性是公开的，哪些属于子类API，哪些真的只会被你的基类调用。&lt;/p&gt;

&lt;p&gt;请记住以上几点，下面是Python风格的指南：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;公开属性不应该有开头下划线。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果公开属性的名字和保留关键字有冲突，在你的属性名尾部加上一个下划线。这比采用缩写和简写更好。（然而，和这条规则冲突的是，‘cls’对任何变量和参数来说都是一个更好地拼写，因为大家都知道这表示class，特别是在类方法的第一个参数里。）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;注意 1：对于类方法，参考之前的参数命名建议。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;-对于简单的公共数据属性，最后仅公开属性名字，不要公开复杂的调用或设值方法。记住在Python中，提供了一条简单的路径来实现未来增强，你应该简单数据属性需要增加功能行为。这种情况下，使用properties将功能实现隐藏在简单数据属性访问语法之后。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;注意 1：Properties仅仅对新风格类有用。&lt;/p&gt;

&lt;p&gt;注意 2：尽量保证功能行为没有副作用，尽管缓存这种副作用看上去并没有什么大问题。&lt;/p&gt;

&lt;p&gt;注意 3: 对计算量大的运算避免试用properties；属性的注解会让调用者相信访问的运算量是相对较小的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;如果你的类是子类的话，你有一些属性并不想让子类访问，考虑将他们命名为两个下划线开头并且结尾处没有下划线。这样会触发Python命名修饰算法，类名会被修饰添加到属性名中。这样可以避免属性命名冲突，以免子类会不经意间包含相同的命名。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;注意 1：注意命名修饰仅仅是简单地将类名加入到修饰名中，所以如果子类有相同的类名合属性名，你可能仍然会遇到命名冲突问题。&lt;/p&gt;

&lt;p&gt;注意 2：命名修饰可以有特定用途，比如在调试时，&lt;code&gt;__getattr__()&lt;/code&gt;比较不方便。然而命名修饰算法的可以很好地记录，并且容意手动执行。&lt;/p&gt;

&lt;p&gt;注意3：不是所有人都喜欢命名修饰。试着权衡避免偶然命名冲突的需求和试用高级调用者使用的潜在可能性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;公开和内部接口-public-and-internal-interfaces:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;公开和内部接口(Public and internal interfaces)&lt;/h2&gt;

&lt;p&gt;任何向后兼容性保证仅对公开接口适用。相应地，用户能够清楚分辨公开接口和内部接口是很重要的。&lt;/p&gt;

&lt;p&gt;文档化的接口被认为是公开的，除非文档中明确申明了它们是临时的或者内部接口，不保证向后兼容性。所有文档中未提到的接口应该被认为是内部的。&lt;/p&gt;

&lt;p&gt;为了更好审视公开接口和内部接口，模块应该在&lt;code&gt;__all&lt;/code&gt;属性中明确申明公开API是哪些。将&lt;code&gt;__all__&lt;/code&gt;设为空list表示该模块中没有公开API。&lt;/p&gt;

&lt;p&gt;即使正确设置了&lt;code&gt;__all&lt;/code&gt;属性，内部接口（包，模块，类，函数，属性或其他命名）也应该以一个下划线开头。&lt;/p&gt;

&lt;p&gt;如果接口的任一一个命名空间（包，模块或类）是内部的，那么该接口也应该是内部的。&lt;/p&gt;

&lt;p&gt;导入的命名应该永远被认为是实现细节。其他模块不应当依赖这些非直接访问的导入命名，除非它们在文档中明确地被写为模块的API，例如&lt;code&gt;os.path&lt;/code&gt;或者包的&lt;code&gt;__init__&lt;/code&gt;模块，那些从子模块展现的功能。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python代码风格指南（二）字符串引用、空格、注释和版本注记(PEP8中文翻译)</title>
      <link>http://nanshu.wang/post/2015-08-30</link>
      <pubDate>Sun, 30 Aug 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-08-30</guid>
      <description>

&lt;h1 id=&#34;字符串引用-string-quotes:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;字符串引用(String Quotes)&lt;/h1&gt;

&lt;p&gt;在Python中表示字符串时，不管用单引号还是双引号都是一样的。但是不推荐将这两种方式看作一样并且混用。最好选择一种规则并坚持使用。当字符串中包含单引号时，采用双引号来表示字符串，反之也是一样，这样可以避免使用反斜杠，代码也更易读。&lt;/p&gt;

&lt;p&gt;对于三引号表示的字符串，使用双引号字符来表示（译注：即用&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;而不是&lt;code&gt;&#39;&#39;&#39;&lt;/code&gt;），这样可以和PEP 257的文档字符串（docstring）规则保持一致。&lt;/p&gt;

&lt;h1 id=&#34;表达式和语句中的空格-whitespace-in-expressions-and-statements:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;表达式和语句中的空格(Whitespace in Expressions and Statements)&lt;/h1&gt;

&lt;h2 id=&#34;一些痛点-pet-peeves:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;一些痛点(Pet peeves)&lt;/h2&gt;

&lt;p&gt;在下列情形中避免使用过多的空白：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;方括号，圆括号和花括号之后：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;      Yes: spam(ham[1], {eggs: 2})
      No:  spam( ham[ 1 ], { eggs: 2 } )
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;逗号，分号或冒号之前：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;      Yes: if x == 4: print x, y; x, y = y, x
      No:  if x == 4 : print x , y ; x , y = y , x
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;不过，在分片操作时，冒号和二元运算符是一样的，应该在其左右两边保留相同数量的空格（就像对待优先级最低的运算符一样）。在扩展的分片操作中，所有冒号的左右两边空格数都应该相等。不过也有例外，当切片操作中的参数被省略时，应该也忽略空格。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  Yes:

      ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]
      ham[lower:upper], ham[lower:upper:], ham[lower::step]
      ham[lower+offset : upper+offset]
      ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]
      ham[lower + offset : upper + offset]

  No:

      ham[lower + offset:upper + offset]
      ham[1: 9], ham[1 :9], ham[1:9 :3]
      ham[lower : : upper]
      ham[ : upper]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;在调用函数时传递参数list的括号之前：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;      Yes: spam(1)
      No:  spam (1)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;在索引和切片操作的左括号之前：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;      Yes: dct[&#39;key&#39;] = lst[index]
      No:  dct [&#39;key&#39;] = lst [index]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;赋值(或其他)运算符周围使用多个空格来和其他语句对齐：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  Yes::

      x = 1
      y = 2
      long_variable = 3

  No::

      x             = 1
      y             = 2
      long_variable = 3
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;其他建议:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;其他建议&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在二元运算符的两边都使用一个空格：赋值运算符(&lt;code&gt;=&lt;/code&gt;)，增量赋值运算符(&lt;code&gt;+=&lt;/code&gt;, &lt;code&gt;-=&lt;/code&gt;
etc.)，比较运算符(&lt;code&gt;==&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;,
&lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;not in&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;, &lt;code&gt;is not&lt;/code&gt;)，布尔运算符(&lt;code&gt;and&lt;/code&gt;,
&lt;code&gt;or&lt;/code&gt;, &lt;code&gt;not&lt;/code&gt;)。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果使用了优先级不同的运算符，则在优先级较低的操作符周围增加空白。请你自行判断，不过永远不要用超过1个空格，永远保持二元运算符两侧的空白数量一样。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  Yes::

      i = i + 1
      submitted += 1
      x = x*2 - 1
      hypot2 = x*x + y*y
      c = (a+b) * (a-b)

  No::

      i=i+1
      submitted +=1
      x = x * 2 - 1
      hypot2 = x * x + y * y
      c = (a + b) * (a - b)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;使用&lt;code&gt;=&lt;/code&gt;符号来表示关键字参数或默认参数值时，不要在其周围使用空格。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  Yes::

      def complex(real, imag=0.0):
          return magic(r=real, i=imag)

  No::

      def complex(real, imag = 0.0):
          return magic(r = real, i = imag)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;在带注释的函数定义中需要在&lt;code&gt;=&lt;/code&gt;符号周围加上空格。此外, 在&lt;code&gt;:&lt;/code&gt;后使用一个空格，在&lt;code&gt;-&amp;gt;&lt;/code&gt;表示带注释的返回值时，其两侧各使用一个空格。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  Yes::

      def munge(input: AnyStr):
      def munge(sep: AnyStr = None):
      def munge() -&amp;gt; AnyStr:
      def munge(input: AnyStr, sep: AnyStr = None, limit=1000):

  No::

      def munge(input: AnyStr=None):
      def munge(input:AnyStr):
      def munge(input: AnyStr)-&amp;gt;PosInt:
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;复合语句（即将多行语句写在一行）一般是不鼓励使用的。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  Yes::

      if foo == &#39;blah&#39;:
          do_blah_thing()
      do_one()
      do_two()
      do_three()

  Rather not::

      if foo == &#39;blah&#39;: do_blah_thing()
      do_one(); do_two(); do_three()
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;有时也可以将短小的if/for/while中的语句写在一行，但对于有多个分句的语句永远不要这样做。也要避免将多行都写在一起。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  Rather not::

      if foo == &#39;blah&#39;: do_blah_thing()
      for x in lst: total += x
      while t &amp;lt; 10: t = delay()

  Definitely not::

      if foo == &#39;blah&#39;: do_blah_thing()
      else: do_non_blah_thing()

      try: something()
      finally: cleanup()

      do_one(); do_two(); do_three(long, argument,
                                   list, like, this)

      if foo == &#39;blah&#39;: one(); two(); three()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;注释-comments:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;注释(Comments)&lt;/h1&gt;

&lt;p&gt;和代码矛盾的注释还不如没有。当代码有改动时，一定要优先更改注释使其保持最新。&lt;/p&gt;

&lt;p&gt;注释应该是完整的多个句子。如果注释是一个短语或一个句子，其首字母应该大写，除非开头是一个以小写字母开头的标识符（永远不要更改标识符的大小写）。&lt;/p&gt;

&lt;p&gt;如果注释很短，结束的句号可以被忽略。块注释通常由一段或几段完整的句子组成，每个句子都应该以句号结束。&lt;/p&gt;

&lt;p&gt;你应该在句尾的句号后再加上2个空格。&lt;/p&gt;

&lt;p&gt;使用英文写作，参考Strunk和White的《The Elements of Style》&lt;/p&gt;

&lt;p&gt;来自非英语国家的Python程序员们，请使用英文来写注释，除非你120%确定你的代码永远不会被不懂你所用语言的人阅读到。&lt;/p&gt;

&lt;h2 id=&#34;块注释-block-comments:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;块注释（Block Comments）&lt;/h2&gt;

&lt;p&gt;块注释一般写在对应代码之前，并且和对应代码有同样的缩进级别。块注释的每一行都应该以&lt;code&gt;#&lt;/code&gt;和一个空格开头（除非该文本是在注释内缩进对齐的）。&lt;/p&gt;

&lt;p&gt;块注释中的段落应该用只含有单个&lt;code&gt;#&lt;/code&gt;的一行隔开。&lt;/p&gt;

&lt;h2 id=&#34;行内注释-inline-comments:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;行内注释（Inline Comments）&lt;/h2&gt;

&lt;p&gt;尽量少用行内注释。&lt;/p&gt;

&lt;p&gt;行内注释是和代码语句写在一行内的注释。行内注释应该至少和代码语句之间有两个空格的间隔，并且以&lt;code&gt;#&lt;/code&gt;和一个空格开始。&lt;/p&gt;

&lt;p&gt;行内注释通常不是必要的，在代码含义很明显时甚至会让人分心。请不要这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = x + 1                 # Increment x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但这样做是有用的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = x + 1                 # Compensate for border
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;文档字符串-documentation-strings:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;文档字符串(Documentation Strings)&lt;/h2&gt;

&lt;p&gt;要知道如何写出好的文档字符串（docstring），请参考PEP 257。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;所有的公共模块，函数，类和方法都应该有文档字符串。对于非公共方法，文档字符串不是必要的，但你应该留有注释说明该方法的功能，该注释应当出现在&lt;code&gt;def&lt;/code&gt;的下一行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;PEP 257描述了好的文档字符应该遵循的规则。其中最重要的是，多行文档字符串以单行&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;结尾，不能有其他字符，例如：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;      &amp;quot;&amp;quot;&amp;quot;Return a foobang

      Optional plotz says to frobnicate the bizbaz first.
      &amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;对于仅有一行的文档字符串，结尾处的&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;应该也写在这一行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;版本注记-version-bookkeeping:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;版本注记(Version Bookkeeping)&lt;/h1&gt;

&lt;p&gt;如果你必须在源代码中包含Subversion, CVS或RCS crud，请这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__version__ = &amp;quot;$Revision$&amp;quot;
# $Source$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上几行的内容应当在模块的文档字符串之后，在其他代码之前，并且在其开始和结束都使用一个空行隔开。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>最长回文子串Longest palindromic substring的四种算法</title>
      <link>http://nanshu.wang/post/2015-08-11</link>
      <pubDate>Tue, 11 Aug 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-08-11</guid>
      <description>

&lt;h2 id=&#34;题目描述:3c5494d961990eaf70786ecbf6c7dfc4&#34;&gt;题目描述：&lt;/h2&gt;

&lt;p&gt;给定字符串$S$，求其最长的回文子串。&lt;/p&gt;

&lt;p&gt;Leetcode:&lt;a href=&#34;https://leetcode.com/problems/longest-palindromic-substring/&#34;&gt;https://leetcode.com/problems/longest-palindromic-substring/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面给出四种算法思路，分别是朴素枚举、动态规划、中心拓展和Manacher算法。&lt;/p&gt;

&lt;p&gt;其中，Manacher算法复杂度为$O(n)$&lt;/p&gt;

&lt;h2 id=&#34;1-朴素枚举:3c5494d961990eaf70786ecbf6c7dfc4&#34;&gt;1. 朴素枚举&lt;/h2&gt;

&lt;p&gt;时间复杂度：$O(n^3)$&lt;/p&gt;

&lt;p&gt;枚举$S$的所有子串，判断子串是否为回文字符串，取其中最长子串。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def palindrome1(s):
    n = len(s)
    ans = 0
    for i in range(n):
        for j in range(i+1,n):
            if j-i+1&amp;gt;ans and s[i:j]==s[j:i:-1]:
                ans = j-i+1
    return ans               
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-动态规划:3c5494d961990eaf70786ecbf6c7dfc4&#34;&gt;2. 动态规划&lt;/h2&gt;

&lt;p&gt;时间复杂度：$O(n^2)$&lt;/p&gt;

&lt;p&gt;注意到枚举子串并判断时产生了大量的重复判断，用状态$dp[i][j]$记录子串$s[i:j]$是否为回文字符串。枚举子串时按照子串长度的顺序进行枚举，若$s[i+1:j-1]$是回文串并且$s[i]==s[j]$，那么$s[i:j]$也是回文串。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def palindrome2(s):
    n = len(s)
    dp = [[False for i in range(n)]for i in range(n)]
    for i in range(n):
        dp[i][i] = True
        dp[i][i-1] = True
    ans = 0
    for l in range(2,n+1):
        for i in range(n-l+1):
            j = i+l-1
            if dp[i+1][j-1] and s[i]==s[j]:
                dp[i][j] = True
                ans = l
    return ans    
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-中心拓展:3c5494d961990eaf70786ecbf6c7dfc4&#34;&gt;3. 中心拓展&lt;/h2&gt;

&lt;p&gt;时间复杂度：$O(n^2)$&lt;/p&gt;

&lt;p&gt;回文串都是以中心对称的，因此可以枚举回文串的中心，再从中心向两边拓展找出当前中心的最长子串。注意中心可以是字符，也可以是字符之间的间隙，为了方便起见，将字符串的字符之间都插入一个特殊符号便于处理，例如&amp;rdquo;ABC&amp;rdquo;变为&amp;rdquo;&lt;em&gt;A_B_C&lt;/em&gt;&amp;ldquo;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def palindrome3(s):
    s = &amp;quot;&amp;quot;.join(map(lambda x:&#39;_&#39;+x,s))+&#39;_&#39;
    n = len(s)
    ans = 0
    for i in range(n):
        l = r = i
        while l-1 &amp;gt;= 0 and r+1 &amp;lt; n and s[l-1]==s[r+1]:
            l -= 1
            r += 1
        ans = max(ans,r-l+1)
    return ans/2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-manacher算法:3c5494d961990eaf70786ecbf6c7dfc4&#34;&gt;4. Manacher算法&lt;/h2&gt;

&lt;p&gt;时间复杂度：$O(n)$&lt;/p&gt;

&lt;p&gt;Manacher算法是由Manacher在1975年提出的，算法的思想也是从左到右依次计算以当前字符为中心的最大回文串，但在计算时利用了回文子串的特殊性质来减少计算量。&lt;/p&gt;

&lt;p&gt;假设已知$S$的两个回文子串$S1$，$S2$，其回文中心字符的位置分别为$c1$，$c2$（$c1$ &amp;lt; $c2$，$c1$在$c2$的左侧），并且$S1$又是$S2$的子串。由于$S2$是回文串，因此$S2$中的任一子串在以$c2$为对称轴的右侧也有完全相同的子串。根据这一性质，存在$S3$为$S2$的子串，并且$S3$和$S1$完全相同并以$c2$为对称轴对称。那么$S3$也同样是回文串，其对称中心为$2*c2-c1$。并且若$S1$不是$S2$的前缀，或者$S2$是$S$的后缀的情况下，以$2*c2-c1$对称的最大回文子串可以直接确定为在$S2$中与$S1$完全对称的子串。&lt;/p&gt;

&lt;p&gt;因此，当确定了当前中心$c1$的最大回文子串$S1$后，设其长度$l1$，向右去找在$S1$内可以直接确定的最大回文子串的对称中心，这些对称中心则可以直接跳过而不去计算。只有当回文串不能直接确定时，才需要进一步比较确定该中心的最大回文子串。&lt;/p&gt;

&lt;p&gt;完整的算法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#length[i] 表示以第i个字符为中心的最大回文串长度
1. 当前中心c = 1
2. 循环以下步骤直到c不合法：
    a. 从两端同时扩展得到以c为中心的最长回文子串，其长度为l,length[c] = l
    b. i = 1,2,..;当length[c-i]所代表的子串是length[c]的子串，且不为length[c]的前缀或者length[c]是原字符串的后缀时，length[c+i]可以直接确定为length[c-i]。即根据length[c-1],length[c-2]...的值来更新可以确定的最长回文子串长度length[c+1],length[c+2]...直到不能确定其回文长度的中心为c+i
    c. 令当前中心c = c+i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样也需要在字符中插入特殊字符在处理中心为间隙的情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def palindrome4(s):
    s = &amp;quot;&amp;quot;.join(map(lambda x:&#39;_&#39;+x,s))+&#39;_&#39;
    n = len(s)
    length = [0 for i in range(n)]
    c = 0
    while c &amp;lt; n:
        l = c-length[c]/2
        r = c+length[c]/2
        while l-1 &amp;gt;= 0 and r+1 &amp;lt; n and s[l-1]==s[r+1]:
            l -= 1
            r += 1
        length[c] = r-l+1
        i = 1
        # 能确定以c+i为中心的最长子串的长度
        while c-i &amp;gt;= 0 and c+i &amp;lt; n and c-i-length[c-i]/2&amp;gt;c-length[c]/2 or\
             (c-i-length[c-i]/2==c-length[c]/2 and c+length[c]/2==n-1):
            length[c+i] = length[c-i]
            i += 1
        # 不能确定以c+i为中心的最长子串的长度，但能确定其至少大于length[c-i]
        if c-i &amp;gt;= 0 and c+i &amp;lt; n and c-i-length[c-i]/2==c-length[c]/2:
            length[c+i] = length[c-i]
        c = c + i
    return max(length)/2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;时间复杂度证明：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;考虑内循环中字符的总比较次数，每次比较的右端字符都在前次比较过的右端字符的后面，也就是说右端字符并没有重复，而右端字符最多有$n$个位置，字符比较次数最多为$n$次&lt;/li&gt;
&lt;li&gt;考虑内循环中确定$length[c+i]$的次数，因为$length$最多有$n$个值，确定$length[c+i]$不会重复访问，因此确定$length[c+i]$最多为$n$次。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;综上，时间复杂度为$O(n)$&lt;/p&gt;

&lt;h1 id=&#34;参考:3c5494d961990eaf70786ecbf6c7dfc4&#34;&gt;参考&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Longest_palindromic_substring&amp;amp;action=edit&#34;&gt;Editing Longest palindromic substring - Wikipedia, the free encyclopedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.akalin.cx/longest-palindrome-linear-time&#34;&gt;Finding the Longest Palindromic Substring in Linear Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.csdn.net/linulysses/article/details/5634104&#34;&gt;Longest Palindrome (最长回文子串) - 夜鱼的专栏 - 博客频道 - CSDN.NET&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Python代码风格指南（一）代码设计(PEP8中文翻译)</title>
      <link>http://nanshu.wang/post/2015-07-04</link>
      <pubDate>Sat, 04 Jul 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-07-04</guid>
      <description>

&lt;p&gt;翻译自：&lt;a href=&#34;https://www.python.org/dev/peps/pep-0008/&#34;&gt;PEP 8 - Style Guide for Python Code&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;介绍-introduction:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;介绍(Introduction)&lt;/h1&gt;

&lt;p&gt;这篇文档说明了Python主要发行版中标准库代码所遵守的规范。请参考实现Python的C代码风格指南信息PEP。&lt;/p&gt;

&lt;p&gt;这篇文档和PEP 257(Docstring Conventions)都改编自Guido(译注：Python之父)最早的Python风格指南文章，并加入了Barry风格指南里的内容。&lt;/p&gt;

&lt;p&gt;语言自身在发生着改变，随着新的规范的出现和旧规范的过时，代码风格也会随着时间演变。&lt;/p&gt;

&lt;p&gt;很多项目都有自己的一套风格指南。若和本指南有任何冲突，应该优先考虑其项目相关的那套指南。&lt;/p&gt;

&lt;h1 id=&#34;保持盲目的一致是头脑简单的表现-a-foolish-consistency-is-the-hobgoblin-of-little-minds:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;保持盲目的一致是头脑简单的表现(A Foolish Consistency is the Hobgoblin of Little Minds)&lt;/h1&gt;

&lt;p&gt;(注：标题语出自Ralph Waldo Emerson, Hobgolin意指民间故事中友好但常制造麻烦的动物角色。)&lt;/p&gt;

&lt;p&gt;Guido的一个重要观点是代码被读的次数远多于被写的次数。这篇指南旨在提高代码的可读性，使浩瀚如烟的Python代码风格能保持一致。正如PEP 20那首《Zen of Python》的小诗里所说的：“可读性很重要(Readability counts)”。&lt;/p&gt;

&lt;p&gt;这本风格指南是关于一致性的。同风格指南保持一致性是重要的，但是同项目保持一致性更加重要，同一个模块和一个函数保持一致性则最为重要。&lt;/p&gt;

&lt;p&gt;然而最最重要的是：要知道何时去违反一致性，因为有时风格指南并不适用。当存有疑虑时，请自行做出最佳判断。请参考别的例子去做出最好的决定。并且不要犹豫，尽管提问。&lt;/p&gt;

&lt;p&gt;特别的：千万不要为了遵守这篇PEP而破坏向后兼容性！&lt;/p&gt;

&lt;p&gt;如果有以下借口，则可以忽略这份风格指南：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;当采用风格指南时会让代码更难读，甚至对于习惯阅读遵循这篇PEP的代码的人来说也是如此。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;需要和周围的代码保持一致性，但这些代码违反了指南中的风格（可是时历史原因造成的）——尽管这可能也是一个收拾别人烂摊子的机会（True in XP style?）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;若是有问题的某段代码早于引入指南的时间，那么没有必要去修改这段代码。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;代码需要和更旧版本的Python保持兼容，而旧版本的Python不支持风格指南所推荐的特性。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;代码设计-code-lay-out:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;代码设计(Code lay-out)&lt;/h1&gt;

&lt;h2 id=&#34;缩进-indentation:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;缩进(Indentation)&lt;/h2&gt;

&lt;p&gt;每个缩进级别采用4个空格。&lt;/p&gt;

&lt;p&gt;连续行所包装的元素应该要么采用Python隐式续行，即垂直对齐于圆括号、方括号和花括号，要么采用*悬挂缩进(hanging indent)*。采用悬挂缩进时需考虑以下两点：第一行不应该包括参数，并且在续行中需要再缩进一级以便清楚表示。&lt;/p&gt;

&lt;p&gt;正确的例子:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 同开始分界符(左括号)对齐
foo = long_function_name(var_one, var_two,
                         var_three, var_four)

# 续行多缩进一级以同其他代码区别
def long_function_name(
        var_one, var_two, var_three,
        var_four):
    print(var_one)

# 悬挂缩进需要多缩进一级
foo = long_function_name(
    var_one, var_two,
    var_three, var_four)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;错误的例子:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 采用垂直对齐时第一行不应该有参数
foo = long_function_name(var_one, var_two,
    var_three, var_four)

# 续行并没有被区分开，因此需要再缩进一级
def long_function_name(
    var_one, var_two, var_three,
    var_four):
    print(var_one)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于续行来说，4空格的规则可以不遵守。&lt;/p&gt;

&lt;p&gt;同样可行的例子:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 悬挂缩进可以不采用4空格的缩进方法。
foo = long_function_name(
  var_one, var_two,
  var_three, var_four)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;多行if语句&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果&lt;code&gt;if&lt;/code&gt;语句太长，需要用多行书写，2个字符(例如,&lt;code&gt;if&lt;/code&gt;)加上一个空格和一个左括号刚好是4空格的缩进，但这对多行条件语句的续行是没用的。因为这会和&lt;code&gt;if&lt;/code&gt;语句中嵌套的其他的缩进的语句产生视觉上的冲突。这份PEP中并没有做出明确的说明应该怎样来区分条件语句和&lt;code&gt;if&lt;/code&gt;语句中所嵌套的语句。以下几种方法都是可行的，但不仅仅只限于这几种方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 不采用额外缩进
if (this_is_one_thing and
    that_is_another_thing):
    do_something()

# 增加一行注释，在编辑器中显示时能有所区分
# supporting syntax highlighting.
if (this_is_one_thing and
    that_is_another_thing):
    # Since both conditions are true, we can frobnicate.
    do_something()

# 在条件语句的续行增加一级缩进
if (this_is_one_thing
        and that_is_another_thing):
    do_something()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;多行结束右圆/方/花括号可以单独一行书写，和上一行的缩进对齐：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my_list = [
    1, 2, 3,
    4, 5, 6,
    ]
result = some_function_that_takes_arguments(
    &#39;a&#39;, &#39;b&#39;, &#39;c&#39;,
    &#39;d&#39;, &#39;e&#39;, &#39;f&#39;,
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以和多行开始的第一行的第一个字符对齐：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my_list = [
    1, 2, 3,
    4, 5, 6,
]
result = some_function_that_takes_arguments(
    &#39;a&#39;, &#39;b&#39;, &#39;c&#39;,
    &#39;d&#39;, &#39;e&#39;, &#39;f&#39;,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tab还是空格-tab-or-space:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;Tab还是空格？(Tab or space?)&lt;/h2&gt;

&lt;p&gt;推荐使用空格来进行缩进。&lt;/p&gt;

&lt;p&gt;Tab应该只在现有代码已经使用tab进行缩进的情况下使用，以便和现有代码保持一致。&lt;/p&gt;

&lt;p&gt;Python 3不允许tab和空格混合使用。&lt;/p&gt;

&lt;p&gt;Python 2的代码若有tab和空格混合使用的情况，应该把tab全部转换为只有空格。&lt;/p&gt;

&lt;p&gt;当使用命令行运行Python 2时，使用&lt;code&gt;-t&lt;/code&gt;选项，会出现非法混用tab和空格的警告。当使用&lt;code&gt;-tt&lt;/code&gt;选项时，这些警告会变成错误。强烈推荐使用这些选项！&lt;/p&gt;

&lt;h2 id=&#34;每行最大长度-maximum-line-length:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;每行最大长度(Maximum Line Length)&lt;/h2&gt;

&lt;p&gt;将所有行都限制在79个字符长度以内。&lt;/p&gt;

&lt;p&gt;对于连续大段的文字（比如文档字符串(docstring)或注释），其结构上的限制更少，这些行应该被限制在72个字符长度内。&lt;/p&gt;

&lt;p&gt;限制编辑器的窗口宽度能让好几个文件同时打开在屏幕上显示，在使用代码评审(code review)工具时在两个相邻窗口显示两个版本的代码效果很好。&lt;/p&gt;

&lt;p&gt;很多工具的默认自动换行会破坏代码的结构，使代码更难以理解。在窗口大小设置为80个字符的编辑器中，即使在换行时编辑器可能会在最后一列放置一个记号，为避免自动换行也需要限制每行字符长度。一些基于web的工具可能根本没有自动换行的功能。&lt;/p&gt;

&lt;p&gt;一些团队会强烈希望行长度比79个字符更长。当代码仅仅只由一个团队维护时，可以达成一致让行长度增加到80到100字符(实际上最大行长是99字符)，注释和文档字符串仍然是以72字符换行。&lt;/p&gt;

&lt;p&gt;Python标准库比较传统，将行长限制在79个字符以内（文档字符串/注释为72个字符）。&lt;/p&gt;

&lt;p&gt;一种推荐的换行方式是利用Python圆括号、方括号和花括号中的隐式续行。长行可以通过在括号内换行来分成多行。应该最好加上反斜杠来区别续行。&lt;/p&gt;

&lt;p&gt;有时续行只能使用反斜杠才。例如，较长的多个&lt;code&gt;with&lt;/code&gt;语句不能采用隐式续行，只能接受反斜杠表示换行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with open(&#39;/path/to/some/file/you/want/to/read&#39;) as file_1, \
     open(&#39;/path/to/some/file/being/written&#39;, &#39;w&#39;) as file_2:
    file_2.write(file_1.read())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;（参照前面关于 &lt;code&gt;多行if语句&lt;/code&gt;的讨论来进一步考虑这里&lt;code&gt;with&lt;/code&gt;语句的缩进。）&lt;/p&gt;

&lt;p&gt;另一个这样的例子是&lt;code&gt;assert&lt;/code&gt;语句。&lt;/p&gt;

&lt;p&gt;要确保续行的缩进适当。逻辑运算符附近的换行处最好是在运算符&lt;strong&gt;之后&lt;/strong&gt;，而不是在其之前。来看一些例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Rectangle(Blob):

    def __init__(self, width, height,
                 color=&#39;black&#39;, emphasis=None, highlight=0):
        if (width == 0 and height == 0 and
                color == &#39;red&#39; and emphasis == &#39;strong&#39; or
                highlight &amp;gt; 100):
            raise ValueError(&amp;quot;sorry, you lose&amp;quot;)
        if width == 0 and height == 0 and (color == &#39;red&#39; or
                                           emphasis is None):
            raise ValueError(&amp;quot;I don&#39;t think so -- values are %s, %s&amp;quot; %
                             (width, height))
        Blob.__init__(self, width, height,
                      color, emphasis, highlight)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;空行-blank-line:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;空行(Blank line)&lt;/h2&gt;

&lt;p&gt;使用2个空行来分隔最高级的函数(function)和类(class)定义。&lt;/p&gt;

&lt;p&gt;使用1个空行来分隔类中的方法(method)定义。&lt;/p&gt;

&lt;p&gt;（尽量少地）使用额外的空行来分隔一组相关的函数。在一系列相关的仅占一行的函数之间，空格也可以被省略，比如一组dummy实现。&lt;/p&gt;

&lt;p&gt;在函数内（尽量少地）使用空行使代码逻辑更清晰。&lt;/p&gt;

&lt;p&gt;Python支持control-L（如:^L）换页符作为空格；许多工具将这些符号作为分页符，因此你可以使用这些符号来分页或者区分文件中的相关区域。注意，一些编辑器和基于web的代码预览器可能不会将control-L识别为分页符，而是显示成其他符号。&lt;/p&gt;

&lt;h2 id=&#34;源文件编码-source-file-encoding:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;源文件编码(Source File Encoding)&lt;/h2&gt;

&lt;p&gt;Python核心发行版中的代码应该一直使用UTF-8（Python 2中使用ASCII）。&lt;/p&gt;

&lt;p&gt;使用ASCII（Python 2）或者UTF-8（Python 3）的文件不应该添加编码声明。&lt;/p&gt;

&lt;p&gt;在标准库中，只有用作测试目的，或者注释或文档字符串需要提及作者名字而不得不使用非ASCII字符时，才能使用非默认的编码。否则，在字符串文字中包括非ASCII数据时，推荐使用&lt;code&gt;\x&lt;/code&gt;, &lt;code&gt;\u&lt;/code&gt;, &lt;code&gt;\U&lt;/code&gt;或&lt;code&gt;\N&lt;/code&gt;等转义符。&lt;/p&gt;

&lt;p&gt;对于Python 3.0及其以后的版本中，标准库遵循以下原则（参见PEP 3131）：Python标准库中的所有标识符都&lt;strong&gt;必须&lt;/strong&gt;只采用ASCII编码的标识符，在可行的条件下也&lt;strong&gt;应当&lt;/strong&gt;使用英文词（很多情况下，使用的缩写和技术术语词都不是英文）。此外，字符串文字和注释应该只包括ASCII编码。只有两种例外：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(a) 测试情况下为了测试非ASCII编码的特性&lt;/li&gt;
&lt;li&gt;(b) 作者名字。作者名字不是由拉丁字母组成的也&lt;strong&gt;必须&lt;/strong&gt;提供一个拉丁音译名。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;鼓励面向全世界的开源项目都采用类似的原则。&lt;/p&gt;

&lt;h2 id=&#34;imports:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;Imports&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Imports应该分行写，而不是都写在一行，例如：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;# 分开写
import os
import sys
# 不要像下面一样写在一行
import sys, os
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样写也是可以的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from subprocess import Popen, PIPE
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Imports应该写在代码文件的开头，位于模块(module)注释和文档字符串之后，模块全局变量(globals)和常量(constants)声明之前。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Imports应该按照下面的顺序分组来写：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;标准库imports&lt;/li&gt;
&lt;li&gt;相关第三方imports&lt;/li&gt;
&lt;li&gt;本地应用/库的特定imports&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不同组的imports之前用空格隔开。&lt;/p&gt;

&lt;p&gt;将任何相关的&lt;code&gt;__all__&lt;/code&gt;说明(specification)放在imports之后。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;推荐使用绝对(absolute)imports，因为这样通常更易读，在import系统没有正确配置（比如中的路径以&lt;code&gt;sys.path&lt;/code&gt;结束）的情况下，也会有更好的表现（或者至少会给出错误信息）：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;import mypkg.sibling
from mypkg import sibling
from mypkg.sibling import example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然而，除了绝对imports，显式的相对imports也是一种可以接受的替代方式。特别是当处理复杂的包布局(package layouts)时，采用绝对imports会显得啰嗦。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from . import sibling
from .sibling import example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Standard library code should avoid complex package layouts and always
  use absolute imports.
  标准库代码应当一直使用绝对imports，避免复杂的包布局。&lt;/p&gt;

&lt;p&gt;隐式的相对imports应该&lt;strong&gt;永不&lt;/strong&gt;使用，并且Python 3中已经被去掉了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当从一个包括类的模块中import一个类时，通常可以这样写：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;from myclass import MyClass
from foo.bar.yourclass import YourClass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果和本地命名的拼写产生了冲突，应当直接import模块：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import myclass
import foo.bar.yourclass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用&amp;rdquo;myclass.MyClass&amp;rdquo;和&amp;rdquo;foo.bar.yourclass.YourClass&amp;rdquo;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;避免使用通配符imports(&lt;code&gt;from &amp;lt;module&amp;gt; import *&lt;/code&gt;)，因为会造成在当前命名空间出现的命名含义不清晰，给读者和许多自动化工具造成困扰。有一个可以正当使用通配符import的情形，即将一个内部接口重新发布成公共API的一部分（比如，使用备选的加速模块中的定义去覆盖纯Python实现的接口，被覆盖的定义恰好在不能提前知晓）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当使用这种方式重新发布命名时，指南后面关于公共和内部接口的部分仍然适用。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习第一战——阿里天池移动推荐算法比赛经验总结攻略</title>
      <link>http://nanshu.wang/post/2015-07-01</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-07-01</guid>
      <description>

&lt;p&gt;历时98天的&lt;a href=&#34;http://tianchi.aliyun.com/competition/introduction.htm?spm=5176.100066.333.5.0n5wpl&amp;amp;raceId=1&#34;&gt;阿里移动推荐算法&lt;/a&gt;终于结束了，有种终于下了贼船的感觉。总的来说，整个比赛的体验并不好：时间太长，资源不够，运气成分大，学到的干货少。虽然学到的干货不多，但这毕竟是在Data Science道路上进军的第一场实战，还是有必要好好总结一下。&lt;/p&gt;

&lt;h1 id=&#34;比赛统计:e22b26d8c6b3a4529619561860cf7538&#34;&gt;比赛统计&lt;/h1&gt;

&lt;p&gt;初赛名次：21&lt;/p&gt;

&lt;p&gt;复赛名次：22&lt;/p&gt;

&lt;p&gt;Python代码行数：2320&lt;/p&gt;

&lt;p&gt;SQL代码行数：6656&lt;/p&gt;

&lt;p&gt;天池平台数据表数：1006&lt;/p&gt;

&lt;p&gt;线下结果数：490&lt;/p&gt;

&lt;h1 id=&#34;初识比赛:e22b26d8c6b3a4529619561860cf7538&#34;&gt;初识比赛&lt;/h1&gt;

&lt;p&gt;虽然比赛的名字叫做移动推荐算法，但本质上也是一个机器学习的问题。比赛给出了一个月（2014.11.18~2014.12.18）的用户对商品操作的行为数据，需要预测12.19号这天的购买行为，评分采用经典的F1值计算。行为数据中包括了时间、地点、用户、商品、行为类型、商品类别六个要素，特征提取的思路就是围绕这六个要素进行。题目还给出了一个商品子集，只需要提交对商品子集购买行为的预测结果。&lt;/p&gt;

&lt;p&gt;大致看了去年前十选手的比赛总结，比赛流程大致分为&lt;strong&gt;4个模块&lt;/strong&gt;：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;特征提取&lt;/li&gt;
&lt;li&gt;训练集构造&lt;/li&gt;
&lt;li&gt;模型学习调参&lt;/li&gt;
&lt;li&gt;模型融合&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这4个模块并不是相互独立的，比如特征提取和训练集构造通常是一起完成，构造训练集也会用到简单模型学习来平衡正负样本数。&lt;/p&gt;

&lt;p&gt;比赛分为&lt;strong&gt;两个赛季&lt;/strong&gt;：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初赛3.20-4.25，仅提供1万用户数据，采用本地调试提交。&lt;/li&gt;
&lt;li&gt;复赛4.30-7.1，有500万用户数据，使用阿里天池平台提交。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;初赛和复赛是需要区别对待的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;工具&lt;/strong&gt;：初赛本地调试，学习模型和特征提取并不拘泥，可以用任意熟悉的工具。然而复赛需要使用天池平台，工具受限制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型or规则？&lt;/strong&gt;初赛数据量很小，很可能模型效果并不好，事实证明，规则实际上是简单粗暴效果佳的。复赛则不可能寄希望于规则，老老实实搞模型吧。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时间分配&lt;/strong&gt;：初赛只用进前500名即可，不用投入太多的精力。到复赛前期越快熟悉环境越好，尽量前期多做，比赛资源并不充足，后期资源会非常有限，速度很慢。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;初赛:e22b26d8c6b3a4529619561860cf7538&#34;&gt;初赛&lt;/h1&gt;

&lt;p&gt;初赛的环境使用了MySQL+Python，选择Python的原因是数据挖掘的包很齐全也容易上手：numpy、scipy、pandas，还有机器学习包scikit-learn，对付比赛足够用了。&lt;/p&gt;

&lt;h2 id=&#34;特征提取:e22b26d8c6b3a4529619561860cf7538&#34;&gt;特征提取&lt;/h2&gt;

&lt;p&gt;根据行为数据的各个字段，很容易可以将特征分为以下&lt;strong&gt;5类&lt;/strong&gt;：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;用户-商品UI特征&lt;/li&gt;
&lt;li&gt;用户特征&lt;/li&gt;
&lt;li&gt;商品特征&lt;/li&gt;
&lt;li&gt;类别特征&lt;/li&gt;
&lt;li&gt;地理位置特征&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;特征类别主要是&lt;strong&gt;计数&lt;/strong&gt;和&lt;strong&gt;4行为转化率&lt;/strong&gt;，比如用户1行为计数，4行为计数/1行为计数等等。&lt;/p&gt;

&lt;p&gt;第一赛季并没有想到时间特征，只是根据时间划分训练集，第二赛季才发现时间特征非常重要。&lt;/p&gt;

&lt;p&gt;为了便于特征提取，我用两种方法存放数据，一种是存入SQL表中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;建立表logUser，user_id为主键,
字段名：
user_id,item_start,item_end

建立表logItem,item_idx为主键，
字段名：
item_idx,item_id,bhv_start,bhv_end

建立表logBhv,bhv_idx为主键，
字段名：
bhv_idx, behavior_type,user_geohash,item_category,time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样存放可以方便地提取UI特征，但事实证明这种方法实际上也没有很快，因为需要大量的SQL查询操作。幸而第一赛季数据量并不大，提取训练集并不是瓶颈。&lt;/p&gt;

&lt;p&gt;第二种方法是将数据按天分开，便于提取用户、商品、地理、类别的全局特征，这部分特征我全部使用了awk脚本提取。&lt;/p&gt;

&lt;h2 id=&#34;构造训练集:e22b26d8c6b3a4529619561860cf7538&#34;&gt;构造训练集&lt;/h2&gt;

&lt;p&gt;线下训练集使用12.18号购买行为标注样本，严格使用18号之前的数据提取特征（包括全局特征），样本为18号之前n天有交互行为的UI对。
线上预测集则使用19号之前n天所有的交互行为的UI对。&lt;/p&gt;

&lt;p&gt;由于正例负例不平衡，需要对训练集进行抽样，抽样比例是在1:5到1:20这个区间里找最优结果。&lt;/p&gt;

&lt;h2 id=&#34;模型训练:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型训练&lt;/h2&gt;

&lt;p&gt;主要使用了Adaboost，Random forrest，Logistic Regression三种模型，直接调用Scikit-lSearn提供的模型训练。其中树型模型的测试结果最好。&lt;/p&gt;

&lt;p&gt;然而第一赛季受到数据量的限制，实际上规则比模型好用多了，比如购物车+时间规则：前一天晚上8点后加入购入车的UI，就可以做到比单个模型结果好很多。&lt;/p&gt;

&lt;h2 id=&#34;模型融合:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型融合&lt;/h2&gt;

&lt;p&gt;第一赛季比赛前期一直死磕模型，成绩一直提不上去，改成规则后才大呼坑爹。&lt;/p&gt;

&lt;p&gt;初赛的最优成绩使用了规则+模型混合的方法，使用规则得到候选集后，再从候选集中去掉多个模型预测按概率排序后的top k负例交集。&lt;/p&gt;

&lt;h1 id=&#34;复赛:e22b26d8c6b3a4529619561860cf7538&#34;&gt;复赛&lt;/h1&gt;

&lt;p&gt;第二赛季要使用天池平台，第一赛季的代码都用不上，得全部推翻重写。天池平台的工具有SQL、Map Reduce、UDF，算法平台也提供了经典的机器学习模型。&lt;/p&gt;

&lt;p&gt;SQL容易学习，但写起来复杂容易出错，没有可读性，不提供参数设置，复用比较麻烦。Map Reduce和UDF学习成本高，可以实现复杂逻辑，但天池对这两个工具的限制很多，文档对用户非常不友好。&lt;/p&gt;

&lt;p&gt;大部分特征、训练集和模型基本需求如计算f1等等都是通过SQL实现，少部分特征和其他复杂功能使用了UDF和Map Reduce。比较有用的是Map Reduce实现了特征Information Gain的计算，用来进行特征筛选，这要感谢会Java的队友/男票&lt;a href=&#34;http://www.xgezhang.com&#34;&gt;xge&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;特征提取-1:e22b26d8c6b3a4529619561860cf7538&#34;&gt;特征提取&lt;/h2&gt;

&lt;p&gt;整个复赛的特征工程一共进行了10次更改，更改大多是增加新特征，从最初的22维增加到最后的380维。特征构建还是一赛季的思路，不过根据一赛季的结果，筛选掉了一部分无用特征。除了简单计数和4行为转化特征，在第一赛季基础上增加的特征有：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;时间特征

&lt;ul&gt;
&lt;li&gt;UI行为距离标注日的小时数&lt;/li&gt;
&lt;li&gt;用时间衰减来计算加权后的行为计数&lt;/li&gt;
&lt;li&gt;多次行为操作之间的时间间隔(Map Reduce实现)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;不同时间粒度的UI特征

&lt;ul&gt;
&lt;li&gt;按照标注日前1天，3天，7天，30天为粒度提取&lt;/li&gt;
&lt;li&gt;按照标注日前4小时，8小时，16小时为粒度提取&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加地理特征

&lt;ul&gt;
&lt;li&gt;用户商品的距离&lt;/li&gt;
&lt;li&gt;用户地理用最后操作位置填充&lt;/li&gt;
&lt;li&gt;商品地理用购买该商品的用户位置填充&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;计数特征去重

&lt;ul&gt;
&lt;li&gt;商品特征计数对用户去重&lt;/li&gt;
&lt;li&gt;类别特征计数对商品去重&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加交叉特征

&lt;ul&gt;
&lt;li&gt;用户&amp;amp;UI特征交叉&lt;/li&gt;
&lt;li&gt;商品&amp;amp;UI特征交叉&lt;/li&gt;
&lt;li&gt;类别&amp;amp;UI特征交叉&lt;/li&gt;
&lt;li&gt;用户&amp;amp;商品特征交叉&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;特征增加对分数提升是最显著的&lt;/strong&gt;，在特征选择中也需要注意：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;不要盲目添加大量特征&lt;/strong&gt;，最好一部分一部分添加，这样会对添加的特征效果有个大体的认识。&lt;/li&gt;
&lt;li&gt;添加的特征要能从现实逻辑上解释得通，最好是&lt;strong&gt;直观上影响购买行为的特征&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;特征筛选或许有用，但最好添加特征时就尽量加入有用的特征，&lt;strong&gt;不要妄图从一大堆特征中再筛选&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;训练集构造:e22b26d8c6b3a4529619561860cf7538&#34;&gt;训练集构造&lt;/h2&gt;

&lt;p&gt;构造训练集前对数据进行了一个初步的统计，第二天的购买行为中仅有32%的UI是有过交互的，剩下64%的购买行为都是当天的偶发购买。前1天有过交互的占16%，前2天21%，前2天23%，前4天25%。也就是说，有一半的正样本是前1天的，因此&lt;strong&gt;仅仅使用前1天的交互UI构造训练集&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;考虑到仅仅需要对商品子集进行预测，为了使训练集和预测集保持一致，又可以减少训练集的规模，所以&lt;strong&gt;只使用了商品子集内的商品构造训练集&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;在比赛前期，这样构造训练集是有效的，缩短了模型训练时间，加快了特征迭代的工作。但是当特征增加不能再提升分数时，应该想到，这样构造训练集存在很大的问题，丢掉了很多正例样本。和其他队伍交流来看，这实际上是比赛后期的一个瓶颈，限制了特征的发挥。&lt;/p&gt;

&lt;p&gt;正确的做法是：在特征工程比较完善之后，最好是比赛中期，使用不止1天的交互和商品全集构造训练集，采用简单模型过滤掉大量负样本。这点我到比赛后期才开始改，平台资源已经不够了，所以并没有做好，也是比赛的一大遗憾。&lt;/p&gt;

&lt;p&gt;可以通过不同的标注日来得到不同的训练集，注意要&lt;strong&gt;预留出相同规模的验证集和测试集&lt;/strong&gt;，到模型融合时会有用。&lt;/p&gt;

&lt;h2 id=&#34;模型学习-调参:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型学习+调参&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;
逻辑回归（Logistic Regression）训练速度预测速度很快，预测效果较差，可以用来进行负样本筛选，模型融合也有用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;随机森林&lt;/strong&gt;
随机森林学习速度一般，预测速度很慢，主要用于模型融合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GBDT二分类&lt;/strong&gt;
GBDT训练速度慢，预测速度一般，单模型的效果最优。唯一的调参经验是：树越多越好，但速度也越慢。并且特征不一样，最优的参数也不一样，调参时注意每次最好只改变一个参数值，否则不容易看出参数的影响。不要使用枚举参数的办法调参，太浪费时间。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;模型融合-1:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型融合&lt;/h2&gt;

&lt;p&gt;总结一下道听途说模型融合的方法：
1. 取不同模型top k并集
2. 取不同模型top k交集
3. 直接将预测概率相加
4. 将预测概率作为特征用lr训练
5. 将预测概率作为特征加入验证集，再将验证集当做训练集训练。&lt;/p&gt;

&lt;p&gt;对我来说以上5种方法都是然并卵，不论使用哪种融合方式，总没有单模型的结果好。具体的原因还没有找到，或许是因为没有对训练集进行抽样，GBDT模型对正负样本比例并不敏感，所以我直接用了所有的负样本，也算是这次比赛的第二大遗憾吧。&lt;/p&gt;

&lt;h1 id=&#34;其他收获:e22b26d8c6b3a4529619561860cf7538&#34;&gt;其他收获&lt;/h1&gt;

&lt;p&gt;与其说是收获，不如说是一些犯过的错误，总结出来自勉：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;不要过早优化&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;主要是模型调参和模型融合，这两部分应当留到比赛中后期来做，过早优化耽误时间也没有意义。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;不要短视&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;比赛每天都在更新名次，但不要只关注短期的分数，更要为长期做打算。比如虽然调参、获得更多的训练集会提升暂时的分数，但是也失去了快速迭代特征的机会。要时刻记住这是一场长时间的比赛，终点才是胜利，中间领跑并不说明问题。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;合理安排时间精力&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;初赛不需要投入全部精力，复赛前期尽量多做，否则后期没有资源。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;不要妄下结论&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于不确定的猜想，一定要用数据验证，比如训练集用商品全集还是子集的区别，是否需要抽样等等。尽量多试多做，要去尝试各种可能性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;不要懒，不要懒，不要懒&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;重要的事情说三遍。有时会觉得麻烦，于是采用了写起来简单但效率低的方法。有时明明应该改成另一种方法，又觉得不改也没影响就算了吧。有时觉得今天太晚了就不做了，马虎总是不检查提交结果，于是又浪费了很多次机会。其实这种时候就会感觉，有个队友还是挺好的。&lt;/p&gt;

&lt;h1 id=&#34;后记:e22b26d8c6b3a4529619561860cf7538&#34;&gt;后记&lt;/h1&gt;

&lt;p&gt;比赛最初的期望是进复赛，进入复赛后又期望进前50，进入前50后又期望进首页，进入首页又期望进前十，最好的时候也排到过第二名。但是比赛后期确实陷入了瓶颈，主要是模型融合和训练集的构造，还是挺遗憾的。虽说如此，但还是超预期完成了目标，也是值得开心一下。貌似赢的积分可以拿去换个杯子T恤和音响，为了身心健康，也决定不再参加阿里天池的其他比赛了，第二战打算去搞搞Kaggle。&lt;/p&gt;

&lt;p&gt;最后，衷心感谢拉我参赛从头到尾互为小号没有放弃的&lt;a href=&#34;http://www.libaier.net&#34;&gt;Libaier&lt;/a&gt;战友，长时间占据榜首传授高招的&lt;strong&gt;江神&lt;/strong&gt;，以闺蜜幸福为代价出售特征的闺蜜男票&lt;strong&gt;Xuhuan&lt;/strong&gt;童鞋，以及男票兼打酱油队友&lt;a href=&#34;http://www.xgezhang.com&#34;&gt;xge&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;完。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记7 高偏差/低偏差，学习曲线，模型选择</title>
      <link>http://nanshu.wang/post/2015-05-17</link>
      <pubDate>Sun, 17 May 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-05-17</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;p&gt;原文：&lt;a href=&#34;https://share.coursera.org/wiki/index.php/ML:Advice_for_Applying_Machine_Learning&#34;&gt;https://share.coursera.org/wiki/index.php/ML:Advice_for_Applying_Machine_Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;面对一个机器学习问题，我们提取好特征，挑选好训练集，选择一种机器学习算法，然后学习预测得到了第一步结果。然而我们不幸地发现，在测试集上的准确率低得离谱，误差高得吓人，要提高准确率、减少误差的话，下一步该做些什么呢？&lt;/p&gt;

&lt;p&gt;可以采用以下的方法来减少预测的误差：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获得更多的训练样本&lt;/li&gt;
&lt;li&gt;减少特征的数量&lt;/li&gt;
&lt;li&gt;增加特征的数量&lt;/li&gt;
&lt;li&gt;使用多项式特征&lt;/li&gt;
&lt;li&gt;增大或减小正则化参数$\lambda$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但不要盲目在这些可行的方法里随便选一种来提升模型，需要用一些诊断模型的技术来帮助我们选择使用哪种策略。&lt;/p&gt;

&lt;h1 id=&#34;1-评估假设:876321dc83c64489eb74f98905ca718c&#34;&gt;1.评估假设&lt;/h1&gt;

&lt;p&gt;即使模型假设对于训练集的误差很低，若存在过拟合，模型的预测也同样会不准确。&lt;/p&gt;

&lt;p&gt;给定一份训练集，我们可以将数据分成两部分：训练集和测试集。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;使用训练集最小化$J(\Theta)$得到$\Theta$参数&lt;/li&gt;
&lt;li&gt;计算测试集的误差：&lt;/li&gt;
&lt;/ol&gt;

&lt;div&gt;
$$J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$$
&lt;/div&gt;

&lt;p&gt;3.计算分类错误率（即0/1分类错误率）&lt;/p&gt;

&lt;div&gt;
$$err(h_\Theta(x),y) =
\begin{matrix}
1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline
0 &amp; \mbox otherwise 
\end{matrix}$$
&lt;/div&gt;

&lt;p&gt;测试集的平均误差为：&lt;/p&gt;

&lt;div&gt;
$$\large
\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})$$
&lt;/div&gt;

&lt;p&gt;也就是测试集上分类错误的样本的比例。&lt;/p&gt;

&lt;h1 id=&#34;2-模型选择与训练-验证-测试集:876321dc83c64489eb74f98905ca718c&#34;&gt;2.模型选择与训练/验证/测试集&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;学习算法若仅仅对训练集拟合较好，并不能说明其假设也是好的。&lt;/li&gt;
&lt;li&gt;训练集上的假设误差通常要比其他数据集上得到的误差要小。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了在假设上选择模型，可以测试模型的多项式的次数来观察误差结果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;无验证集&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对不同的多项式次数的模型通过训练集得到最优化参数$\Theta$。&lt;/li&gt;
&lt;li&gt;找到在预测集上误差最小的模型的多项式次数$d$。&lt;/li&gt;
&lt;li&gt;使用测试集估计泛化误差$J_{test}(\Theta^{(d)})$。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在这个例子中，我们用测试集训练得到的一个变量，即多项式次数$d$，但这样做会使其他数据集的误差更大。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，我们引入了第三种数据集，即交叉验证集(Cross Validation Set)，来作为选择$d$的中间数据集。这样，测试集会给出一个准确，非乐观估计的误差结果。&lt;/p&gt;

&lt;p&gt;例如，将数据集分成三份：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;训练集：60%&lt;/li&gt;
&lt;li&gt;交叉验证集：20%&lt;/li&gt;
&lt;li&gt;测试集：20%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于这三个数据集我们可以计算三个不同误差值：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;有验证集&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对不同的多项式次数的模型通过训练集得到最优化参数$\Theta$。&lt;/li&gt;
&lt;li&gt;找到在验证集上误差最小的模型的多项式次数$d$。&lt;/li&gt;
&lt;li&gt;使用测试集估计泛化误差$J_{test}(\Theta^{(d)})$。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;使用验证集则避免了使用测试集来确定多项式次数$d$。&lt;/p&gt;

&lt;h1 id=&#34;3-诊断偏差-vs-方差:876321dc83c64489eb74f98905ca718c&#34;&gt;3.诊断偏差 vs. 方差&lt;/h1&gt;

&lt;p&gt;我们来讨论一下多项式次数$d$和过拟合以及欠拟合之间的关系。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;我们需要区分导致预测结果差的原因是偏差还是方差。&lt;/li&gt;
&lt;li&gt;高偏差也就是欠拟合，高方差也就是过拟合。我们需要在这两者之间找到一个黄金分割。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;随着多项式次数$d$的增加，训练集的误差会&lt;strong&gt;减少&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;同时，交叉验证集的误差会随着$d$的增加而&lt;strong&gt;减少&lt;/strong&gt;，但在$d$增加到某一点之后，会随着$d$的增加而&lt;strong&gt;增加&lt;/strong&gt;，形成一个凸曲线&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高偏差（欠拟合）：$J_{train}(\Theta)$和$J_{CV}(\Theta)$都较高，并且$J_{CV}(\Theta) \approx J_{train}(\Theta)$。&lt;/li&gt;
&lt;li&gt;高方差（过拟合）：$J_{train}(\Theta)$较低，且$J_{CV}(\Theta)$比$J_{train}(\Theta)$高得多。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以用下图来表示：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/300px-Features-and-polynom-degree.png&#34; alt=&#34;Features-and-polynom-degree&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;4-正则化和偏差-方差:876321dc83c64489eb74f98905ca718c&#34;&gt;4.正则化和偏差/方差&lt;/h1&gt;

&lt;p&gt;下面来分析正则化参数$\lambda$。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda$较大：高偏差（欠拟合）&lt;/li&gt;
&lt;li&gt;$\lambda$不大不小：正好&lt;/li&gt;
&lt;li&gt;$\lambda$较小：高方差（过拟合）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;较大的$\lambda$参数会惩罚$\Theta$参数，即简单化结果函数的曲线，造成欠拟合。&lt;/p&gt;

&lt;p&gt;$\lambda$和训练集以及验证集的关系如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda$较小：$J_{train}(\Theta)$较低，且$J_{CV}(\Theta)$较高（高方差/过拟合）。&lt;/li&gt;
&lt;li&gt;$\lambda$不大不小：$J_{train}(\Theta)$和$J_{CV}(\Theta)$都较低，并且$J_{CV}(\Theta) \approx J_{train}(\Theta)$。&lt;/li&gt;
&lt;li&gt;$\lambda$较大：$J_{train}(\Theta)$和$J_{CV}(\Theta)$都较高（高偏差/欠拟合）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图说明了$\lambda$值和假设之间的关系：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/300px-Features-and-lambda.png&#34; alt=&#34;Features-and-lambda&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;为了选择模型和正则化参数$lambda$，我们需要：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;列出$\lambda$测试的值，比如 $\lambda \in \lbrace0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24\rbrace$；&lt;/li&gt;
&lt;li&gt;选择一个$\lambda$的值进行计算；&lt;/li&gt;
&lt;li&gt;创建模型集，比如按照多项式次数或其他指标来创建；&lt;/li&gt;
&lt;li&gt;选择一个模型来学习$\Theta$值；&lt;/li&gt;
&lt;li&gt;用所选的模型学习得到$\Theta$值，使用选择的$\lambda$值计算$J_{train}(\Theta)$（为下一步学习参数$\Theta$）；&lt;/li&gt;
&lt;li&gt;使用学习（带$\lambda$）得到的参数$\Theta$计算不带正则项或是$\lambda=0$的训练误差$J_{train}(\Theta)$；&lt;/li&gt;
&lt;li&gt;使用学习（带$\lambda$）得到的参数$\Theta$计算不带正则项或是$\lambda=0$的交叉验证误差$J_{CV}(\Theta)$；&lt;/li&gt;
&lt;li&gt;对模型集合所有$\lambda$取值重复上述步骤，选择使交叉验证集误差最小的组合；&lt;/li&gt;
&lt;li&gt;如果需要使用图形化结果来帮助决策的话，可以绘制$\lambda$和$J_{train}(\Theta)$的图像，以及$\lambda$和$J_{CV}(\Theta)$的图像；&lt;/li&gt;
&lt;li&gt;使用最好的$\Theta$和$\lambda$组合，在测试集上进行预测计算$J_{test}(\Theta)$的值来验证模型对问题是否有好的泛化能力。&lt;/li&gt;
&lt;li&gt;为了帮助选择最好的多项式次数和$\lambda$的值，可以采用学习曲线来诊断。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;5-学习曲线:876321dc83c64489eb74f98905ca718c&#34;&gt;5.学习曲线&lt;/h1&gt;

&lt;p&gt;训练3个样本很容易得到0误差，因为我们永远可以找到一条二次曲线完全经过3个点。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当训练集越来越大时，二次函数的误差也会增加。&lt;/li&gt;
&lt;li&gt;误差值会在训练集大小m增加到一定程度后慢慢平缓。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;高偏差的情况&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;小训练集&lt;/strong&gt;：$J_{train}(\Theta)$较低，$J_{CV}(\Theta)较高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;大训练集&lt;/strong&gt;：$J_{train}(\Theta)$和$J_{CV}(\Theta)都较高，并且$J_{train}(\Theta) \approx J_{CV}(\Theta)$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果学习算法有高偏差的问题，那么获取更多的训练数据并不会有很多改进。&lt;/p&gt;

&lt;p&gt;对于高方差的问题，对于训练集大小有如下关系：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;高方差的情况&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;小训练集&lt;/strong&gt;：$J_{train}(\Theta)$较低，$J_{CV}(\Theta)较高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;大训练集&lt;/strong&gt;：$J_{train}(\Theta)$会略微增加，$J_{CV}(\Theta)会略微降低，并且$J_{train}(\Theta) &amp;lt; J_{CV}(\Theta)$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果学习算法有高方差的问题，那么获取更多的训练数据是有用的。&lt;/p&gt;

&lt;p&gt;下图展示了训练集大小和高偏差/高方差问题之间的关系。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/500px-High-variance-high-bias.png&#34; alt=&#34;High-variance-high-bias&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;6-再次考虑如何选择提升模型的下一步:876321dc83c64489eb74f98905ca718c&#34;&gt;6.再次考虑如何选择提升模型的下一步&lt;/h1&gt;

&lt;p&gt;决策过程可以分解成以下几点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获得更多的训练样本

&lt;ul&gt;
&lt;li&gt;解决高方差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;减少特征的数量

&lt;ul&gt;
&lt;li&gt;解决高方差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加特征的数量

&lt;ul&gt;
&lt;li&gt;解决高偏差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;使用多项式特征

&lt;ul&gt;
&lt;li&gt;解决高偏差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加正则参数$\lambda$

&lt;ul&gt;
&lt;li&gt;解决高偏差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;减少正则参数$\lambda$

&lt;ul&gt;
&lt;li&gt;解决高方差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;7-诊断神经网络:876321dc83c64489eb74f98905ca718c&#34;&gt;7.诊断神经网络&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;参数较少的神经网络很容易欠拟合，但同时计算也较容易。&lt;/li&gt;
&lt;li&gt;参数较多的大型神经网络更容易过拟合，但同时计算量较大。在这种情况下可以使用正则化（增加$\lambda$）来避免过拟合问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用单个隐藏层是一个较好地开始默认设置。你可以使用验证集在多个隐藏层上训练神经网络。&lt;/p&gt;

&lt;h1 id=&#34;8-模型选择总结:876321dc83c64489eb74f98905ca718c&#34;&gt;8.模型选择总结&lt;/h1&gt;

&lt;p&gt;以下是机器学习诊断的一些总结&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;选择多项式次数M&lt;/li&gt;
&lt;li&gt;如何选择模型中得参数$\Theta$（即模型选择）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有3种方式解决：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;获取更多数据（非常困难）&lt;/li&gt;
&lt;li&gt;选择拟合数据最好且没有过拟合的模型（非常困难）&lt;/li&gt;
&lt;li&gt;通过正则化来减少过拟合的机会&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;偏差：近似误差（预测值和期望值之间的差值）&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高偏差 = 欠拟合（BU）&lt;/li&gt;
&lt;li&gt;$J_{train}(\Theta)$和$J_{CV}(\Theta)都较高，并且$J_{train}(\Theta) \approx J_{CV}(\Theta)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;方差：有限数据集之间的估计误差值&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高方差 = 过拟合（VO）&lt;/li&gt;
&lt;li&gt;$J_{train}(\Theta)$较低，并且$J_{train}(\Theta) &amp;lt;&amp;lt; J_{CV}(\Theta)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;偏差-方差权衡的直觉&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;复杂模型=&amp;gt;数据敏感=&amp;gt;受训练集X变化的影响=&amp;gt;高方差，低偏差&lt;/li&gt;
&lt;li&gt;简单模型=&amp;gt;更死板=&amp;gt;不受训练集X变化的影响=&amp;gt;低方差，高偏差&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;机器学习的最重要的目标之一：找到一个模型在偏差-方差的权衡之间刚刚好。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;正则化影响&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda$值较小（过拟合）使模型容易受噪声影响，导致高方差。&lt;/li&gt;
&lt;li&gt;$\lambda$值较大（欠拟合）会将参数值接近于0，导致高偏差。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;模型复杂度影响&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;多项式次数较低的模型（模型复杂度低）有高偏差和低方差。在这种情况下，模型拟合总是很差。&lt;/li&gt;
&lt;li&gt;多项式次数较高的模型（模型复杂度高）拟合训练集极好，拟合测试集极差。导致训练集上低偏差，但高方差。&lt;/li&gt;
&lt;li&gt;在现实中，我们想要选择一个模型在以上两种情况之间，既然可以很好地拟合数据，也有很好地泛化能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用诊断时的一些典型经验法则&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取更多地训练样本可以解决高方差问题，不能解决高偏差问题。&lt;/li&gt;
&lt;li&gt;减少特征数量可以解决高方差问题，不能解决高偏差问题。&lt;/li&gt;
&lt;li&gt;增加特征数量可以解决高偏差问题，不能解决高方差问题。&lt;/li&gt;
&lt;li&gt;增加多项式特征和交互特征（特征和特征交互）解决高偏差问题，不能解决高方差问题。&lt;/li&gt;
&lt;li&gt;当使用梯度下降时，减少正则化参数$\lambda$值可以解决高方差问题，增加$\lambda$值可以解决高偏差问题。&lt;/li&gt;
&lt;li&gt;当使用神经网络时，小型神经网络更容易欠拟合，大型神经网络更容易过拟合。交叉验证是选择神经网络大小的一种方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://class.coursera.org/ml/lecture/index&#34;&gt;https://class.coursera.org/ml/lecture/index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE555/Chap9.Part2.pdf&#34;&gt;http://www.cedar.buffalo.edu/~srihari/CSE555/Chap9.Part2.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.stephenpurpura.com/post/13052575854/managing-bias-variance-tradeoff-in-machine-learning&#34;&gt;http://blog.stephenpurpura.com/post/13052575854/managing-bias-variance-tradeoff-in-machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap3/Bias-Variance.pdf&#34;&gt;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap3/Bias-Variance.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记5 神经网络2 参数学习</title>
      <link>http://nanshu.wang/post/2015-03-26</link>
      <pubDate>Thu, 26 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-03-26</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;成本函数-cost-function:8735cce9d7fd270391f5ec199a69102a&#34;&gt;成本函数(Cost Function)&lt;/h1&gt;

&lt;p&gt;以下是我们会用到的一些变量：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$L$表示神经网络的层数&lt;/li&gt;
&lt;li&gt;$s_l$表示第$l$层的神经单元数(不包括偏差(bias)单元)&lt;/li&gt;
&lt;li&gt;$K$表示输出单元数(分类数)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当有多个输出类别时，采用$h_\Theta(x)_k$表示第$k$个输出的假设结果。&lt;/p&gt;

&lt;p&gt;神经网络的成本函数是logistic回归中的成本函数更普遍的一种形式。&lt;/p&gt;

&lt;p&gt;logistic回归中的成本函数为：&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)}\log(h_{\theta}) + (1-y^{(i)}) \log(1-h_{\theta}(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2$$
&lt;/div&gt;

&lt;p&gt;神经网络的成本函数稍微复杂一点：&lt;/p&gt;

&lt;div&gt;
$$\begin{gather*}
J(\Theta) = - \frac{1}{m} \left[ \sum_{i=1}^m \sum_{k=1}^K y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2
\end{gather*}$$
&lt;/div&gt;

&lt;p&gt;和logistic回归相比，第一个方括号内多了一层累加求和，即对多类输出节点的成本函数进行累加，会遍历所有的输出节点。&lt;/p&gt;

&lt;p&gt;正则化的部分则是将每一层的参数都考虑了进去(包括偏差单元相关的参数值)。和logistic回归一样，每一项都进行了平方。&lt;/p&gt;

&lt;p&gt;注意:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;双层累加中对于输出层所有单元的logistic回归的成本函数进行求和&lt;/li&gt;
&lt;li&gt;三层累加中则是对于整个神经网络中单独的$\Theta$参数的平方进行求和&lt;/li&gt;
&lt;li&gt;三层累加中的$i$和训练集中的$i$并不一样&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;向后传播-backpropagation-算法:8735cce9d7fd270391f5ec199a69102a&#34;&gt;向后传播(Backpropagation)算法&lt;/h1&gt;

&lt;p&gt;向后传播是神经网络中表示最小化成本函数的术语，就像logistic回归和线性回归中的梯度下降(gradient descent)一样。&lt;/p&gt;

&lt;p&gt;我们的目标是计算：&lt;/p&gt;

&lt;div&gt;
$$\min_\Theta J(\Theta)$$
&lt;/div&gt;

&lt;p&gt;即我们想最小化成本函数$J$，得到最优化参数$\Theta$。&lt;/p&gt;

&lt;p&gt;观察我们用于计算$J(\Theta)$偏导的等式：&lt;/p&gt;

&lt;div&gt;
$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)$$
&lt;/div&gt;

&lt;p&gt;在向后传播中我们需要对每个节点计算：&lt;/p&gt;

&lt;div&gt;
$$\delta_j^{(l)} \text{＝第} l \text{层的节点}j \text{的误差}$$
&lt;/div&gt;

&lt;p&gt;回忆一下，$a_j^{(i)}$表示第$l$层的节点$j$的激活结果。对于最后一层，误差的计算为：&lt;/p&gt;

&lt;div&gt;$$\large
\delta^{(L)} = a^{(L)} - y$$&lt;/div&gt;

&lt;p&gt;其中，$L$表示层数，$a^{(L)}$是最后一层激活单元的向量化表示。因此对于最后一层，误差的计算就是简单的真实结果和预测结果的差值。&lt;/p&gt;

&lt;p&gt;为了得到除了最后一层以外其他层的误差值，我们采用以下等式从右往左地向后计算：&lt;/p&gt;

&lt;div&gt;$$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ g&#39;(z^{(l)})$$&lt;/div&gt;

&lt;p&gt;也就是用$\Theta$去乘后一层的误差值，再对每一个元素乘上$g&amp;rsquo;$，也就是激活函数$g$输入值为$z^{(l)}$时的导数。&lt;/p&gt;

&lt;p&gt;$g&amp;rsquo;$也可以写成：&lt;/p&gt;

&lt;div&gt;$$g&#39;(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})$$&lt;/div&gt;

&lt;p&gt;那么，对于内部节点来说，完整的向后传播计算等式为：&lt;/p&gt;

&lt;div&gt;$$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})$$&lt;/div&gt;

&lt;p&gt;其中，证明比较复杂，不过实现向后传播的算法也不用知道证明的细节。&lt;/p&gt;

&lt;p&gt;对于每个训练样本$t$，可以采用激活值和误差值来计算偏导项：&lt;/p&gt;

&lt;div&gt;$$\dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}} = \frac{1}{m}\sum_{t=1}^m a_j^{(t)(l)} {\delta}_i^{(t)(l+1)}$$&lt;/div&gt;

&lt;p&gt;上述式子忽略了正则项，后面会再详细说明。&lt;/p&gt;

&lt;p&gt;注意，其中$\delta^{l+1}$和$a^{l+1}$都是有$s_{l+1}$个元素的向量。类似地，$a^{(l)}$是有$s_l$个元素的向量。这两个向量相乘会产生和$\Theta^{(l)}$同样维度的$s_{l+1} * s_l$的矩阵。这个计算过程也就是得到了$\Theta^{(l)}$中每个元素的梯度项。&lt;/p&gt;

&lt;p&gt;现在，将以上过程串起来，我们得到整个向后传播算法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;给定训练集$\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$&lt;/li&gt;
&lt;li&gt;设$\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace \forall (l,i,j)$&lt;/li&gt;
&lt;li&gt;对于每个训练集 $\text{for } t = 1 \text{ to } m$:

&lt;ul&gt;
&lt;li&gt;$a^{(1)} := x^{(t)}$&lt;/li&gt;
&lt;li&gt;采用向前传播算法计算$a^{(l)}, l = 2,3,&amp;hellip;,l$&lt;/li&gt;
&lt;li&gt;计算$\delta^{(L)} = a^{(L)} - y^{(t)}$&lt;/li&gt;
&lt;li&gt;计算$\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$
&lt;div&gt;$$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)}) .* a^{(l)} .* (1 - a^{(l)})$$&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;$\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$，向量运算表示为：&lt;div&gt;$$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$$&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;$D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$s&lt;/li&gt;
&lt;li&gt;$D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$D^{(l)}_{i,j}$即最后我们所求的偏导。&lt;/p&gt;

&lt;h1 id=&#34;向后传播的直观理解:8735cce9d7fd270391f5ec199a69102a&#34;&gt;向后传播的直观理解&lt;/h1&gt;

&lt;p&gt;成本函数为：&lt;/p&gt;

&lt;div&gt;
$$
\begin{gather*}
J(\theta) = - \frac{1}{m} \left[ \sum_{t=1}^m \sum_{k=1}^K y^{(t)}_k \ \log (h_\theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \theta_{j,i}^{(l)})^2
\end{gather*}
$$
&lt;/div&gt;

&lt;p&gt;如果我们只考虑一个训练样本$(t=1)$，并且忽略正则项，那么成本函数为：&lt;/p&gt;

&lt;div&gt;
$$cost(t) =y^{(t)} \ \log (h_\theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\theta(x^{(t)}))$$
&lt;/div&gt;

&lt;p&gt;直观上可以将以上的等式近似看作：&lt;/p&gt;

&lt;div&gt;$$cost(t) \approx (h_\theta(x^{(t)})-y^{(t)})^2$$&lt;/div&gt;

&lt;p&gt;$\delta_j^{(l)}$是$a_j^{(l)}$的误差。&lt;/p&gt;

&lt;p&gt;严格来说，$\delta$值实际上是成本函数的偏导：&lt;/p&gt;

&lt;div&gt;$$\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)}} cost(t)$$&lt;/div&gt;

&lt;p&gt;注意偏导是成本函数切线的斜率，斜率越陡峭越不准确。&lt;/p&gt;

&lt;h1 id=&#34;梯度检查-gradient-checking:8735cce9d7fd270391f5ec199a69102a&#34;&gt;梯度检查(Gradient Checking)&lt;/h1&gt;

&lt;p&gt;梯度检查能够保证我们的向后传播算法的正确性。&lt;/p&gt;

&lt;p&gt;我们可以用以下方法近似地估算成本函数的偏导：&lt;/p&gt;

&lt;div&gt;$$\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$$&lt;/div&gt;

&lt;p&gt;$\Theta$是很多个矩阵构成，可以对$\Theta_j$的偏导进行以下估算：&lt;/p&gt;

&lt;div&gt;$$\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$$&lt;/div&gt;

&lt;p&gt;$\epsilon$的取值必须较小，才能得到接近真实的值。但如果$\epsilon$的值太小，计算可能会有精度问题而无法得到正确值。Andrew给的经验值时$\epsilon = 10^{-4}$。&lt;/p&gt;

&lt;p&gt;如果可以验证梯度近似值和$\delta$大致相等，那么向后传播算法是正确的。在真正计算中不需要计算梯度近似值，因为其计算非常慢。&lt;/p&gt;

&lt;h1 id=&#34;随机初始化:8735cce9d7fd270391f5ec199a69102a&#34;&gt;随机初始化&lt;/h1&gt;

&lt;p&gt;在神经网络中，不能将$\theta$权重初始化为0，否则在向后传播时，所有的节点会重复更新一样的值。&lt;/p&gt;

&lt;p&gt;因此需要随机初始化权重：&lt;/p&gt;

&lt;p&gt;将$\Theta_{ij}^{(l)}$赋为$[-\epsilon,\epsilon]$范围内的一个随机数：&lt;/p&gt;

&lt;div&gt;$$\epsilon = \dfrac{\sqrt{6}}{\sqrt{\mathrm{Loutput} + \mathrm{Linput}}}$$&lt;/div&gt;

&lt;div&gt;$$\Theta^{(l)} =  2 \epsilon \; \mathrm{rand}(\mathrm{Loutput}, \mathrm{Linput} + 1)    - \epsilon$$&lt;/div&gt;

&lt;p&gt;其中，$Loutput$和$Linput+1$是$\Theta$参数的维度。这里的$\epsilon$和梯度检查中的$\epsilon$没有关系。&lt;/p&gt;

&lt;h1 id=&#34;神经网络总结:8735cce9d7fd270391f5ec199a69102a&#34;&gt;神经网络总结&lt;/h1&gt;

&lt;p&gt;首先，选择神经网络的结构，确定神经网络的层数和每层的隐藏单元的个数。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;输入层单元数等于特征$x^(i)$的维度&lt;/li&gt;
&lt;li&gt;输出层的单元数等于分类个数&lt;/li&gt;
&lt;li&gt;默认设置：1个隐藏层。如果有多于1个隐藏层，则隐藏层的单元个数都一样多。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;训练神经网络&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;随机初始化权重&lt;/li&gt;
&lt;li&gt;实现向前传播计算$h_{\theta}(x^{(i)})$&lt;/li&gt;
&lt;li&gt;实现成本函数&lt;/li&gt;
&lt;li&gt;实现向后传播计算偏导值&lt;/li&gt;
&lt;li&gt;采用梯度检查来确认向后传播的正确性后，再取消梯度检查。&lt;/li&gt;
&lt;li&gt;采用梯度下降或其他已有的最优化函数来最小化成本函数，得到$\theta$权重值。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于每个训练样本都循环采用向前和向后传播算法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i = 1:m,
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>python numpy 计算自相关系数</title>
      <link>http://nanshu.wang/post/2015-03-15</link>
      <pubDate>Sun, 15 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-03-15</guid>
      <description>&lt;p&gt;在分析时间序列时，通常需要计算一个序列的自相关系数。自相关(&lt;a href=&#34;http://en.wikipedia.org/wiki/Autocorrelation&#34;&gt;Autocorrelation&lt;/a&gt;)又叫做序列相关，通常采用自相关系数来发现序列的重复规律，周期等信息。&lt;/p&gt;

&lt;p&gt;我们有序列$X:x_1,x_2,x_3,&amp;hellip;,x_n$，设$X_{s,t}$为$s$时刻开始，$t$时刻结束的序列：$x_s,x_{s+1}&amp;hellip;,x_{t-1},x_t$。$\mu_{s,t}$为序列$X_{s,t}$的均值，$\sigma_{s,t}$为序列$X_{s,t}$的标准差。那么一阶自相关系数为：&lt;/p&gt;

&lt;div&gt;

$$R(1) = \frac{E(X_{2,n}-\mu_{2,n})(X_{1,n-1}-\mu_{1,n-1})}{\sigma_{2,n}\sigma_{1,n-1}}$$

&lt;/div&gt;

&lt;p&gt;同理$k$阶自相关系数为：&lt;/p&gt;

&lt;div&gt;

$$R(k) = \frac{E(X_{k+1,n}-\mu_{k+1,n})(X_{1,n-k}-\mu_{1,n-k})}{\sigma_{k+1,n}\sigma_{1,n-k}}$$

&lt;/div&gt;

&lt;p&gt;python的numpy库里没有直接计算序列自相关系数的函数，但有计算两个不同序列的相关系数函数： &lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html&#34;&gt;correlate&lt;/a&gt;。给定两个序列$X,Y$，correlation(X,Y) = $\sum XY$。可以利用correlate函数计算$X$的自相关性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autocorrelation(x,lags):#计算lags阶以内的自相关系数，返回lags个值，分别计算序列均值，标准差
	n = len(x)
	x = numpy.array(x)
	result = [numpy.correlate(x[i:]-x[i:].mean(),x[:n-i]-x[:n-i].mean())[0]\
		/(x[i:].std()*x[:n-i].std()*(n-i)) \
		for i in range(1,lags+1)]
	return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常在实际中，很多时间序列的均值和标准受时间变化的影响较小，可以看作是恒定的，此时：&lt;/p&gt;

&lt;div&gt;

$$R(k) = \frac{E(X_{k+1,n}-\mu)(X_{1,n-k}-\mu)}{\sigma^2}$$

&lt;/div&gt;

&lt;p&gt;同样可以利用correlate函数实现：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autocorrelation(x,lags):#计算lags阶以内的自相关系数，返回lags个值，将序列均值、标准差视为不变
	n = len(x)
	x = numpy.array(x)
	variance = x.var()
	x = x-x.mean()
	result = numpy.correlate(x, x, mode = &#39;full&#39;)[-n+1:-n+lags+1]/\
		(variance*(numpy.arange(n-1,n-1-lags,-1)))
	return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考：&lt;a href=&#34;http://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation&#34;&gt;http://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记5 神经网络1 模型表达</title>
      <link>http://nanshu.wang/post/2015-03-03-2</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-03-03-2</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;神经网络:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经网络&lt;/h1&gt;

&lt;h2 id=&#34;非线性假设:d35c8128725ed53542064246ed42d2c0&#34;&gt;非线性假设&lt;/h2&gt;

&lt;p&gt;在特征变量数较大的情况下，采用线性回归会很难处理，比如我的数据集有3个特征变量，想要在假设中引入所有特征变量的平方项：&lt;/p&gt;

&lt;div&gt;
$$g(\theta_0 + \theta_1x_1^2 + \theta_2x_1x_2 + \theta_3x_1x_3  + \theta_4x_2^2 + \theta_5x_2x_3  + \theta_6x_3^2 )$$
&lt;/div&gt;

&lt;p&gt;共有6个特征，假设我们想知道选取其中任意两个可重复的平方项有多少组合，采用允许重复的组合公式计算$\frac{(n+r-1)!}{r!(n-1)!}$，共有$\frac{(3 + 2 - 1)!}{(2!\cdot (3-1)!)} = 6$种特征变量的组合。对于100个特征变量，则共有$\frac{(100 + 2 - 1)!}{(2\cdot (100-1)!)} = 5050$个新的特征变量。&lt;/p&gt;

&lt;p&gt;可以大致估计特征变量的平方项组合个数的增长速度为$\mathcal{O}(\frac{n^2}2)$，立方项的组合个数的增长为$\mathcal{O}(n^3)$。这些增长都十分陡峭，让实际问题变得很棘手。&lt;/p&gt;

&lt;p&gt;在变量假设十分复杂的情况下，神经网络提供了另一种机器学习算法。&lt;/p&gt;

&lt;h1 id=&#34;神经元和大脑:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经元和大脑&lt;/h1&gt;

&lt;p&gt;神经网络是对大脑工作方式的一种简单模拟。有证据表明，大脑对所有的功能（如视觉，触觉，听觉等）都采用了一种“学习算法”。将听觉皮层和视觉神经连接到一起，听觉皮层可以学会“看见”。这种理论叫作“neuroplasticity”，已经得到了很多例子和实验验证。&lt;/p&gt;

&lt;h1 id=&#34;模型表达:d35c8128725ed53542064246ed42d2c0&#34;&gt;模型表达&lt;/h1&gt;

&lt;p&gt;简单来说，每个神经元都有输入（树突dendrites）和输出（轴突axons）。在模型中，输入就是我们的特征变量，输出就是模型假设的结果。&lt;/p&gt;

&lt;p&gt;在神经网络中，分类问题通常采用logistic函数，也叫做sigmoid激活函数(sigmoid activation function)。$\theta$参数有时也被称为权重(weights)。&lt;/p&gt;

&lt;p&gt;下面是一种简单的神经网络：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/neuron_model.jpg&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;第一层是输入节点(nodes)，第二层是输出节点，也就是我们假设函数的结果$h_{\theta}(x)$。&lt;/p&gt;

&lt;p&gt;第一层叫作“输入层”(input layer)，最后一层叫作“输出层”(output layer)，输入层和输出层之间还可以有多层，统称为“隐藏层”(hidden layer)。如下图中，第二层就叫隐藏层。隐藏层节点表示为$a^2_0 \cdots a^2_n$，被称作“激活单元(activation units)”。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/neuron_network_3layers.jpg&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;$a_i^{(j)}$表示第$j$层的$i$单元被“激活”，$\Theta^{(j)}$表示从第$j$层到第$j+1$层的权重矩阵。&lt;/p&gt;

&lt;p&gt;上图的神经网络中，激活单元的计算分别为：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline
a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline
h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;每一层都有自己的权重矩阵$\Theta^{(j)}$，如果第$j$层有 $s_j$ 个单元，第$j+1$层有$s_{j+1}$个单元，则$\Theta^{(j)}$是$s_{j+1} \times (s_j+1)$的矩阵。&amp;rdquo;+1&amp;rdquo;是因为第$j$层包括一个&amp;rdquo;偏差节点(bias nodes)“，$x_0$和$\Theta_0^{(j)}$。换句话说，输出节点不包括偏差节点，但输入节点会包括偏差节点。&lt;/p&gt;

&lt;p&gt;举个例子，第一层有2个输入节点，第二层有4个激活单元。$\Theta^{(1)}$的维度为$4 \times 3$。&lt;/p&gt;

&lt;p&gt;下面将以上模型表达向量化。&lt;/p&gt;

&lt;p&gt;采用$z^{(i)}_k$表示$g$函数的输入，那么有：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
a_1^{(2)} = g(z_1^{(2)}) \newline
a_2^{(2)} = g(z_2^{(2)}) \newline
a_3^{(2)} = g(z_3^{(2)}) \newline
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;给定第$j$层的节点$k$，变量$z$等于：&lt;/p&gt;

&lt;div&gt;
$$
z_k^{(j)} = \Theta_{k,0}^{(j-1)}x_0 + \Theta_{k,1}^{(j-1)}x_1 + \cdots + \Theta_{k,n}^{(j-1)}x_n 
$$
&lt;div&gt;

$x$和$z^{(j)}$的向量表示为：

&lt;div&gt;
$$
\begin{align*}
x = 
\begin{bmatrix}
x_0 \newline
x_1 \newline
\cdots \newline
x_n
\end{bmatrix} &amp;
z^{(j)} = 
\begin{bmatrix}
z_1^{(j)} \newline
z_2^{(j)} \newline
\cdots \newline
z_n^{(j)}
\end{bmatrix}
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;因此，$z^{(j}) = \Theta^{(j-1)} x$&lt;/p&gt;

&lt;p&gt;令$x = a^{(j-1)}$，则$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$&lt;/p&gt;

&lt;p&gt;最后的结果为&lt;/p&gt;

&lt;div&gt;
$$
h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) = g(\Theta^{(j)}a^{(j)})
$$
&lt;/div&gt;

&lt;h1 id=&#34;神经网络拟合逻辑运算:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经网络拟合逻辑运算&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ AND\ x_2$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;神经网络模型：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
g(z^{(2)})
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;其中，$x_0$为偏差，恒等于1。&lt;/p&gt;

&lt;p&gt;权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
-30 &amp; 20 &amp; 20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;仅当$x_1$和$x_2$同时为1时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \newline
\newline
&amp; x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline
&amp; x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline
&amp; x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline
&amp; x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1
\end{align*}
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ NOR\ x_2$, $NOR$为$NOT\ OR$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;模型同上，权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
10 &amp; -20 &amp; -20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;仅当$x_1$和$x_2$同时为0时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ OR\ x_2$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;模型同上，权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
-10 &amp; 20 &amp; 20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;当$x_1$和$x_2$不同时为0时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ XNOR\ x_2$，$XNOR$为$NOT\ XOR$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将前面得到的模型组合起来$x_1\ XNOR\ x_2 = (x_1\ AND\ x_2)\ OR\ (x_1\ XOR\ x_2)$&lt;/p&gt;

&lt;p&gt;模型如下：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_1^{(2)} \newline
a_2^{(2)} 
\end{bmatrix} \rightarrow
\begin{bmatrix}
a^{(3)}
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;第一层到第二层的$\Theta^{(1)}$分别表示$AND$和$NOR$操作：&lt;/p&gt;

&lt;div&gt;
$$
\Theta^{(1)} = 
\begin{bmatrix}
-30 &amp; 20 &amp; 20 \newline
10 &amp; -20 &amp; -20
\end{bmatrix}
$$
&lt;/div&gt;  

&lt;p&gt;第二层到第三层的$\Theta^{(2)}$表示$OR$操作：&lt;/p&gt;

&lt;div&gt;
$$
\Theta^{(2)} = 
\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}
$$
&lt;/div&gt;

&lt;h1 id=&#34;多类分类问题:d35c8128725ed53542064246ed42d2c0&#34;&gt;多类分类问题&lt;/h1&gt;

&lt;p&gt;输出用向量来表示多类分类问题：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2 \newline
\cdots \newline
x_n
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(2)} \newline
a_1^{(2)} \newline
a_2^{(2)} \newline
\cdots
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(3)} \newline
a_1^{(3)} \newline
a_2^{(3)} \newline
\cdots
\end{bmatrix} \rightarrow \cdots \rightarrow
\begin{bmatrix}
h_\Theta(x)_1 \newline
h_\Theta(x)_2 \newline
h_\Theta(x)_3 \newline
h_\Theta(x)_4 \newline
\end{bmatrix} \rightarrow
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;最后的结果：&lt;/p&gt;

&lt;div&gt;
$$h_\Theta(x) = 
\begin{bmatrix}
0 \newline
0 \newline
1 \newline
0 \newline
\end{bmatrix}$$
&lt;/div&gt;

&lt;p&gt;表示该样本属于第三类，$h_{\Theta}(x)_3$。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>经过省察的人生The examined life</title>
      <link>http://nanshu.wang/post/2015-03-03-1</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-03-03-1</guid>
      <description>

&lt;p&gt;译自&lt;a href=&#34;http://books.google.lt/books?id=WwpZVuylPgYC&amp;amp;pg=PA228&amp;amp;hl=zh-CN&amp;amp;source=gbs_toc_r&amp;amp;cad=3#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;《A companion to Socrates》第14章&lt;/a&gt;，作者Richard Kraut&lt;/p&gt;

&lt;h1 id=&#34;经过省察的人生:f856045648ed1256c7d09f14d1393f16&#34;&gt;经过省察的人生&lt;/h1&gt;

&lt;p&gt;“未经省察过的人生是不值得过的人生”（《申辩》38a5-6）。这句苏格拉底耳熟能详的名言，也许是有史以来一个哲学家嘴中最肆无忌惮的话。苏格拉底的哲学思想和他的生活方式的本质也蕴含在这句话中。苏格拉底把自己的人生作为——或者更确切地说，柏拉图把他的人生作为——真正经过了省察的人生。要了解我们如何才能达到这种人生的要求，必须从柏拉图的作品出发，来好好学习苏格拉底给我们树立起来的好榜样。这种人生的要求或许能激励我们或许不能使我们信服，因为准确地说，这是苏格拉底对他同时代人产生的影响。苏格拉底的魅力和崇高的理想可能会被削弱。因为他说过，未经省察过的人生正是许多人的生活方式，是只有当我们有强烈倾向时才做出的选择。同时，如果遵循苏格拉底的坚持，所有人都按他的方式生活，这种人生的要求则会显得荒诞不经，甚至可以说是异乎寻常的苛刻。在任何社会中，最多只有几个人可以像苏格拉底那样，把所有的日子都用来讨论道德伦理。所有其他的人都过着毫无价值的生活吗？苏格拉底有什么好的理由来批评绝大多数人类的行为生活吗？如果只有少数人像苏格拉底那样奉献自己，他对同时代人的生活几乎没有影响，那么在更一般的情况下，经过省察的人生又有什么价值？对于这个问题，确切得说，应该是对于一个能选择这样生活的人来说，经过省察的人生到底有什么好的？即使我们都可以掌控这种生活，我们又为什么要做出尝试呢？&lt;/p&gt;

&lt;p&gt;如果还有道德哲学家的话，在当今的学术界恐怕也没谁赞同苏格拉底的名言。他们会说，我们被要求不用过分的方式（谋杀，袭击，盗窃）去伤害他人；我们要在一定程度上（这个程度多少也是一个争议的话题）至少造福一些其他人；我们要诚实，公正，善良，宽容；我们应该做的事，在道德意义上必须是正确的，因为道德本身就是正确的，而不是为达到某种非道德目的的方式。他们会说，那种苏格拉底与他的跟随者们从事的伦理讨论是值得追求的（毕竟这和这些哲学家的生活有点像）；但他们会补充说，没有必要甚至也不希望每个人都从事那种让苏格拉底着迷的抽象伦理探究。当然，他们还会说，这不是判断一个好人的人生是否经受住了审察的必要特点。在他们认为，成为一个好人明显到任何拥有常识的人都能看出来：在幼时拥有良好的教导，能分辨道德意义上的对错，并且坚持做对的事。人们需要变得善于使用这样的词语如“应该”，“正确”，“善”，“正义”，“诚实”。但要做到这一点，我们需要的不是像苏格拉底认为的那样，进入抽象和困难的道德哲学领域的，询问自己和他人诸如这样的问题：“什么是勇气？”，“ 什么是正义？”，“什么成为朋友？“相反，我们需要的是获得社会和情感技能，使我们认识到什么在道德意义上是对的，并且带着良心去做道德意义上对的事。苏格拉底根据这种思维方式，简单误以为成为一个好人，全部或部分地包括了成为一个好的道德哲学家。&lt;/p&gt;

&lt;p&gt;然而，并非所有人都同意，苏格拉底对经过省察的人生的呼吁可以这样容易就被否定掉。在现代很大一部分时期中，他对经过省察的人生的呼吁为他赢得了一个接近基督和其他宗教领袖的地位，被视为人类历史上一个伟大的道德模范。本杰明·乔伊特（ Benjamin Jowett），十九世纪后期一个柏拉图作品的重要译者，告诉他的学生：“在牛津大学，基督和苏格拉底的人物传记是我们最浓厚的兴趣所在（虽然程度不一）。”1这样的比较在二十世纪也有，苏格拉底被一位存在主义作家卡尔·雅斯贝尔斯（ Karl Jaspers，1962）视为一个“个人模范”（继佛陀，孔子，基督之后）。但在二十一世纪的最初几年，道德哲学的学术界并没有苦苦抓住苏格拉底不放，而更像是避开了他（因为还有亚里士多德，休谟，康德，尼采， 西奇威克）。&lt;/p&gt;

&lt;p&gt;苏格拉关于未经省察过的人生不值得过的人生的理论，我想既不能被接受也不能被拒绝。刚刚也描绘了若被拒绝的情况。&lt;strong&gt;在下文中，我将解释这样的人生有什么意义以及为什么苏格拉底认为我们必须这样做。&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;柏拉图的-申辩:f856045648ed1256c7d09f14d1393f16&#34;&gt;柏拉图的《申辩》&lt;/h1&gt;

&lt;p&gt;展开这种探讨时苏格拉底本人在场——或者更确切地说，在柏拉图的《申辩》中，苏格拉底被描述成为我们在演讲，说出了这句“未经省察过的人生是不值得过的人生。”苏格拉底这句话时，陪审团已经判他有罪，而且将决​​定实施什么样的刑法。Meletus是其中一个控告者，提出将他判处死刑，在这时提出另一种惩罚是苏格拉底的权利。他问：他应当被请求流放到其他的城市，并且放弃他实践哲学的方式吗？他的回答是他不愿放弃哲学，因为这违背了他的神的意愿。然后他补充说：“如果我说，一个人最大的好处是每天都在讨论德性和其他你们听我同别人交谈和省察的东西，未经省察过的人生不值得过的人生，你会更容易被我的话说服的。“（38a1-6）。苏格拉底提出了把罚款作为处罚，但陪审团决定他应该被处死。&lt;/p&gt;

&lt;p&gt;但是，为什么要每个人都要过一种省察过的人生呢？&lt;strong&gt;苏格拉底没有试图在这里回答这个问题，在任何辩护的其他地方也没有。&lt;/strong&gt; 他把经过省察的人生与进一步讨论德性就是好的联系起来，但是，这在柏拉图的《申辩》中同样没有做出解释。对于苏格拉底对这些说法没有给出论据，我们不应该感到惊讶。毕竟，他没有在做一个关于哲学的演讲，而是在针对指控为自己辩护，表明他的生活方式存在好的动机且没有不良影响。他的听众并不关心否可以证明是最好的生活是经过省察的生活；而想知道怎样对待持这种想法的人。&lt;/p&gt;

&lt;p&gt;假设我们假定智慧是一个人可以拥有的最好的东西。并且假定我们接受苏格拉底比其他所有的人都更能通过伦理讨论来接近智慧。这些前提将会确保苏格拉底式的诘问法也有很大的价值的结论，但将其作为结论则会受到挑战。我们为什么要认为智慧是最好的东西呢？并且为什么要假设这种德性是凭借或部分凭借苏格拉底进行的那种抽象的道德探究组成的呢？似乎这样说也合情合理，尽管有许多不同形式的智慧，但通过诡辩习得复杂的道德哲学也许是其中之一，可以假定它比所有其他形式的智慧更加有价值。苏格拉底式的诘问法可以提高个人的哲学技能——但能说它使人变得更好了吗？它提高了人做好事的能力吗？柏拉图的《申辩》不 能回答这些问题，并没有打算回答。&lt;/p&gt;

&lt;h1 id=&#34;不值得过的人生:f856045648ed1256c7d09f14d1393f16&#34;&gt;不值得过的人生&lt;/h1&gt;

&lt;p&gt;因此，我们将不得不转向柏拉图的其他作品，来找到对苏格拉底所这句“未经省察过的人生是不值得过的人生的更好诠释”。但在我们抛开《申辩》之前，我们应该先停下来想一想，以确保我们真正懂得苏格拉底的这句话是什么意思。&lt;/p&gt;

&lt;p&gt;其中的这个词——anexetastos（“unexamined未经省察的”）——的构成没有什么问题。Exetazein的意思是 “ 检查，探究，审察，测试，证明”。它和它的同源词 ，exetasis，经常被苏格拉底用来描述当在他与他人的谈话时在做的事。用同源形容词anexetastos形容的人生， 是一个没有受到苏格拉底式伦理道德省察的人生。&lt;/p&gt;

&lt;p&gt;但其他苏格拉底这句话中的另一个词——biotos ——需要仔细理解。译成短语“值得过的，”从乔伊特（Jowett）起一直是一种标准翻译。当我们说某人的人生在某时是不值得过的，我们的意思应该是他没有比死更好的活法。在理想的情况下，人生不值得过的时候正好是死亡来临的时候。同样，如果一个人的人生从来没有在任何时候值得一过，那么任何时候在他的人生里，死都是好的。&lt;/p&gt;

&lt;p&gt;如果我们理解苏格拉底的意思是应把所有的未经省察的人生都归入这个类别，那么我们则默认他对城邦的公民采取了其恶劣的态度。如果苏格拉底把他们从危难之中拯救出来（他在战场上杀敌），他则会认为这对他们没有什么好处。人们便会存有疑惑：为什么他认为他要为救他们动一根手指头？&lt;/p&gt;

&lt;p&gt;是不值得过的人生不单单是坏人生：这种人生太缺少价值以至于对一个人来说去死反而是最好的选择。我们没有理由将未经省察的人生里深深的痛苦归于苏格拉底的观点。因为 biotos 并不一定意味着“值得过的”，也可译为“应该过的”。这样，苏格拉底的这句话应该理解为：“未经省察的人生不是应该过的人生。”这并不意味着人生应该被终止，相反，它可以被理解为人们不应该过那样的人生。如果一个人的人生是那样，那他必须做出改变——不是死亡会更好，而是可能有一个更好的人生。&lt;/p&gt;

&lt;p&gt;这是苏格拉底的使命，去说服城邦的其他公民未经省察过的人生错过了最好的东西，因此他们必须改变他们的人生。苏格拉底的使命里没有哪部分是在说服他们人生不值得过最好去死。（试图证明这一点的意义又是什么呢？）因此，我建议我们放弃这个标准翻译，采用柏拉图希腊文版《申辩》38a5-6 的意思：“人们不应该过未经省察的人生。”我们不用害怕，以这种方式解释苏格拉底，才能把他的哲学思想变成一条不温不火的建议。每个人应该进行苏格拉底式探寻的观点是哲学家做出的最大胆的宣称之一。一旦一个人对这种观点笃信不疑，讨论对于过着这样人生的人是否去死更好，既没有哲学意义也没有现实意义。&lt;/p&gt;

&lt;p&gt;另外，关于苏格拉底说这话的含义，还有一点要强调：他认为探究伦理道德是一个过程，一个人应该因此花掉一生的时间，而不仅仅只是暂时的。一个人要达到他的要求，则要花了大半年去询问他问过的问题，然后转向其他事情，并不再关切这类问题。对于经过省察的人生的呼吁，和一下观点也有联系：一个人最大的好处是“每天都讨论德行”等伦理问题（《申辩》38a3）。 我们应该认识到这是多么大胆的话。我们只有期待苏格拉底可以给我们足够的理由来接受它。&lt;/p&gt;

&lt;h1 id=&#34;苏格拉底对话:f856045648ed1256c7d09f14d1393f16&#34;&gt;苏格拉底对话&lt;/h1&gt;

&lt;p&gt;我们从哪里能找到他的论据呢？几乎所有的柏拉图对话集中都有一个对话者——通常他占有主导地位——他就是苏格拉底。但也有学者认为，这些对话的某些内容里，这些谈话给了我们一个名叫“苏格拉底”的历史人物大概的形象；而在另一些国家，“苏格拉底”却成为了柏拉图哲学的代言人，他的哲学虽然在历史上受苏格拉底极大影响，但也有显著不同.2 这正是在这篇文章中应遵循的方法。&amp;rdquo;人们不应该过未经省察的人生这种观点是很可能是苏格拉底哲学的核心，但它却不是柏拉图本人赞同的——或者可以说他在哲学生涯中只是之中都没有赞同过。为了说明这一点，我们只需要举出《理想国》中所描述的个等级的划分：哲学家，士兵和工人。只有哲学家可以说过上经过省察的人生：他们是那些专门被培训成提出和回答各种各样的苏格拉底式问题的人。与此相反，士兵和工人，过了不假思索的人生，只是接受法律和统治者的决定。统治者不同于任何其他人，他的工作是对实际问题有智慧。因此，《理想国》中的苏格拉底拒绝了这种观点，即每个人都可以过上经过考察的人生。&lt;/p&gt;

&lt;p&gt;关于《申辩》和《理想国》中的冲突应当作何解释呢？&lt;strong&gt;一个令人难以置信的假设是，这里的差异实际上反映了柏拉图自己思想的转变。&lt;/strong&gt; 其实我个人比较相信这个假设。根据这种解释，柏拉图让苏格拉底在《申辩》中说一个人应该过经过省察的人生这样的话，实际上他是将自己的哲学思想放进了苏格拉底的演讲中。但是，这样来看待苏格拉底和柏拉图的关系不是一种可信的方式。我们没有理由怀疑，历史上的苏格拉底的确与许多和他同时代的人一起讨论德性，而他这样做引起了很大的敌意，但他却还是坚持认为这种探寻有很大的价值。无论是历史上的苏格拉底是否逐字逐句地依照我们今天读到的希腊文本谈论了关于未经省察的人生，这都不重要。不能轻易否认，经过省察的巨大人生价值是对历史上的苏格拉底的生平和思想的中心的假定。正如我们看到的那样，这种主导观念被《理想国》的主要对话者修正了。我认为，这种差异最合理的解释是柏拉图表达了他老师的中心思想，于是得出的结论是苏格拉底没有意识到其中的局限性。柏拉图说，是啊，经过省察的人生的确是人类能过上的最好的人生；但大家都去尝试便不是什么好事了。让小部分人过上那样的人生，去引导其他的人。&lt;/p&gt;

&lt;p&gt;我们认识到，柏拉图受到了苏格拉底的启发，但却超越他。在研究柏拉图对话时，要把他俩区别开来：第一，围绕主题的对话内容使用的可能是历史上苏格拉底的真实想法；第二，那些有更占主导地位的更充分的思想，有可能是柏拉图做出修改甚至不同于苏格拉底的。毫无疑问柏拉图以某种方式在所有作品里注入了他自己的思想；不可能一个哲学家的才华和独创性不被放入对别人语录的被动记载中。即便如此，基于其他的互相不同类型和不同之处，把其中的组成进行分类，也是有帮助的。&lt;strong&gt;其中那些被称为“苏格拉底式”的内容比较少，几乎完全是以道德、探索为中心，结构上也比较简单。&lt;/strong&gt; 这些内容主要关于对坏想法的抨击或证明对话者的局限；其中也包含积极的想法，但不多，也从没有与形而上学和认识论的整合到一起。这样的描述发生在苏格拉底与Laches, Charmides, Euthyphro, Crito, Protagoras, Hippias Minor, Hippias Major, Lysis 以及 Gorgias的对话中。在这些对话中，柏拉图的思想或多或少都在他老师搭建的范围内展开。正如柏拉图在《申辩》中提到的那样，苏格拉底不关心任何除了提高人生以外的话题。至少在成年后，&lt;strong&gt;苏格拉底是一个道德哲学家，不是一个形而上学者、不是一个认识论者，更不是科学家。&lt;/strong&gt; 亚里士多德也证实了这一假定：他说，苏格拉底只关注道德，不研究自然世界。（《形而上学》 I.6 987b1-2）&lt;/p&gt;

&lt;p&gt;许多其他对话都有一个相当不同的角色。这些对话篇幅更长，他们提出或审查的观点会经过更充分地阐述，涉及到的道德内容是与形而上学和认识论交织在一起的。 斐多篇“，”克拉底鲁篇“，理想国，菲德洛斯，巴门尼德，泰阿泰德篇，智者，政治家，和蒂迈欧篇” 都是如此 。对柏拉图的作品划分类别，不是说每篇作品都只属于一个类别。诸如美诺、尤西弗伦这样的作品既包括了苏格拉底式的对话特点，也有第二类作品的特点。&lt;/p&gt;

&lt;p&gt;大多数的苏格拉底式对话是柏拉图早期作品，那时苏格拉底的影响力是最强的；但也有可能他在同一时期也在创作冗长的涉及形而上学、认识论和道德的作品。他意识到对于他复杂的作品来说，苏格拉底式对话是为读者准备的一个绝好的铺垫。这可以解释为什么像Lysis 和 Charmides中，含有更长和更复杂的对话里的暗示。&lt;/p&gt;

&lt;p&gt;现在回到我们的主题上来。&lt;strong&gt;这句“人们不应该过未经省察的人生”属于历史上的苏格拉底，但不属于柏拉图笔下的苏格拉底。&lt;/strong&gt; 我们想知道苏格拉底到底对这个观点给出了什么论据，当然《申辩》中论据没有给出。那从哪里看起呢？最好就是从我们现在看的苏格拉底式对话出发，因为在这些内容中，柏拉图的哲思更贴近苏格拉底的特征，即与其他的哲学话题相比，伦理问题占有主导地位。&lt;/p&gt;

&lt;p&gt;我们将看到，这样来做是值得的：通过这些对话，能更加深入理解苏格拉底对于最重要的道德伦理的讨论所给出的理由。这些苏格拉底式的作品能够读出来，若是没有用足够的耐心和智慧来省察人生，究竟会出什么错。&lt;/p&gt;

&lt;h1 id=&#34;关于未经省察的人生的调查:f856045648ed1256c7d09f14d1393f16&#34;&gt;关于未经省察的人生的调查&lt;/h1&gt;

&lt;p&gt;尤西弗罗认为 自己是对于宗教事务的专家。在以他的名字命名的对话中，他对自己的父亲提起诉讼，因为他需要为一个家里死去的奴隶负责。在那个时代，起诉自己的父亲被认为是一个非常极端的行为。但尤西弗罗 仍然认为宗教义务让他必须这样做。也许在这一点上他是对的——但他在做出这个决定是他处于什么位置呢？他是否有任何根据来回答这样一个问题：那种行为是有关宗教职责的？&lt;/p&gt;

&lt;p&gt;在苏格拉底和尤西弗伦对话的过程中，他发现尤西弗伦没有认真考虑这件事，也没有思考虔诚的本质这个问题。尤西弗罗 是过着未经省察的人生的一个明显的例子（即使他认为自己是对于虔诚这个问题的专家），一篇以他名字命名的对话揭示了忽视哲学问题会产生多么严重的后果。除非他花大力气对的宗教职责做出准确的推测，否则他的宗教人生会变得很糟糕。似乎可以这样说，在这个例子中，他指控自己的父亲谋杀他人似乎是不虔诚的，他犯下了一个可怕的错误，便是没有意识到自己在做什么。&lt;/p&gt;

&lt;p&gt;从这个对话中得出的一个简单的教训是缺乏对道德哲学问题的重视将对人生价值产生灾难性的后果。尤西弗罗 将会犯下一个最严重的错误，不是因为他是自私，贪图权力，贪婪，而是因为他是蒙昧，愚笨，肤浅。&lt;strong&gt;因为他没有求知欲，没有兴趣探寻哲学的伦理问题，他缺乏同时也永远不会理解他所称的道德的系统和一般性含义，甚至是其中的皮毛也不会理解。&lt;/strong&gt; 毫无疑问，他是从他的父母学习到如何使用这个词 hosiotEs （“虔诚”），也学到了其他的规范灌输的词。但一个人的童年接受的教育至今只有一次，自己不能想好，去决定应该学习哪些规范词语。&lt;/p&gt;

&lt;p&gt;苏格拉底式的对话里有另一个人的显示出道德模糊的例子：克里托，在以其名字命名的对话里，他建议苏格拉底通过贿赂他的狱吏来越狱。克里托帮助苏格拉底逃脱的其中一个原因是他害怕周围的人耻笑他：在这种情况下很多人认为苏格拉底的朋友应该使用他们能力帮助他逃脱，如果他们没有这样做，则看起来像懦夫的行为（45E-46A）。值得注意的是，虽然克里托跟随在苏格拉底身边多年，但苏格拉底却一直没能使他从别人看法的奴役中解放出来。苏格拉底向克里托提过多次，他始终坚持一点：在做任何决定前必须首先多次考虑的是自己做的是否正义，而不是别人会怎样看。这个对话给我们的一个教导是，政治决策——如是否该接受处罚，即使处罚是不公正的——必须基于一个公民应该如何对待他的城市的一般理论。直到一个人抛开民意的影响，为公民责任的理论努力，一个人才不会误入政治事务的歧途。幸运的是， 克里托碰巧受到了苏格拉底的影响；若只凭借他自己，他将会为了男子气概的外表，成为欲望的奴隶。相比之下，尤西弗罗虽不在乎他的公众形象，但除了对于宗教事务有莫名的无知并且不加思索的自信外，也没有可以指引苏格拉底的地方。&lt;strong&gt;他们各自拥有各自的未经省察的人生。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果参考民众的意见确实能提出关于对与错，好和坏，正义和非正义等深思的问题，那对于认真思考关于道德问题的普遍意见是没有异议的。（克里托 47B）但是，假设发现有人进行这样的研究——并有人声称他们已经苦苦思考很久关于艰难的道德问题，并已成为这方面的专家。这正是柏拉图的《普罗塔哥拉》开篇描述的情况：与这部作品同名的著名智者来到雅典，希波克拉底要苏格拉底陪他去拜访这个著名智者和并成为他的学生。苏格拉底警告他：不要轻易委托出像灵魂这样珍贵的东西，特别是一个人不是他所宣称的专家（313A-C）。希波克拉底感到迫切需要道德教育，这种教育要超越孩童时期父母对自己的教育，他认为这样做是正确的。他渴望道德知识，但为了获得它，他必须了解自称有知识的人是否真的能教导自己。苏格拉底与希波克拉底的对话意味着，一个人永远不能放弃自己最重要的智慧，不能把自己的教育完全放在别人的手中。每个人都应该检验那些自称有道德知识的人，但成功且可靠地做到这一点，必须先教育自己，先决定要接受那种道德观点。这也有可能，有些人真的是道德专家，他们与他人讨论道德问题，能为他人作出最好的解答。这种可能性没有排除我们所有人都要过上经过省察的人生的需求除；除非我们这样做，否则我们给了别人极大伤害我们的机会。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;即使是那些钦佩并与苏格拉底在结交的人也不能避免误入歧途。&lt;/strong&gt; Charmides 和 Critias在苏格拉底被审判时都在场，明显没有从与苏格拉底的对话中受益 (他们是404 到403BC期间统治雅典的三十僭主之一)。 阿尔西比亚得也没有受益，他在后来的伯罗奔尼撒战争中背叛了雅典。（在以阿尔西比亚得名字命名的对话中，虽然这篇对话的作者不是柏拉图，但也值得仔细研究，柏拉图《会饮》中他的形象也同样如此 212C-223D）。阿尔西比亚得正如在柏拉图《会饮》中被描绘的那样，当着苏格拉底的面承认他对他的方式生活感到羞处（216B-ĉ）。但即使是这样，他也没能使自己严肃对待哲学。Charmides, Critias和 Alcibiades为未经省察的人生的困难提供了进一步例子，当我们不进行道德探寻时，我们也能发现这些困难。如果 像克里托和尤西弗罗 这样的人没有像Critias, Charmides, 和 Alcibiades一样成为违法犯罪者，那可真是走运。&lt;/p&gt;

&lt;p&gt;柏拉图《Ion》中，与作品同名对话者提供了一个明显的例子：他屈服于理性思考的能力，容纳别人的思想和情感。Ion是一个rhapsode——专业的荷马史诗朗诵和解说者。但他承认，他并不懂得荷马史诗的主题；甚至，他说，荷马史诗的思想浸入了他的灵魂，并通过他再浸入听众的灵魂。苏格拉底认为，灵感的传递开始与激发了诗人创作的的缪斯女神：创作诗句的作家为了接受来自神的影响抛开自己的智慧，反过来rhapsode的灵感来自于他读的诗，和听众听到了来自rhapsode的吟诵，也放弃了他们本身的思想，被听到的溢美之词所掌控（533d-35A）。在一定程度上，诗歌爱好者生活在诗歌的影响下，没有仔细省察其中的含义，他们是把自己的生活交由别人来掌握（如希波克拉底）。如果他们走运，激发他们的诗句的观点是真实的，那么他们就把自己交给了一个好的指引者。但这并不是过自己人生的方式。&lt;/p&gt;

&lt;p&gt;柏拉图的《高尔吉亚篇》探讨另一种模式：任由一个人的灵魂不加批判地受到别人的影响。雅典政治生活的成功需要一个人做任何吸引民众的事（502E，513B-C）：一个必须获得公众赞同的人，一个能用演讲取悦大众的人。一个人必须像糕点师一样，根据味蕾单独选择配料，而不考虑食用者的健康。Callicles是最后一个也是最大胆的的一个苏格拉底的对话者，他批评前两次对话者高尔吉亚和波卢什回答苏格拉底的方式，总是表现出他们不愿意说的感到羞愧的一面：他们自己不相信自己所说，并且按照设计好的方式在回答（482C-E）。&lt;/p&gt;

&lt;p&gt;人们在公共场合表现出表面的忠诚，实际上是在日常社会生活交往中表现的虚假的自我——在理想国第二章也有提到，&lt;strong&gt;当格劳孔提到，绝大多数人在隐身的状态下（戴着盖斯的戒指），都会去尝试做坏事，即使他们自己在公众面前会谴责这种不正当行为。&lt;/strong&gt; 在与他人的日常交往中，他们躲在一个虚假的自我之后，为了达到目的他们戴着面具。他们不但不知道自己应该拥有什么，什么值得拥有，还给他人留下自己缺乏且渴望之物的虚假印象。他们没有真正的自我——一个自己设计的自我，经过剖析的自我——所以他们的社会面具后隐藏的除了对别人的被动模仿外，什么都不是。柏拉图介于苏格拉底式人生与未经省察的人生之间的思考，是对普通人的毫不偏颇的刻画的来源。&lt;/p&gt;

&lt;p&gt;然而，在苏格拉底式对话中有一段，属于对普通人决定如何行动的标准，完全符合普通人的欲望。在柏拉图的《普罗塔哥拉》中，苏格拉底认为大多数人只凭借未来的快乐和痛苦来做决定（352B-56C）。他们认为快乐和痛苦是人类生活中的两个最强大的力量，他们把快乐作为唯一好的东西，痛苦是唯一不好的东西。因此，当他们拒绝追求快乐，或愿意接受痛苦，那只是因为他们进行了理性的计算，以便在今后获得最大的快乐，或者最少的痛苦。苏格拉底没有攻击《普罗塔哥拉》中提到的决策标准；在这篇对话中，他只是努力在证明一个这样做决定的人，懂得未来的快乐和痛苦是人生的关键因素，并且我们不能够和我们可以做到最好的行为产生对立。但令人惊讶的是，这里苏格拉底把普通人赋予了他对话者本没有的特点：一个人做采纳的决定表现出了他们想要的事物。&lt;/p&gt;

&lt;p&gt;然而，不能作出这样疯狂的假设，即仅仅因为有了真正的决策标准，人生就会变好。标准必须是正确的，因此一个人必须找到原因，为什么这些快乐和痛苦在人生中这样重要。这不是一个大多数人都想知道的问题。苏格拉底描述的他们的视野是情绪化的：&lt;strong&gt;他们认为，与欲望，恐惧，爱，快乐，和痛苦比较起来，知识是对人生的影响不那么重要。&lt;/strong&gt; 这种态度使他们不能思考真正的人生目标。即使这种知识是能够获取的，他们认为这对自己的行为也不会有什么影响。因此，他们向他们追求快乐的欲望投降，尽力避免痛苦。他们认为唯一值得他们付诸行动的是如何得到这些好东西的最佳组合。然而他们却不会想知道，是否有更好的方法去作出决定，这些好的东西是否有坏处，不会对将来的快乐和痛苦产生影响。因此，像大多与苏格拉底对话的人一样，他们冒着巨大的风险：一个人经历的属于自己的快乐和痛苦不是正确的决策知道——如果有其他一类事物也是有好有坏——那么大多数人的处境会很糟糕。&lt;/p&gt;

&lt;h1 id=&#34;大多数人都是其他人:f856045648ed1256c7d09f14d1393f16&#34;&gt;“ 大多数人都是其他人”&lt;/h1&gt;

&lt;p&gt;奥斯卡·王尔德的《深渊书简》写道：“大多数人都是其他人，他们的想法是别人的意见，他们的生活是在模仿，他们的激情都是虚假的。”，这和经过省察的人生有了类似的表达情感，也是一种接受苏格拉底式人生要求的方法。有人可能会对王尔德提出反对：那为什么我的观点不会也是别人的观点呢？如果我的想法就是别人的想法会不会非常可怖呢？我相信这个命题成立，那还存在什么异议——仅仅是因为别人认同我，我就应该停止这样做？&lt;/p&gt;

&lt;p&gt;王尔德抓住了要点，但如果我们拿他说，永远不会形成和抛弃智慧的标准（即衡量看法的能力）是致命的缺点，不假思索地接受其他人的话语、行为和感受。想要变得像另外一个人是很严重的错误，如果这仅仅是一个模仿他人的不假思索的行为，而不思考这是否值得模仿。如果我爱一幅画，它应该是因为我感受到了我自己的眼睛看到到的感情，并且从内心出发认为这值得欣赏。别人喜欢一幅画，能够使我趋向认为这幅画有某些地方值得欣赏的；但是如果我并没发现这幅画好在哪里，并且没有如实得表达出来，那么我并没有得到他们的真实看法（当然假定别人的看法是真实的没有误导性的）。同样的观点不仅适用于画作，同时也适用于对好坏的评价。如果我们只是模仿他人的思想和行为，但却不加以评判思想的质量，那么我们则是不是用自己的心灵在面对世界。&lt;strong&gt;我们成为了其他人，失去了自我。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;阿尔西比亚得这样描述苏格拉底：再以没有人能像他一样。（会饮 221C）。但是，尽管苏格拉底未经省察的人生不存在未经思索的模仿，但这没有要求我们为了不寻常而变得不寻常。苏格拉底也没有这种动机，认为真实的自我只有通过评价决定这个单独标准来评判——虽然正如我们看见的那样，他没有批评那些有羞耻心的人或是害怕别人异样的眼神的人。&lt;strong&gt;经过省察的人生值得度过，因为对于正确行为的标准我们知之甚少；所以出于我们的无知，我们的人生没有那么好。必须通过自己的努力、凭借自己的看法去发现正确的标准。但一旦我们发现了那些标准，我们都会变得相似。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;告诫我们不要过未经省察的人生的意思是&lt;strong&gt;通过自身努力才能发现真理&lt;/strong&gt;。相反，它最好被看成是基于人类发展和人类本性：人类天生就被渴望快乐憎恨痛苦。他们追求权力和地位，因为财富，权力，地位能带来快乐。这是成为雅典重要的价值观并不是偶然。（申辩 29d-e）；这是全人类社会的共同的重要价值。虽然每一个孩子都会接受的道德教育，学会如何与他人沟通，学会如“好”，“正义”和“可耻”这些共同的道德规范。教育会留下在道德认识上留下巨大空白，这个空白能够被进一步的求索来填补；因为童年形成的对正义和善良的概念依然是初步的。他们接受的有限教育和强大的心理力量做着产生了冲突，心理想要快乐，权力和地位，并且引导这行为。那些感到理所当然需要进一步的教育的人常常不知道如何满足这种需要：他们通过美丽的诗句和名誉权利，把自己的想法表达给喜欢用华丽肤浅方式的人。或者，他们干脆放弃屈服于心理因素，不再思考，并把其对渴望快乐和憎恨厌恶痛苦的强烈欲望作为正确行为的唯一标准。&lt;/p&gt;

&lt;p&gt;经过省察的人生的巨大价值在于，有一种可信的方式来找出我们头脑中的错误和教育里的缺陷，来填补关于好、正义、善良的概念空白。苏格拉底把它作为一个永远不会结束的过程。无论一个人在的道德认识上取得了多大的进步，学习依然是无止尽的，永远需要对于德性和其他道德规范的讨论。科学的思考方式已经普及。正如我们现在认识到的，科学永远从一个问题跳到另一个问题，每个解决方案的都会产生新的研究领域。我们可以说，苏格拉底在寻求一个道德的科学。这不是通过一次性回答所有的问题来终结所有的道德规范的讨论。无论我们学习了多少，学习都是无休止的，甚至可以温故而知新。那些在这个不断后退的目标方面取得进展的人，扩展了知识深度避免了在生活中范严重的错误。&lt;/p&gt;

&lt;h1 id=&#34;德性-知识-和良好的意愿:f856045648ed1256c7d09f14d1393f16&#34;&gt;德性，知识，和良好的意愿&lt;/h1&gt;

&lt;p&gt;这就是为什么我们应该过经过省察的人生。但这不是苏格拉底唯一坚持的观点。他说，我们也应该努力成为优秀的人，成为公正，勇敢 sophron（“克制”，“自制”，“稳健”，“节制”）的人，拥有所有的德性。这些劝诫互相有关系吗，或他们相互独立？——甚至可能互相之间有干扰？在《普罗塔哥拉》中，苏格拉底认为，它们是一个统一体；它们都有各自单独的一面，在正确的理解下是不会产生冲突的。&lt;strong&gt;德性的统一性所揭示的事实是，在寻求知识和智慧的不同道路上，它们之间相关联系。&lt;/strong&gt; 为了获得某种德性，一个人在人生的某一面必须做到足够好：例如，要变得勇敢，就要面对恐惧，并以正确的方式来处理恐惧。但要做到这一点，必须问自己人生和境况的哪些方面会有恐惧。是死亡吗？我们真的害怕它的到来吗？这不是我们的童年本身的道德修养能够让我们有能力来回答的一个问题。可以这样回答，省察自己的人生，找到真正有害的事情，这才是值得害怕的。这种方式反过来是不能被成功应用的，除非你了解对人来说，什么是好的；知道什么是好的，和知道什么是坏的是两个独立的问题。所以真正的勇气（不是愿意承担风险的愚蠢行为）不能从苏格拉底那种从事哲学探究的人身上拿走。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;同样的道理也适用于德性的品质：它是一个人拥有的好品格，并且它的好只能通过伦理探寻来发现。&lt;/strong&gt;在生命的初期，我们对真正的人生目标没有理解的时候，德性不要求被拥有；但很久之后，当我们知道了我们的人生目标是什么以及怎样去实现的时候，就不一样了。这就是苏格拉底在《普罗塔哥拉》中提出的，德性是智慧和知识的形式。&lt;strong&gt;这并不能得出这样的结论，即拥有德性的人没有感情。一个勇敢的人会害怕应该害怕的东西，有节制的人能通过恰当的方式体验快乐。&lt;/strong&gt; 但这些情绪反应将随着不同场合的改变流露。每个人都有幼稚情绪，这种情绪不只在童年才有，也终会被正义的情感所消灭和取代。&lt;/p&gt;

&lt;p&gt;在“美诺篇”中 ，苏格拉底认为，在以下几个方面，德性是知识的一种。考虑一个通常被认为是好的东西，德性除外：例如健康、美丽或强壮。这些优点从长远和总体的角度来看的确是好的，但肯定存在健康、美丽或强壮也有不利的方面的情况。例如，一个强大和健康的人会高估自己的力量和体能，承担可能致命的任务。男孩英俊的面孔会导致别人采用损害他智力发展的方式对待他。这些看起来好的东西是否真的好取决于它们怎样被运用，然后如何运用这些好的东西并没有与其相辅相成。要知道如何使用这些天资才能使一个人真正受益，是需要一定的检验和实践的问题。对于只能算作一般的资质的智力水平也是一样，在不同情况下都有利有弊。根据苏格拉底的说法，好坏的差异在于一个人对智慧的运用——对于如何利用好的食物的知识。（类似的论证请参见 Euthydemus 278d-82A）。&lt;/p&gt;

&lt;p&gt;他的理由和康德的《道德形而上学的基本原理》开篇十分类似。康德在开篇中写道：“除了善意，世界上在也没有一种能被称得上经过认证的好可以被构想出来，甚至在世界之外也没有”。他举出了很多例子：心灵品质如聪慧，和勇气，不同情形下的好“能够产生害处，“如果使用这些好的意愿不好的话”对于其他的幸运——权力，财富，名誉，健康，满足——也是如此。&lt;/p&gt;

&lt;p&gt;他们将变成“傲慢与自大，如果在心中没有一个良好的意愿的话。”康德的结论是，没有什么比但良好的意愿更具备“内在的无条件的价值”。3没有什么情况下良好的意愿会导致不好和损害。&lt;/p&gt;

&lt;p&gt;但康德在开篇提出的与苏格拉底的“美诺篇”中的观点类似， 但却没有得出一个好人应当进行道德探寻的结论——每天都像苏格拉底一样同别人探讨德性是什么。康德认为，普通的道德代理人不需要靠哲学帮忙来将规范概念应用到特定情况中，他们只需要防范规范刺激下动机的恶化。一个心智健全的成年人知道道义之路在哪。需要倍加努力的不是知道如何行事，而是有一个恰当的动机去行事，&lt;strong&gt;因为一个人的意愿可以强大到驱使一个人单纯的动机，并且克服所有强烈的动机。&lt;/strong&gt;对康德来说，完善一个人的灵魂是一个 净化心灵的过程，而不是哲学训练的过程。像苏格拉底一样，他得到了灵魂深处核心的价值的东西——但这些东西并不包含复杂抽象的哲学思考。只有在灵魂之外的堕落这一点上，两个哲学家的观点是一致的。&lt;/p&gt;

&lt;p&gt;跟随康德的引导，二十世纪最有影响力的道德哲学家之一WD罗斯认为任何受过教育有道德观念的人能够列出一个完整的职责表和一个完整的包括好坏事物的表。4 需要哲学技能和观察的是理解什么是一件事成为责任、成为好的。但是这样的职业与教导普通成年人应该如何度过人生没有关系。他认为，现实事物中的困难在于，当责任有冲突时知道该怎么做或者好坏发生冲突，没有哪方能胜过谁时。在这种情况下，做什么取决于每种情况下的细节，&lt;strong&gt;抽象的道德理论可以说不能引导普通民众的道德意识。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在一个方面，功利主义——唯一的职责是利益最大化—— - 比康德哲学更接近苏格拉底哲学。因为功利主义采取了修正的态度来对待道德常识框架，即孩子们从他们的父母和社区来学习。 孩子们通常没有被教育要尽可能多做善事。他们 之前设定的目标 是更加温和，而且往往他们学习的道德由各种各样的规则组成：不打架，不撒谎，不拿别人的东西、爱护兄弟姐妹，尊重长老。功利主义的主要思想——个人应该关注全世界的幸福，个人不能作恶无论这是否导致了更大的善——是大多数功利主义只有在阅读哲学和与教师，学生，朋友谈话时才话能发现。为了成为一个功利主义的人，必须先经过一段时间的自我反省：我们必须要通过做一些伤害，来讯问一个人是否真的认为最大的好的政策是附着在更广泛接受的规则的复杂的网络的进步。一个人也必须问自己最重要的问题：什么是真的好？但在另一个方面，功利主义完全不赞同苏格拉底被省察的人生的要求。因为，正如我们所看到的，苏格拉底的禁令是基于“每天都在探讨德性”或者可以给人类带来最大利益的话题。苏格拉底认为，道德谈话的主要内容是永远不会枯竭的；道德生活的主要问题永远不能得到一次性解决，因为每一个新的理解会带来新的问题。这是 功利主义强烈反对 的想法 。他们认为功利主义者的程式永远不能加以改进。他们 认为， 一旦人们了解到什么是好的， 就没有必要继续思考了：一个人应该仅仅尽可能产生多的，并且这不是一个需要进行的哲学探究的做法。我们需要互相探讨我们的行为会产生什么影响；但是我们不需要谈论道德。古代伦理学的苏格拉底式角色&lt;/p&gt;

&lt;p&gt;古代主要道德哲学家​站在苏格拉底的立场的至少有：他们认为如果哲学观念基于系统和抽象的思考，并进一步超越了我们童年时期学习的共同道德观，就很在某种程度上，影响一个人的思想，过上糟糕的人生，给别人造成巨大损害。他们认为，最重要的是我们必须达成一个共同理解，超过小孩子所认为的好。这是 的希腊伦理学的主要概念，苏格拉底式的对话展示了没有理解什么是好快是不可能理解德性，并且奠定了其核心基础。(Laches and Charmides 在其中所扮演的角色十分重要.) 柏拉图在《理想国》中的观点是，最高的智慧是对好的形式的认识——需要多年的科学训练来获得。亚里士多德说，学习道德的学生要从努力成为一个更好的人开始，为了做到这一点，他一定要了解人生首要的好。就像一个射手瞄准目标，他将能够切中要害——以他应该的方式生活及做决定——通过哲学论证，来发现对他重要的是什么。伊壁鸠鲁认为好就是明显且是被普遍接受的快乐；但他们认为的快乐在种类和价值上有极大不同，&lt;strong&gt;唯有哲学思考能确定哪些是最适合我们去追求的。&lt;/strong&gt;斯多葛学派认为，&lt;strong&gt;只有接受目的论的宇宙结构可以帮助我们摆脱破坏我们的幸福的幼稚情绪。&lt;/strong&gt;Pyrrhonian怀疑是唯一苏格拉底式的：他们自豪自己是唯一的哲学学校，&lt;strong&gt;继续从事苏格拉底后对智慧的探寻，避开一切定论的意见。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于所有的这些古代的思想家和学者，只有系统和抽象的思考才能揭示我们的生活中恰当行为的核心。如果我们只根据现成的第一手资料来做决定，不假思索，我们则会会误入歧途，对所有的成年人都是这样。这些苏格拉底的跟随者认为，我们必须采纳这两种看法中的一种：（柏拉图说）我们必须把让少数专家来完善和系统化我们的规范用词，并按照他们的理论制定方法，把道德意识渗入普通人的心中；或（苏格拉底说）我们每个人必须竭尽所能靠自己经历这个过程。无论哪种方式，我们在日常行为决策中都会用错道德规范的概念，除非这些概念是通过恰当使用的过程——即通过哲学理论——中形成的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记4 正则化</title>
      <link>http://nanshu.wang/post/2015-02-17</link>
      <pubDate>Tue, 17 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-17</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;正则化-regularization:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化 Regularization&lt;/h1&gt;

&lt;p&gt;为了和正规方程(normal equation)里&amp;rdquo;正规&amp;rdquo;区分开来，这里Regularization都译作“正则化”，有些地方也用的是“正规化”。以下内容来自&lt;a href=&#34;http://en.wikipedia.org/w/index.php?title=Regularization_(mathematics&#34;&gt;wikipedia&lt;/a&gt;)：&lt;/p&gt;

&lt;p&gt;正则化是指通过引入额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。正则化的一种理论解释是它试图引入&lt;a href=&#34;http://en.wikipedia.org/wiki/Occam%27s_razor&#34;&gt;奥卡姆剃刀原则&lt;/a&gt;。而从贝叶斯的观点来看，正则化则是在模型参数上引入了某种先验的分布。&lt;/p&gt;

&lt;p&gt;机器学习中最常见的正则化是$L_1$和$L_2$正则化。正则化是在学习算法的损失(成本)函数$E(X,Y)$的基础上在加上一项正则化参数项：$E(X,Y)+\alpha|w|$，其中$w$是参数向量，$\alpha$是正则项的参数值，需要在实际训练中调整。正则化在许多模型中都适用，对于线性回归模型来说，采用$L_1$正则化的模型叫作lasso回归，采用$L_2$的叫作ridge回归。对于logistic回归，神经网络，支持向量机，随机条件场和一些矩阵分解方法，正则化也适用。在神经网络中，$L_2$正则化又叫作“权重衰减”(weight decay)。$L_1$正则化能产生稀疏模型，因此在特征选择中很有用，但是$L_1$范式不可微，所以需要在学习算法中修改，特别是基于梯度下降的算法。&lt;/p&gt;

&lt;h1 id=&#34;过拟合问题:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;过拟合问题&lt;/h1&gt;

&lt;p&gt;欠拟合(也叫做高偏差(high bias))是指不能很好地拟合数据，一般是因为模型函数太简单或者特征较少。过拟合问题是指过于完美拟合了训练集数据，而对新的样本失去了一般性，不能有效预测新样本，这个问题也叫做高方差(high variances)。造成过拟合的原因可能是特征量太多或者模型函数过于复杂。线性回归和logistic回归都存在欠拟合和过拟合的问题。&lt;/p&gt;

&lt;p&gt;要解决过拟合的问题，通常有两种方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;减少特征数量&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;手动筛选特征&lt;/li&gt;
&lt;li&gt;采用特征筛选算法&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正则化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保留所有的特征，但尽可能使参数$\theta_j$尽量小。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;正则化在很多特征变量对目标值只有很小影响的情况下非常有用。&lt;/p&gt;

&lt;h1 id=&#34;成本函数:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;成本函数&lt;/h1&gt;

&lt;p&gt;在原有的成本函数的基础上加上使参数$\theta_j$正则化的项：&lt;/p&gt;

&lt;div&gt;
$$\min \frac1{2m}(\sum_{i=1}^m (h_{\theta}(x^{(i)}-y^{(i)}) + \lambda \sum_{j=1}^n \theta_j^2))$$
&lt;/div&gt;

&lt;p&gt;其中$\lambda$叫做正则化参数，决定了参数正则化项的影响大小。引入正则化参数项，可以避免模型过拟合的问题。如果$\lambda$的值设置过大，可能会使模型函数出现欠拟合的问题。&lt;/p&gt;

&lt;h1 id=&#34;正则化线性回归:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化线性回归&lt;/h1&gt;

&lt;h2 id=&#34;梯度下降法:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;梯度下降法&lt;/h2&gt;

&lt;p&gt;常数项$\theta_0$不用正则化，因此更新策略为：&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; \text{Repeat}\ \lbrace \newline
&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
&amp; \rbrace
\end{align*}
&lt;/div&gt;

&lt;p&gt;上面的式子可以写成：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j(1 - \alpha \frac{\lambda}m)-\alpha (\frac1m \sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}) $$
&lt;/div&gt;

&lt;p&gt;注意到$(1 - \alpha \frac{\lambda}m) &amp;lt; 1$，直观上可以理解为将式子中第一项$\theta_j$值减小了一点，第二项还是和无正则化的更新策略的第二项一致。&lt;/p&gt;

&lt;h2 id=&#34;正规方程:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正规方程&lt;/h2&gt;

&lt;p&gt;正则化正规方程求解$\theta$为：&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline
&amp; \text{where}\ \ L = 
\begin{bmatrix}
 0 &amp; &amp; &amp; &amp; \newline
 &amp; 1 &amp; &amp; &amp; \newline
 &amp; &amp; 1 &amp; &amp; \newline
 &amp; &amp; &amp; \ddots &amp; \newline
 &amp; &amp; &amp; &amp; 1 \newline
\end{bmatrix}
\end{align*}
&lt;/div&gt;

&lt;p&gt;$L$是(n+1)*(n+1)的矩阵，除了右上角的值为0外，对角线上其他值都为1。直观上理解，不包括右上角$X_0$项，$L$是一个单位矩阵。&lt;/p&gt;

&lt;p&gt;当$m\leq n$时，$X^TX$不可逆，但加入$\lambda L$后，$X^TX$也变得可逆了。&lt;/p&gt;

&lt;h1 id=&#34;正则化logistic回归:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化logistic回归&lt;/h1&gt;

&lt;h2 id=&#34;成本函数-1:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;成本函数&lt;/h2&gt;

&lt;p&gt;加上正则化参数$\theta$项：&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$
&lt;/div&gt;

&lt;p&gt;注意$\sum_{j=1}^n \theta_j^2$中参数的下标时从1到n，没有包括常数项$\theta_0$&lt;/p&gt;

&lt;h2 id=&#34;梯度下降:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;梯度下降&lt;/h2&gt;

&lt;p&gt;和线性回归一样，$\theta_0$和其他参数要分开更新：&lt;/p&gt;

&lt;div&gt;
    $$\begin{align*}
&amp; \text{Repeat}\ \lbrace \newline
&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
&amp; \rbrace
\end{align*}$$
&lt;/div&gt;

&lt;p&gt;这和正则化线性回归的更新策略是一样的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记3 有监督学习 分类 logistic回归</title>
      <link>http://nanshu.wang/post/2015-02-12</link>
      <pubDate>Thu, 12 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-12</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;分类问题:662363920bb15b8d466d06a6774c21df&#34;&gt;分类问题&lt;/h1&gt;

&lt;p&gt;分类问题和回归问题不同的是，分类问题的预测值$y$只能取离散值，而非连续值。首先来看一个二类分类问题，预测值$y$只能取0或1。0又被称作负例(negative class)，1被称作正例(positive class)。通常也用&amp;rdquo;-&amp;ldquo;,&amp;rdquo;+&amp;ldquo;符号来表示。对于一个样本集输入$x^{(i)}$，对应的目标值$y^{(i)}$也被为标注(lable)。&lt;/p&gt;

&lt;h2 id=&#34;logistic回归:662363920bb15b8d466d06a6774c21df&#34;&gt;logistic回归&lt;/h2&gt;

&lt;p&gt;也可以用线性回归的方法运用到分类问题上，但是这样做很容易得到不好的结果。稍微改变一下我们的假设函数$h_\theta(x)$，使其的取值在{0,1}范围内：&lt;/p&gt;

&lt;div&gt;
$$h_\theta(x) = g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$$
$$g(z)=\frac1{1+e^{-z}}$$
&lt;/div&gt;

&lt;p&gt;$g(z)$叫做logistic函数，也叫做sigmoid函数。$g(z)$的函数图像如下：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/logistic-function.png&#34; alt=&#34;logistic-function&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;当$z\rightarrow \infty$时，$g(z)$趋近于1；当$z\rightarrow -\infty$时，$g(z)$趋近于0。因此$h(x)$的取值在0到1范围内。&lt;/p&gt;

&lt;p&gt;求$g(z)$的导数可得：&lt;/p&gt;

&lt;div&gt;
$$g&#39;(z)=g(z)(1-g(z))$$
&lt;/div&gt;

&lt;p&gt;下面是对分类问题作出的一些假设，预测函数$h_\theta(x)$将给出样本目标值分类为1的概率：&lt;/p&gt;

&lt;div&gt;
$$P(y=1|x;\theta) = h_{\theta}(x)$$
$$P(y=0|x;\theta) = 1-h_{\theta}(x)$$
$$p(y|x;\theta) = (h_{\theta}(x)^y(1-h_{\theta}(x)^{1-y}))$$
&lt;/div&gt;

&lt;p&gt;那么$\theta$的似然函数为：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
L(\theta) =&amp; p(\overrightarrow y|X;\theta)\\
=&amp; \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=&amp; \Pi_{i=1}^m (h_\theta(x^{(i)})^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}})
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;求log似然函数:&lt;/p&gt;

&lt;div&gt;
$$l(\theta)=\log L(\theta)=\sum_{i=1}^m y^{(i)}\log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))$$
&lt;/div&gt;

&lt;p&gt;求最大似然估计，同样可以采用梯度下降的方法，更新$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta+\alpha \nabla_\theta l(\theta)$$
&lt;/div&gt;

&lt;p&gt;这里是求最大值，因此更新$\theta$是加上$l(\theta)$的偏导。&lt;/p&gt;

&lt;p&gt;解之得到：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j+\alpha (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$$
&lt;/div&gt;

&lt;p&gt;这和之前在线性回归模型得到的LMS更新策略一样，这并不是巧合，而是因为线性回归和logistic回归都属于广义线性模型(GLM models)。&lt;/p&gt;

&lt;h2 id=&#34;perceptron学习算法:662363920bb15b8d466d06a6774c21df&#34;&gt;perceptron学习算法&lt;/h2&gt;

&lt;p&gt;有趣的是，如果这里不采用logistic函数，而是采用一种简单粗暴的只考虑阈值的函数g(z)：&lt;/p&gt;

&lt;div&gt;
$$g(z) = \begin{cases}1,if z\geq 0\\0,if z &lt; 0\end{cases}$$
&lt;/div&gt;

&lt;p&gt;我们得到的更新$\theta$的策略和采用logistic函数得到的策略是一致。这种算法叫做感知器(perceptron)学习算法，感知器原指一种用来刻画大脑神经元的粗糙模型。虽然表面上看这种简单粗暴的方式和其他算法得到的结果是一样的，但是这是一种和logistic回归以及最小二乘线性回归非常不同的一类算法，它不能推导出有意义的概率解释，也不能通过极大似然估计得到。&lt;/p&gt;

&lt;h2 id=&#34;牛顿法:662363920bb15b8d466d06a6774c21df&#34;&gt;牛顿法&lt;/h2&gt;

&lt;p&gt;为了求$f(\theta)=0$时$\theta$的取值，牛顿法每次更新$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-\frac{f(\theta)}{f&#39;(\theta)}$$
&lt;/div&gt;

&lt;p&gt;要最大化似然函数$l(\theta)$的值，使其导数$l&amp;rsquo;(\theta)＝0$。更新策略为：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-\frac{l&#39;(\theta)}{l&#39;&#39;(\theta)}$$
&lt;/div&gt;

&lt;p&gt;当$\theta$为向量时，推广更一般的牛顿法，这种方法也叫做牛顿－拉普森法(Newton-Raphson method)：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-H^{-1} \nabla_\theta l(\theta)$$
&lt;/div&gt;

&lt;p&gt;$\nabla_\theta l(\theta)$是$l(\theta)$对于$\theta$的偏导。$H$是$(n+1)*(n+1)$的矩阵，叫做Hessian：&lt;/p&gt;

&lt;div&gt;
$$H_{ij}=\frac{\delta^2 l(\theta)}{\delta \theta_i \delta \theta_j}$$
&lt;/div&gt;

&lt;p&gt;牛顿法收敛的速度通常比批量梯度下降要快，但是牛顿法每次迭代的计算量更大，每次迭代重新计算Hessian矩阵，需要$O(n^2)$的时间复杂度。但在n没有很大的情况下，牛顿法是更有效率的。将牛顿法用于logistic回归的log似然函数$l(\theta)$得到的方法也被称为Fisher scoring。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释</title>
      <link>http://nanshu.wang/post/2015-02-11</link>
      <pubDate>Wed, 11 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-11</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:a41c550b5852f048f913986db76cdf89&#34;&gt;有监督学习&lt;/h1&gt;

&lt;h2 id=&#34;局部加权线性回归-locally-weighted-linear-regression:a41c550b5852f048f913986db76cdf89&#34;&gt;局部加权线性回归(Locally weighted linear regression)&lt;/h2&gt;

&lt;p&gt;参数学习算法(parametric learning algorithm)：参数个数固定&lt;/p&gt;

&lt;p&gt;非参数学习算法(non-parametric learning algorithm)：参数个数随样本增加&lt;/p&gt;

&lt;p&gt;特征选择对参数学习算法非常重要，否则会出现下面的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欠拟合(underfitting)：特征过少，模型过于简单，高偏差(high bias)，不能很好拟合训练集&lt;/li&gt;
&lt;li&gt;过拟合(overfitting)：特征过多，模型过于复杂，高方差(high variance)，过于拟合训练集，不能很好预测新样本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于非参数学习算法来说，并不需要进行精心的特征选择，局部加权线性回归就是这样。&lt;/p&gt;

&lt;p&gt;局部加权回归又叫做Loess，其成本函数为：&lt;/p&gt;

&lt;div&gt;
$$\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$$
&lt;/div&gt;

&lt;p&gt;$w^{(i)}$是非负权重，通常定义$w^{(i)}$如下：&lt;/p&gt;

&lt;div&gt;
$$w^{(i)}=exp(- \frac{(x^{(i)}-x)^2}{2\tau^2})$$
&lt;/div&gt;

&lt;p&gt;权值取决于用于预测的输入变量$x$的值：离$x$越近的样本，权值越接近1；越远的样本，越接近0。$\tau$被称为带宽(bandwidth)参数，决定了以$x$为中心，样本权重递减的速度。$\tau$越大，递减速度越慢；$\tau$越小，递减速度越快。&lt;/p&gt;

&lt;p&gt;参数学习算法的参数是固定的，一经学习得到不再改变。而非参数学习算法的参数并不固定，每次预测都要重新学习一组新的参数，并且要一直保留完整的训练样本集。当样本集很大时，局部加权回归的计算开销会很大，Andrew Moore提出的KD-tree方法可以在大数据集上的计算更高效。&lt;/p&gt;

&lt;h2 id=&#34;回归模型的概率解释:a41c550b5852f048f913986db76cdf89&#34;&gt;回归模型的概率解释&lt;/h2&gt;

&lt;p&gt;遇到一个回归问题，为什么采用线性回归，又为什么采用最小二乘作为成本函数？其实最小二乘回归中蕴含了非常自然的概率假设。假设目标值和输入值之间满足以下关系：&lt;/p&gt;

&lt;div&gt;
$$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$$
&lt;/div&gt;

&lt;p&gt;其中，$\epsilon^{(i)}$是误差项，包含了模型未考虑到的影响目标值的因素和随机噪声。假设误差项独立同分布，服从均值为0，方差为$\sigma^2$的高斯分布（正态分布），即$\epsilon^{(i)}\sim N(0,\sigma^2)$。$\epsilon^{(i)}$的密度函数为：&lt;/p&gt;

&lt;div&gt;
$$p(\epsilon^{(i)})=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$
&lt;/div&gt;

&lt;p&gt;也就是说：&lt;/p&gt;

&lt;div&gt;
$$p(y^{(i)}|x^{(i)};\theta)=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
&lt;/div&gt;

&lt;p&gt;或者也可以这样写：$(y^{(i)}|x^{(i)};\theta) \sim N(\theta^Tx^{(i)},\sigma^2)$。&lt;/p&gt;

&lt;p&gt;为什么采用正态分布？一种原因是数学上处理起来比较便利，二是因为根据中心极限定理，独立的随机变量的和，即多种随机误差的累积，其总的影响是接近正态分布的。&lt;/p&gt;

&lt;p&gt;$p(y^{(i)}|x^{(i)};\theta)$的含义是给定条件$x^{(i)}$，参数设定为$\theta$时，$y^{(i)}$的分布值。不能将$\theta$看作是概率条件，因为$\theta$不是随机变量，而是实际存在的真实值，虽然我们不知道真实值到底是多少。因此在以上的式子中，用了分号而不是逗号来区分$x^{(i)}$和$\theta$。&lt;/p&gt;

&lt;p&gt;定义$\theta$的似然(likelihood)函数：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
L(\theta)=&amp; p(y|X;\theta)\\
=&amp; \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=&amp; \Pi_{i=1}^m \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;似然(likelihood)和概率(probability)实际上是一个东西，但是似然函数是对参数$\theta$定义的，为了加以区分，使用了似然这一术语。我们可以说参数的似然，数据的概率，但不能说数据的似然，参数的概率。&lt;/p&gt;

&lt;p&gt;极大似然估计的含义是选择参数$\theta$，使参数的似然函数最大化，也就是说选择参数使得已有样本数据出现的概率最大。&lt;/p&gt;

&lt;p&gt;为方便求解，再定义函数$l(\theta)$：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
l(\theta) =&amp; \log L(\theta) \\
=&amp; \sum_{i=1}^m \log \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\
=&amp; m\log (\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2)
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;可得，要最大化似然函数$L(\theta)$，也就是最大化$l(\theta)$，也就是最小化：&lt;/p&gt;

&lt;div&gt;
$$\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$$
&lt;/div&gt;

&lt;p&gt;这个式子刚好是最小二乘法中定义的成本函数$J(\theta)$。总结一下，最小二乘回归模型刚好就是在假设了误差独立同服从正态分布后，得到的最大似然估计。注意到，正态分布中的方差$\sigma^2$的取值对模型并没有影响。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记1 有监督学习 线性回归 LMS算法 正规方程</title>
      <link>http://nanshu.wang/post/2015-02-10</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-10</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:b33d274e87081502d65882ed2d51cd57&#34;&gt;有监督学习&lt;/h1&gt;

&lt;p&gt;先理清几个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$x^{(i)}$表示&amp;rdquo;输入&amp;rdquo;变量(&amp;ldquo;input&amp;rdquo; variables)，也称为特征值(features)。&lt;/li&gt;
&lt;li&gt;$y^{(i)}$表示&amp;rdquo;输出&amp;rdquo;变量(&amp;ldquo;output&amp;rdquo; variables)，也称为目标值(target)。&lt;/li&gt;
&lt;li&gt;一对$(x^{(i)},y^{(i)})$称为一个训练样本(training example)，用作训练的数据集就是就是一组$m$个训练样本${(x^{(i)},y^{(i)});i=1,&amp;hellip;,m}$，被称为训练集(training set)。&lt;/li&gt;
&lt;li&gt;$X$表示输入变量的取值空间，$Y$表示输出变量的取值空间。那么$h:X \rightarrow Y$是训练得到的映射函数，对于每个取值空间X的取值，都能给出取值空间Y上的一个预测值。函数$h$的含义为假设(hypothesis)。&lt;/li&gt;
&lt;li&gt;图形化表示整个过程：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/supervised-learning.png&#34; alt=&#34;supervised-learning&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当预测值y为连续值时，则有监督学习问题是回归(regression)问题；预测值y为离散值时，则为分类(classification)问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;线性回归-linear-regression:b33d274e87081502d65882ed2d51cd57&#34;&gt;线性回归(Linear Regression)&lt;/h2&gt;

&lt;p&gt;先简单将y表示为x的线性函数：&lt;/p&gt;

&lt;div&gt;
$$h(x) = \sum_{i=0}^{n}\theta _ix_i=\theta^Tx$$
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;$\theta$ 称为参数(parameters)，也叫做权重(weights)，参数决定了$X$到$Y$的射映空间。&lt;/li&gt;
&lt;li&gt;用$x_0=1$来表示截距项(intercept term)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有了训练集，如果通过学习得到参数$\theta$？&lt;/p&gt;

&lt;p&gt;一种方法是，让预测值$h(x)$尽量接近真实值y，定义成本函数(cost function):&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = \frac12\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)^2$$
&lt;/div&gt;

&lt;p&gt;这实际上就是最小二乘成本函数，我们把这个回归模型叫做普通最小二乘回归模型(ordinary least squares regression model)。&lt;/p&gt;

&lt;h2 id=&#34;lms算法:b33d274e87081502d65882ed2d51cd57&#34;&gt;LMS算法&lt;/h2&gt;

&lt;p&gt;为了找到使成本函数$J(\theta)$最小的参数$\theta$，采用搜索算法：给定一个$\theta$的初值，然后不断改进，每次改进都使$J(\theta)$更小，直到最小化$J(\theta)$的$\theta$的值收敛。&lt;/p&gt;

&lt;p&gt;考虑梯度下降(gradient descent)算法：从初始$\theta$开始，不断更新：&lt;/p&gt;

&lt;p&gt;$$\theta_j:=\theta_j-\alpha \frac{\delta}{\delta\theta_j}J(\theta)$$&lt;/p&gt;

&lt;p&gt;注意，更新是同时对所有$j=0,&amp;hellip;,n$的$\theta_j$值进行。$\alpha$被称作学习率(learning rate)，也是梯度下降的长度，若$\alpha$取值较小，则收敛的时间较长；相反，若$\alpha$取值较大，则可能错过最优值。&lt;/p&gt;

&lt;p&gt;假设我们只有一个训练样本$(x,y)$，此时$J(\theta) = \frac12(h_{\theta}(x)-y)^2$，求偏导项得到：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
\frac{\delta}{\delta\theta_j}J(\theta) =&amp; \frac{\delta}{\delta\theta_j}\frac12(h_{\theta}(x)-y)^2\\
=&amp; (h_{\theta}(x)-y)*\frac{\delta}{\delta\theta_j}(h_{\theta}(x)-y)\\
=&amp; ((h_{\theta}(x)-y))*\frac{\delta}{\delta\theta_j}(\sum_{i=0}^{n}\theta_ix_i-y)\\
=&amp; (h_{\theta}(x)-y)*x_j
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;每次按照以下式子更新$\theta_j$的值：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))*x_j^{(i)}$$
&lt;/div&gt;

&lt;p&gt;这种更新方法叫做LMS更新策略(Least Mean Squares update rule)，也叫做Widrow-Hoff 学习策略。&lt;/p&gt;

&lt;p&gt;采用LMS方法，参数更新的次数和误差项$(y^{(i)}-h_{\theta}(x^{(i)}))$成正比。也就是说，如果预测值与真实值的误差项较小，则参数调整改变不会很大，相反，如果误差项较大，参数进行的调整更大。&lt;/p&gt;

&lt;p&gt;如果训练集不只一个训练样本，可以采用以下方法更新参数：&lt;/p&gt;

&lt;p&gt;Repeat until convergence{&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$\theta_j:=\theta_j+\alpha\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)})x_j^{(i)})$&lt;/code&gt; (for every j)&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;实际上，这里的求和刚好是$\frac{\delta J(\theta)}{\delta\theta_j}$的值。这种方法每一步更新都会遍历每所有的训练样本，因此被称作批量梯度下降(batch gradient descent)。&lt;/p&gt;

&lt;p&gt;梯度下降法通常容易受局部最优值的影响，但这里的最优问题只有一个全局最优值，没有局部最优值。因此梯度下降总是收敛到全局最优解（学习率$\alpha$不能取太大，否则错过最优值）。&lt;/p&gt;

&lt;p&gt;除了批量梯度下降，还有一种方法叫做随机梯度下降(stochastic gradient descent)，也叫做增量梯度下降(incremental gradient descent)。其更新策略为：&lt;/p&gt;

&lt;p&gt;Loop{&lt;/p&gt;

&lt;p&gt;for i=1 to m,{&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}$&lt;/code&gt; (for every j).&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;随机梯度下降和批量梯度下降不同点在于，批量梯度下降每一步更新$\theta$值，都需要遍历全部的训练样本，而随机梯度下降在遇到每个训练样本时，更新$\theta$之后继续处理下一个样本，每个样本只遍历一次，算法的学习时间比批量梯度下降快很多。但是，随机梯度下降可能永远不会收敛到全局最优值，而是在成本函数$J(\theta)$最优值周围附近摇摆。但是在实际问题中，接近最优值的参数值可能已经是足够好的结果了，特别是对于数据量非常大的训练集来说，随机梯度下降是比批量梯度下降更好的选择。&lt;/p&gt;

&lt;p&gt;在实际使用梯度下降算法时，将输入变量归一到同一取值范围，能够减少算法的迭代次数。这是因为$\theta$在小的取值范围内会下降很快，但在大的取值范围内会下降较慢。并且当输入变量取值范围不够均衡时，$\theta$更容易在最优值周围波动。采用特征缩放(feature scaling)和均值归一化(mean normalization)可以避免这些问题。选择学习率$\alpha$的值，要观察$J(\theta)$值在每次迭代后的变化。已经证明了如果$\alpha$的取值足够小，则$J(\theta)$每次迭代后的值都会减少。如果$J(\theta)$在某次迭代后反而增加了，说明学习率$\alpha$的值应该减小，因为错过了最优值。Andrew Ng推荐的一个经验是每次将$\alpha$减少3倍。&lt;/p&gt;

&lt;h2 id=&#34;正规方程-the-normal-equations:b33d274e87081502d65882ed2d51cd57&#34;&gt;正规方程(The normal equations)&lt;/h2&gt;

&lt;p&gt;梯度下降是最小化$J(\theta)$的一种方式，正规方程是另一种求解参数$\theta$的方法，这种方法可以直接求出最优值参数结果，不需要迭代更新，也不需要事先对数据进行归一化预处理。这种方法实际上是直接求出$J(\theta)$的导数，并令其为0。&lt;/p&gt;

&lt;div&gt;
$$J(\theta)=\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)}))^2=\frac12(X\theta-\overrightarrow y)^T(X\theta-\overrightarrow y)$$

$$\nabla_{\theta}J(\theta)=0$$ 
&lt;/div&gt;

&lt;p&gt;解之，&lt;/p&gt;

&lt;div&gt;
$$\nabla_{\theta}J(\theta) = X^TX\theta-X^T\overrightarrow y=0$$
&lt;/div&gt;

&lt;p&gt;得到正规方程：&lt;/p&gt;

&lt;div&gt;
$$X^TX\theta=X^T\overrightarrow y$$
&lt;/div&gt;

&lt;p&gt;求解$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta=(X^TX)^{-1}X^T\overrightarrow y$$
&lt;/div&gt;

&lt;p&gt;正规方程求解$\theta$的时间复杂度为$O(n^3)$，n是特征数量。当特征数量很大时，正规方程求解会很慢。Andrew Ng给出的一个经验参考是：当n&amp;gt;10,000时，采用梯度下降比正规方程更好。&lt;/p&gt;

&lt;p&gt;比较一下正规方程和梯度下降：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;正规方程&lt;/th&gt;
&lt;th&gt;梯度下降&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;不需要调整参数&lt;/td&gt;
&lt;td&gt;需要调整参数$\alpha$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;不需要迭代&lt;/td&gt;
&lt;td&gt;需要迭代更新$\theta$值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n较大时效率低&lt;/td&gt;
&lt;td&gt;n较大时效率也不错&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;正规方程还会遇到$X^TX$不可逆的情况，通常这是因为输入变量中存在线性相关的变量或者是因为特征太多($n\geq m$)。解决方法是去掉线性相关的冗余变量，或者删掉一些特征。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
