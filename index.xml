<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title> on Nanshu&#39;s blog </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://nanshu.wang/</link>
    <language>en-us</language>
    <author>Nanshu Wang</author>
    <copyright>Copyright (c) 2015, Nanshu Wang; all rights reserved.</copyright>
    <updated>Sun, 15 Mar 2015 00:00:00 UTC</updated>
    
    <item>
      <title>python numpy 计算自相关系数</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/python-numpy%E8%AE%A1%E7%AE%97%E8%87%AA%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</link>
      <pubDate>Sun, 15 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/python-numpy%E8%AE%A1%E7%AE%97%E8%87%AA%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</guid>
      <description>&lt;p&gt;在分析时间序列时，通常需要计算一个序列的自相关系数。自相关(&lt;a href=&#34;http://en.wikipedia.org/wiki/Autocorrelation&#34;&gt;Autocorrelation&lt;/a&gt;)又叫做序列相关，通常采用自相关系数来发现序列的重复规律，周期等信息。&lt;/p&gt;

&lt;p&gt;我们有序列$X:x_1,x_2,x_3,&amp;hellip;,x_n$，设$X_{s,t}$为$s$时刻开始，$t$时刻结束的序列：$x_s,x_{s+1}&amp;hellip;,x_{t-1},x_t$。$\mu_{s,t}$为序列$X_{s,t}$的均值，$\sigma_{s,t}$为序列$X_{s,t}$的标准差。那么一阶自相关系数为：&lt;/p&gt;

&lt;div&gt;

$$R(1) = \frac{E(X_{2,n}-\mu_{2,n})(X_{1,n-1}-\mu_{1,n-1})}{\sigma_{2,n}\sigma_{1,n-1}}$$

&lt;/div&gt;

&lt;p&gt;同理$k$阶自相关系数为：&lt;/p&gt;

&lt;div&gt;

$$R(k) = \frac{E(X_{k+1,n}-\mu_{k+1,n})(X_{1,n-k}-\mu_{1,n-k})}{\sigma_{k+1,n}\sigma_{1,n-k}}$$

&lt;/div&gt;

&lt;p&gt;python的numpy库里没有直接计算序列自相关系数的函数，但有计算两个不同序列的相关系数函数： &lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html&#34;&gt;correlate&lt;/a&gt;。给定两个序列$X,Y$，correlation(X,Y) = $\sum XY$。可以利用correlate函数计算$X$的自相关性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autocorrelation(x,lags):#计算lags阶以内的自相关系数，返回lags个值，分别计算序列均值，标准差
	n = len(x)
	x = numpy.array(x)
	result = [numpy.correlate(x[i:]-x[i:].mean(),x[:n-i]-x[:n-i].mean())[0]\
		/(x[i:].std()*x[:n-i].std()*(n-i)) \
		for i in range(1,lags+1)]
	return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常在实际中，很多时间序列的均值和标准受时间变化的影响较小，可以看作是恒定的，此时：&lt;/p&gt;

&lt;div&gt;

$$R(k) = \frac{E(X_{k+1,n}-\mu)(X_{1,n-k}-\mu)}{\sigma^2}$$

&lt;/div&gt;

&lt;p&gt;同样可以利用correlate函数实现：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autocorrelation(x,lags):#计算lags阶以内的自相关系数，返回lags个值，将序列均值、标准差视为不变
	n = len(x)
	x = numpy.array(x)
	variance = x.var()
	x = x-x.mean()
	result = numpy.correlate(x, x, mode = &#39;full&#39;)[-n+1:-n+lags+1]/\
		(variance*(numpy.arange(n-1,n-1-lags,-1)))
	return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考：&lt;a href=&#34;http://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation&#34;&gt;http://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记5 神经网络1 模型表达</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1/</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1/</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;神经网络:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经网络&lt;/h1&gt;

&lt;h2 id=&#34;非线性假设:d35c8128725ed53542064246ed42d2c0&#34;&gt;非线性假设&lt;/h2&gt;

&lt;p&gt;在特征变量数较大的情况下，采用线性回归会很难处理，比如我的数据集有3个特征变量，想要在假设中引入所有特征变量的平方项：&lt;/p&gt;

&lt;div&gt;
$$g(\theta_0 + \theta_1x_1^2 + \theta_2x_1x_2 + \theta_3x_1x_3  + \theta_4x_2^2 + \theta_5x_2x_3  + \theta_6x_3^2 )$$
&lt;/div&gt;

&lt;p&gt;共有6个特征，假设我们想知道选取其中任意两个可重复的平方项有多少组合，采用允许重复的组合公式计算$\frac{(n+r-1)!}{r!(n-1)!}$，共有$\frac{(3 + 2 - 1)!}{(2!\cdot (3-1)!)} = 6$种特征变量的组合。对于100个特征变量，则共有$\frac{(100 + 2 - 1)!}{(2\cdot (100-1)!)} = 5050$个新的特征变量。&lt;/p&gt;

&lt;p&gt;可以大致估计特征变量的平方项组合个数的增长速度为$\mathcal{O}(\frac{n^2}2)$，立方项的组合个数的增长为$\mathcal{O}(n^3)$。这些增长都十分陡峭，让实际问题变得很棘手。&lt;/p&gt;

&lt;p&gt;在变量假设十分复杂的情况下，神经网络提供了另一种机器学习算法。&lt;/p&gt;

&lt;h1 id=&#34;神经元和大脑:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经元和大脑&lt;/h1&gt;

&lt;p&gt;神经网络是对大脑工作方式的一种简单模拟。有证据表明，大脑对所有的功能（如视觉，触觉，听觉等）都采用了一种“学习算法”。将听觉皮层和视觉神经连接到一起，听觉皮层可以学会“看见”。这种理论叫作“neuroplasticity”，已经得到了很多例子和实验验证。&lt;/p&gt;

&lt;h1 id=&#34;模型表达:d35c8128725ed53542064246ed42d2c0&#34;&gt;模型表达&lt;/h1&gt;

&lt;p&gt;简单来说，每个神经元都有输入（树突dendrites）和输出（轴突axons）。在模型中，输入就是我们的特征变量，输出就是模型假设的结果。&lt;/p&gt;

&lt;p&gt;在神经网络中，分类问题通常采用logistic函数，也叫做sigmoid激活函数(sigmoid activation function)。$\theta$参数有时也被称为权重(weights)。&lt;/p&gt;

&lt;p&gt;下面是一种简单的神经网络：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/neuron_model.jpg&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;第一层是输入节点(nodes)，第二层是输出节点，也就是我们假设函数的结果$h_{\theta}(x)$。&lt;/p&gt;

&lt;p&gt;第一层叫作“输入层”(input layer)，最后一层叫作“输出层”(output layer)，输入层和输出层之间还可以有多层，统称为“隐藏层”(hidden layer)。如下图中，第二层就叫隐藏层。隐藏层节点表示为$a^2_0 \cdots a^2_n$，被称作“激活单元(activation units)”。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/neuron_network_3layers.jpg&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;$a_i^{(j)}$表示第$j$层的$i$单元被“激活”，$\Theta^{(j)}$表示从第$j$层到第$j+1$层的权重矩阵。&lt;/p&gt;

&lt;p&gt;上图的神经网络中，激活单元的计算分别为：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline
a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline
h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;每一层都有自己的权重矩阵$\Theta^{(j)}$，如果第$j$层有 $s_j$ 个单元，第$j+1$层有$s_{j+1}$个单元，则$\Theta^{(j)}$是$s_{j+1} \times (s_j+1)$的矩阵。&amp;rdquo;+1&amp;rdquo;是因为第$j$层包括一个&amp;rdquo;偏差节点(bias nodes)“，$x_0$和$\Theta_0^{(j)}$。换句话说，输出节点不包括偏差节点，但输入节点会包括偏差节点。&lt;/p&gt;

&lt;p&gt;举个例子，第一层有2个输入节点，第二层有4个激活单元。$\Theta^{(1)}$的维度为$4 \times 3$。&lt;/p&gt;

&lt;p&gt;下面将以上模型表达向量化。&lt;/p&gt;

&lt;p&gt;采用$z^{(i)}_k$表示$g$函数的输入，那么有：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
a_1^{(2)} = g(z_1^{(2)}) \newline
a_2^{(2)} = g(z_2^{(2)}) \newline
a_3^{(2)} = g(z_3^{(2)}) \newline
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;给定第$j$层的节点$k$，变量$z$等于：&lt;/p&gt;

&lt;div&gt;
$$
z_k^{(j)} = \Theta_{k,0}^{(j-1)}x_0 + \Theta_{k,1}^{(j-1)}x_1 + \cdots + \Theta_{k,n}^{(j-1)}x_n 
$$
&lt;div&gt;

$x$和$z^{(j)}$的向量表示为：

&lt;div&gt;
$$
\begin{align*}
x = 
\begin{bmatrix}
x_0 \newline
x_1 \newline
\cdots \newline
x_n
\end{bmatrix} &amp;
z^{(j)} = 
\begin{bmatrix}
z_1^{(j)} \newline
z_2^{(j)} \newline
\cdots \newline
z_n^{(j)}
\end{bmatrix}
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;因此，$z^{(j}) = \Theta^{(j-1)} x$&lt;/p&gt;

&lt;p&gt;令$x = a^{(j-1)}$，则$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$&lt;/p&gt;

&lt;p&gt;最后的结果为&lt;/p&gt;

&lt;div&gt;
$$
h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) = g(\Theta^{(j)}a^{(j)})
$$
&lt;/div&gt;

&lt;h1 id=&#34;神经网络拟合逻辑运算:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经网络拟合逻辑运算&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ AND\ x_2$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;神经网络模型：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
g(z^{(2)})
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;其中，$x_0$为偏差，恒等于1。&lt;/p&gt;

&lt;p&gt;权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
-30 &amp; 20 &amp; 20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;仅当$x_1$和$x_2$同时为1时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \newline
\newline
&amp; x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline
&amp; x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline
&amp; x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline
&amp; x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1
\end{align*}
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ NOR\ x_2$, $NOR$为$NOT\ OR$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;模型同上，权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
10 &amp; -20 &amp; -20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;仅当$x_1$和$x_2$同时为0时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ OR\ x_2$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;模型同上，权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
-10 &amp; 20 &amp; 20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;当$x_1$和$x_2$不同时为0时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ XNOR\ x_2$，$XNOR$为$NOT\ XOR$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将前面得到的模型组合起来$x_1\ XNOR\ x_2 = (x_1\ AND\ x_2)\ OR\ (x_1\ XOR\ x_2)$&lt;/p&gt;

&lt;p&gt;模型如下：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_1^{(2)} \newline
a_2^{(2)} 
\end{bmatrix} \rightarrow
\begin{bmatrix}
a^{(3)}
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;第一层到第二层的$\Theta^{(1)}$分别表示$AND$和$NOR$操作：&lt;/p&gt;

&lt;div&gt;
$$
\Theta^{(1)} = 
\begin{bmatrix}
-30 &amp; 20 &amp; 20 \newline
10 &amp; -20 &amp; -20
\end{bmatrix}
$$
&lt;/div&gt;  

&lt;p&gt;第二层到第三层的$\Theta^{(2)}$表示$OR$操作：&lt;/p&gt;

&lt;div&gt;
$$
\Theta^{(2)} = 
\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}
$$
&lt;/div&gt;

&lt;h1 id=&#34;多类分类问题:d35c8128725ed53542064246ed42d2c0&#34;&gt;多类分类问题&lt;/h1&gt;

&lt;p&gt;输出用向量来表示多类分类问题：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2 \newline
\cdots \newline
x_n
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(2)} \newline
a_1^{(2)} \newline
a_2^{(2)} \newline
\cdots
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(3)} \newline
a_1^{(3)} \newline
a_2^{(3)} \newline
\cdots
\end{bmatrix} \rightarrow \cdots \rightarrow
\begin{bmatrix}
h_\Theta(x)_1 \newline
h_\Theta(x)_2 \newline
h_\Theta(x)_3 \newline
h_\Theta(x)_4 \newline
\end{bmatrix} \rightarrow
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;最后的结果：&lt;/p&gt;

&lt;div&gt;
$$h_\Theta(x) = 
\begin{bmatrix}
0 \newline
0 \newline
1 \newline
0 \newline
\end{bmatrix}$$
&lt;/div&gt;

&lt;p&gt;表示该样本属于第三类，$h_{\Theta}(x)_3$。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>经过省察的人生The examined life</title>
      <link>http://nanshu.wang/%E8%A7%82%E5%8F%B9/the_examined_life/</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%A7%82%E5%8F%B9/the_examined_life/</guid>
      <description>

&lt;p&gt;译自&lt;a href=&#34;http://books.google.lt/books?id=WwpZVuylPgYC&amp;amp;pg=PA228&amp;amp;hl=zh-CN&amp;amp;source=gbs_toc_r&amp;amp;cad=3#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;《A companion to Socrates》第14章&lt;/a&gt;，作者Richard Kraut&lt;/p&gt;

&lt;h1 id=&#34;经过省察的人生:f856045648ed1256c7d09f14d1393f16&#34;&gt;经过省察的人生&lt;/h1&gt;

&lt;p&gt;“未经省察过的人生是不值得过的人生”（《申辩》38a5-6）。这句苏格拉底耳熟能详的名言，也许是有史以来一个哲学家嘴中最肆无忌惮的话。苏格拉底的哲学思想和他的生活方式的本质也蕴含在这句话中。苏格拉底把自己的人生作为——或者更确切地说，柏拉图把他的人生作为——真正经过了省察的人生。要了解我们如何才能达到这种人生的要求，必须从柏拉图的作品出发，来好好学习苏格拉底给我们树立起来的好榜样。这种人生的要求或许能激励我们或许不能使我们信服，因为准确地说，这是苏格拉底对他同时代人产生的影响。苏格拉底的魅力和崇高的理想可能会被削弱。因为他说过，未经省察过的人生正是许多人的生活方式，是只有当我们有强烈倾向时才做出的选择。同时，如果遵循苏格拉底的坚持，所有人都按他的方式生活，这种人生的要求则会显得荒诞不经，甚至可以说是异乎寻常的苛刻。在任何社会中，最多只有几个人可以像苏格拉底那样，把所有的日子都用来讨论道德伦理。所有其他的人都过着毫无价值的生活吗？苏格拉底有什么好的理由来批评绝大多数人类的行为生活吗？如果只有少数人像苏格拉底那样奉献自己，他对同时代人的生活几乎没有影响，那么在更一般的情况下，经过省察的人生又有什么价值？对于这个问题，确切得说，应该是对于一个能选择这样生活的人来说，经过省察的人生到底有什么好的？即使我们都可以掌控这种生活，我们又为什么要做出尝试呢？&lt;/p&gt;

&lt;p&gt;如果还有道德哲学家的话，在当今的学术界恐怕也没谁赞同苏格拉底的名言。他们会说，我们被要求不用过分的方式（谋杀，袭击，盗窃）去伤害他人；我们要在一定程度上（这个程度多少也是一个争议的话题）至少造福一些其他人；我们要诚实，公正，善良，宽容；我们应该做的事，在道德意义上必须是正确的，因为道德本身就是正确的，而不是为达到某种非道德目的的方式。他们会说，那种苏格拉底与他的跟随者们从事的伦理讨论是值得追求的（毕竟这和这些哲学家的生活有点像）；但他们会补充说，没有必要甚至也不希望每个人都从事那种让苏格拉底着迷的抽象伦理探究。当然，他们还会说，这不是判断一个好人的人生是否经受住了审察的必要特点。在他们认为，成为一个好人明显到任何拥有常识的人都能看出来：在幼时拥有良好的教导，能分辨道德意义上的对错，并且坚持做对的事。人们需要变得善于使用这样的词语如“应该”，“正确”，“善”，“正义”，“诚实”。但要做到这一点，我们需要的不是像苏格拉底认为的那样，进入抽象和困难的道德哲学领域的，询问自己和他人诸如这样的问题：“什么是勇气？”，“ 什么是正义？”，“什么成为朋友？“相反，我们需要的是获得社会和情感技能，使我们认识到什么在道德意义上是对的，并且带着良心去做道德意义上对的事。苏格拉底根据这种思维方式，简单误以为成为一个好人，全部或部分地包括了成为一个好的道德哲学家。&lt;/p&gt;

&lt;p&gt;然而，并非所有人都同意，苏格拉底对经过省察的人生的呼吁可以这样容易就被否定掉。在现代很大一部分时期中，他对经过省察的人生的呼吁为他赢得了一个接近基督和其他宗教领袖的地位，被视为人类历史上一个伟大的道德模范。本杰明·乔伊特（ Benjamin Jowett），十九世纪后期一个柏拉图作品的重要译者，告诉他的学生：“在牛津大学，基督和苏格拉底的人物传记是我们最浓厚的兴趣所在（虽然程度不一）。”1这样的比较在二十世纪也有，苏格拉底被一位存在主义作家卡尔·雅斯贝尔斯（ Karl Jaspers，1962）视为一个“个人模范”（继佛陀，孔子，基督之后）。但在二十一世纪的最初几年，道德哲学的学术界并没有苦苦抓住苏格拉底不放，而更像是避开了他（因为还有亚里士多德，休谟，康德，尼采， 西奇威克）。&lt;/p&gt;

&lt;p&gt;苏格拉关于未经省察过的人生不值得过的人生的理论，我想既不能被接受也不能被拒绝。刚刚也描绘了若被拒绝的情况。&lt;strong&gt;在下文中，我将解释这样的人生有什么意义以及为什么苏格拉底认为我们必须这样做。&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;柏拉图的-申辩:f856045648ed1256c7d09f14d1393f16&#34;&gt;柏拉图的《申辩》&lt;/h1&gt;

&lt;p&gt;展开这种探讨时苏格拉底本人在场——或者更确切地说，在柏拉图的《申辩》中，苏格拉底被描述成为我们在演讲，说出了这句“未经省察过的人生是不值得过的人生。”苏格拉底这句话时，陪审团已经判他有罪，而且将决​​定实施什么样的刑法。Meletus是其中一个控告者，提出将他判处死刑，在这时提出另一种惩罚是苏格拉底的权利。他问：他应当被请求流放到其他的城市，并且放弃他实践哲学的方式吗？他的回答是他不愿放弃哲学，因为这违背了他的神的意愿。然后他补充说：“如果我说，一个人最大的好处是每天都在讨论德性和其他你们听我同别人交谈和省察的东西，未经省察过的人生不值得过的人生，你会更容易被我的话说服的。“（38a1-6）。苏格拉底提出了把罚款作为处罚，但陪审团决定他应该被处死。&lt;/p&gt;

&lt;p&gt;但是，为什么要每个人都要过一种省察过的人生呢？&lt;strong&gt;苏格拉底没有试图在这里回答这个问题，在任何辩护的其他地方也没有。&lt;/strong&gt; 他把经过省察的人生与进一步讨论德性就是好的联系起来，但是，这在柏拉图的《申辩》中同样没有做出解释。对于苏格拉底对这些说法没有给出论据，我们不应该感到惊讶。毕竟，他没有在做一个关于哲学的演讲，而是在针对指控为自己辩护，表明他的生活方式存在好的动机且没有不良影响。他的听众并不关心否可以证明是最好的生活是经过省察的生活；而想知道怎样对待持这种想法的人。&lt;/p&gt;

&lt;p&gt;假设我们假定智慧是一个人可以拥有的最好的东西。并且假定我们接受苏格拉底比其他所有的人都更能通过伦理讨论来接近智慧。这些前提将会确保苏格拉底式的诘问法也有很大的价值的结论，但将其作为结论则会受到挑战。我们为什么要认为智慧是最好的东西呢？并且为什么要假设这种德性是凭借或部分凭借苏格拉底进行的那种抽象的道德探究组成的呢？似乎这样说也合情合理，尽管有许多不同形式的智慧，但通过诡辩习得复杂的道德哲学也许是其中之一，可以假定它比所有其他形式的智慧更加有价值。苏格拉底式的诘问法可以提高个人的哲学技能——但能说它使人变得更好了吗？它提高了人做好事的能力吗？柏拉图的《申辩》不 能回答这些问题，并没有打算回答。&lt;/p&gt;

&lt;h1 id=&#34;不值得过的人生:f856045648ed1256c7d09f14d1393f16&#34;&gt;不值得过的人生&lt;/h1&gt;

&lt;p&gt;因此，我们将不得不转向柏拉图的其他作品，来找到对苏格拉底所这句“未经省察过的人生是不值得过的人生的更好诠释”。但在我们抛开《申辩》之前，我们应该先停下来想一想，以确保我们真正懂得苏格拉底的这句话是什么意思。&lt;/p&gt;

&lt;p&gt;其中的这个词——anexetastos（“unexamined未经省察的”）——的构成没有什么问题。Exetazein的意思是 “ 检查，探究，审察，测试，证明”。它和它的同源词 ，exetasis，经常被苏格拉底用来描述当在他与他人的谈话时在做的事。用同源形容词anexetastos形容的人生， 是一个没有受到苏格拉底式伦理道德省察的人生。&lt;/p&gt;

&lt;p&gt;但其他苏格拉底这句话中的另一个词——biotos ——需要仔细理解。译成短语“值得过的，”从乔伊特（Jowett）起一直是一种标准翻译。当我们说某人的人生在某时是不值得过的，我们的意思应该是他没有比死更好的活法。在理想的情况下，人生不值得过的时候正好是死亡来临的时候。同样，如果一个人的人生从来没有在任何时候值得一过，那么任何时候在他的人生里，死都是好的。&lt;/p&gt;

&lt;p&gt;如果我们理解苏格拉底的意思是应把所有的未经省察的人生都归入这个类别，那么我们则默认他对城邦的公民采取了其恶劣的态度。如果苏格拉底把他们从危难之中拯救出来（他在战场上杀敌），他则会认为这对他们没有什么好处。人们便会存有疑惑：为什么他认为他要为救他们动一根手指头？&lt;/p&gt;

&lt;p&gt;是不值得过的人生不单单是坏人生：这种人生太缺少价值以至于对一个人来说去死反而是最好的选择。我们没有理由将未经省察的人生里深深的痛苦归于苏格拉底的观点。因为 biotos 并不一定意味着“值得过的”，也可译为“应该过的”。这样，苏格拉底的这句话应该理解为：“未经省察的人生不是应该过的人生。”这并不意味着人生应该被终止，相反，它可以被理解为人们不应该过那样的人生。如果一个人的人生是那样，那他必须做出改变——不是死亡会更好，而是可能有一个更好的人生。&lt;/p&gt;

&lt;p&gt;这是苏格拉底的使命，去说服城邦的其他公民未经省察过的人生错过了最好的东西，因此他们必须改变他们的人生。苏格拉底的使命里没有哪部分是在说服他们人生不值得过最好去死。（试图证明这一点的意义又是什么呢？）因此，我建议我们放弃这个标准翻译，采用柏拉图希腊文版《申辩》38a5-6 的意思：“人们不应该过未经省察的人生。”我们不用害怕，以这种方式解释苏格拉底，才能把他的哲学思想变成一条不温不火的建议。每个人应该进行苏格拉底式探寻的观点是哲学家做出的最大胆的宣称之一。一旦一个人对这种观点笃信不疑，讨论对于过着这样人生的人是否去死更好，既没有哲学意义也没有现实意义。&lt;/p&gt;

&lt;p&gt;另外，关于苏格拉底说这话的含义，还有一点要强调：他认为探究伦理道德是一个过程，一个人应该因此花掉一生的时间，而不仅仅只是暂时的。一个人要达到他的要求，则要花了大半年去询问他问过的问题，然后转向其他事情，并不再关切这类问题。对于经过省察的人生的呼吁，和一下观点也有联系：一个人最大的好处是“每天都讨论德行”等伦理问题（《申辩》38a3）。 我们应该认识到这是多么大胆的话。我们只有期待苏格拉底可以给我们足够的理由来接受它。&lt;/p&gt;

&lt;h1 id=&#34;苏格拉底对话:f856045648ed1256c7d09f14d1393f16&#34;&gt;苏格拉底对话&lt;/h1&gt;

&lt;p&gt;我们从哪里能找到他的论据呢？几乎所有的柏拉图对话集中都有一个对话者——通常他占有主导地位——他就是苏格拉底。但也有学者认为，这些对话的某些内容里，这些谈话给了我们一个名叫“苏格拉底”的历史人物大概的形象；而在另一些国家，“苏格拉底”却成为了柏拉图哲学的代言人，他的哲学虽然在历史上受苏格拉底极大影响，但也有显著不同.2 这正是在这篇文章中应遵循的方法。&amp;rdquo;人们不应该过未经省察的人生这种观点是很可能是苏格拉底哲学的核心，但它却不是柏拉图本人赞同的——或者可以说他在哲学生涯中只是之中都没有赞同过。为了说明这一点，我们只需要举出《理想国》中所描述的个等级的划分：哲学家，士兵和工人。只有哲学家可以说过上经过省察的人生：他们是那些专门被培训成提出和回答各种各样的苏格拉底式问题的人。与此相反，士兵和工人，过了不假思索的人生，只是接受法律和统治者的决定。统治者不同于任何其他人，他的工作是对实际问题有智慧。因此，《理想国》中的苏格拉底拒绝了这种观点，即每个人都可以过上经过考察的人生。&lt;/p&gt;

&lt;p&gt;关于《申辩》和《理想国》中的冲突应当作何解释呢？&lt;strong&gt;一个令人难以置信的假设是，这里的差异实际上反映了柏拉图自己思想的转变。&lt;/strong&gt; 其实我个人比较相信这个假设。根据这种解释，柏拉图让苏格拉底在《申辩》中说一个人应该过经过省察的人生这样的话，实际上他是将自己的哲学思想放进了苏格拉底的演讲中。但是，这样来看待苏格拉底和柏拉图的关系不是一种可信的方式。我们没有理由怀疑，历史上的苏格拉底的确与许多和他同时代的人一起讨论德性，而他这样做引起了很大的敌意，但他却还是坚持认为这种探寻有很大的价值。无论是历史上的苏格拉底是否逐字逐句地依照我们今天读到的希腊文本谈论了关于未经省察的人生，这都不重要。不能轻易否认，经过省察的巨大人生价值是对历史上的苏格拉底的生平和思想的中心的假定。正如我们看到的那样，这种主导观念被《理想国》的主要对话者修正了。我认为，这种差异最合理的解释是柏拉图表达了他老师的中心思想，于是得出的结论是苏格拉底没有意识到其中的局限性。柏拉图说，是啊，经过省察的人生的确是人类能过上的最好的人生；但大家都去尝试便不是什么好事了。让小部分人过上那样的人生，去引导其他的人。&lt;/p&gt;

&lt;p&gt;我们认识到，柏拉图受到了苏格拉底的启发，但却超越他。在研究柏拉图对话时，要把他俩区别开来：第一，围绕主题的对话内容使用的可能是历史上苏格拉底的真实想法；第二，那些有更占主导地位的更充分的思想，有可能是柏拉图做出修改甚至不同于苏格拉底的。毫无疑问柏拉图以某种方式在所有作品里注入了他自己的思想；不可能一个哲学家的才华和独创性不被放入对别人语录的被动记载中。即便如此，基于其他的互相不同类型和不同之处，把其中的组成进行分类，也是有帮助的。&lt;strong&gt;其中那些被称为“苏格拉底式”的内容比较少，几乎完全是以道德、探索为中心，结构上也比较简单。&lt;/strong&gt; 这些内容主要关于对坏想法的抨击或证明对话者的局限；其中也包含积极的想法，但不多，也从没有与形而上学和认识论的整合到一起。这样的描述发生在苏格拉底与Laches, Charmides, Euthyphro, Crito, Protagoras, Hippias Minor, Hippias Major, Lysis 以及 Gorgias的对话中。在这些对话中，柏拉图的思想或多或少都在他老师搭建的范围内展开。正如柏拉图在《申辩》中提到的那样，苏格拉底不关心任何除了提高人生以外的话题。至少在成年后，&lt;strong&gt;苏格拉底是一个道德哲学家，不是一个形而上学者、不是一个认识论者，更不是科学家。&lt;/strong&gt; 亚里士多德也证实了这一假定：他说，苏格拉底只关注道德，不研究自然世界。（《形而上学》 I.6 987b1-2）&lt;/p&gt;

&lt;p&gt;许多其他对话都有一个相当不同的角色。这些对话篇幅更长，他们提出或审查的观点会经过更充分地阐述，涉及到的道德内容是与形而上学和认识论交织在一起的。 斐多篇“，”克拉底鲁篇“，理想国，菲德洛斯，巴门尼德，泰阿泰德篇，智者，政治家，和蒂迈欧篇” 都是如此 。对柏拉图的作品划分类别，不是说每篇作品都只属于一个类别。诸如美诺、尤西弗伦这样的作品既包括了苏格拉底式的对话特点，也有第二类作品的特点。&lt;/p&gt;

&lt;p&gt;大多数的苏格拉底式对话是柏拉图早期作品，那时苏格拉底的影响力是最强的；但也有可能他在同一时期也在创作冗长的涉及形而上学、认识论和道德的作品。他意识到对于他复杂的作品来说，苏格拉底式对话是为读者准备的一个绝好的铺垫。这可以解释为什么像Lysis 和 Charmides中，含有更长和更复杂的对话里的暗示。&lt;/p&gt;

&lt;p&gt;现在回到我们的主题上来。&lt;strong&gt;这句“人们不应该过未经省察的人生”属于历史上的苏格拉底，但不属于柏拉图笔下的苏格拉底。&lt;/strong&gt; 我们想知道苏格拉底到底对这个观点给出了什么论据，当然《申辩》中论据没有给出。那从哪里看起呢？最好就是从我们现在看的苏格拉底式对话出发，因为在这些内容中，柏拉图的哲思更贴近苏格拉底的特征，即与其他的哲学话题相比，伦理问题占有主导地位。&lt;/p&gt;

&lt;p&gt;我们将看到，这样来做是值得的：通过这些对话，能更加深入理解苏格拉底对于最重要的道德伦理的讨论所给出的理由。这些苏格拉底式的作品能够读出来，若是没有用足够的耐心和智慧来省察人生，究竟会出什么错。&lt;/p&gt;

&lt;h1 id=&#34;关于未经省察的人生的调查:f856045648ed1256c7d09f14d1393f16&#34;&gt;关于未经省察的人生的调查&lt;/h1&gt;

&lt;p&gt;尤西弗罗认为 自己是对于宗教事务的专家。在以他的名字命名的对话中，他对自己的父亲提起诉讼，因为他需要为一个家里死去的奴隶负责。在那个时代，起诉自己的父亲被认为是一个非常极端的行为。但尤西弗罗 仍然认为宗教义务让他必须这样做。也许在这一点上他是对的——但他在做出这个决定是他处于什么位置呢？他是否有任何根据来回答这样一个问题：那种行为是有关宗教职责的？&lt;/p&gt;

&lt;p&gt;在苏格拉底和尤西弗伦对话的过程中，他发现尤西弗伦没有认真考虑这件事，也没有思考虔诚的本质这个问题。尤西弗罗 是过着未经省察的人生的一个明显的例子（即使他认为自己是对于虔诚这个问题的专家），一篇以他名字命名的对话揭示了忽视哲学问题会产生多么严重的后果。除非他花大力气对的宗教职责做出准确的推测，否则他的宗教人生会变得很糟糕。似乎可以这样说，在这个例子中，他指控自己的父亲谋杀他人似乎是不虔诚的，他犯下了一个可怕的错误，便是没有意识到自己在做什么。&lt;/p&gt;

&lt;p&gt;从这个对话中得出的一个简单的教训是缺乏对道德哲学问题的重视将对人生价值产生灾难性的后果。尤西弗罗 将会犯下一个最严重的错误，不是因为他是自私，贪图权力，贪婪，而是因为他是蒙昧，愚笨，肤浅。&lt;strong&gt;因为他没有求知欲，没有兴趣探寻哲学的伦理问题，他缺乏同时也永远不会理解他所称的道德的系统和一般性含义，甚至是其中的皮毛也不会理解。&lt;/strong&gt; 毫无疑问，他是从他的父母学习到如何使用这个词 hosiotEs （“虔诚”），也学到了其他的规范灌输的词。但一个人的童年接受的教育至今只有一次，自己不能想好，去决定应该学习哪些规范词语。&lt;/p&gt;

&lt;p&gt;苏格拉底式的对话里有另一个人的显示出道德模糊的例子：克里托，在以其名字命名的对话里，他建议苏格拉底通过贿赂他的狱吏来越狱。克里托帮助苏格拉底逃脱的其中一个原因是他害怕周围的人耻笑他：在这种情况下很多人认为苏格拉底的朋友应该使用他们能力帮助他逃脱，如果他们没有这样做，则看起来像懦夫的行为（45E-46A）。值得注意的是，虽然克里托跟随在苏格拉底身边多年，但苏格拉底却一直没能使他从别人看法的奴役中解放出来。苏格拉底向克里托提过多次，他始终坚持一点：在做任何决定前必须首先多次考虑的是自己做的是否正义，而不是别人会怎样看。这个对话给我们的一个教导是，政治决策——如是否该接受处罚，即使处罚是不公正的——必须基于一个公民应该如何对待他的城市的一般理论。直到一个人抛开民意的影响，为公民责任的理论努力，一个人才不会误入政治事务的歧途。幸运的是， 克里托碰巧受到了苏格拉底的影响；若只凭借他自己，他将会为了男子气概的外表，成为欲望的奴隶。相比之下，尤西弗罗虽不在乎他的公众形象，但除了对于宗教事务有莫名的无知并且不加思索的自信外，也没有可以指引苏格拉底的地方。&lt;strong&gt;他们各自拥有各自的未经省察的人生。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果参考民众的意见确实能提出关于对与错，好和坏，正义和非正义等深思的问题，那对于认真思考关于道德问题的普遍意见是没有异议的。（克里托 47B）但是，假设发现有人进行这样的研究——并有人声称他们已经苦苦思考很久关于艰难的道德问题，并已成为这方面的专家。这正是柏拉图的《普罗塔哥拉》开篇描述的情况：与这部作品同名的著名智者来到雅典，希波克拉底要苏格拉底陪他去拜访这个著名智者和并成为他的学生。苏格拉底警告他：不要轻易委托出像灵魂这样珍贵的东西，特别是一个人不是他所宣称的专家（313A-C）。希波克拉底感到迫切需要道德教育，这种教育要超越孩童时期父母对自己的教育，他认为这样做是正确的。他渴望道德知识，但为了获得它，他必须了解自称有知识的人是否真的能教导自己。苏格拉底与希波克拉底的对话意味着，一个人永远不能放弃自己最重要的智慧，不能把自己的教育完全放在别人的手中。每个人都应该检验那些自称有道德知识的人，但成功且可靠地做到这一点，必须先教育自己，先决定要接受那种道德观点。这也有可能，有些人真的是道德专家，他们与他人讨论道德问题，能为他人作出最好的解答。这种可能性没有排除我们所有人都要过上经过省察的人生的需求除；除非我们这样做，否则我们给了别人极大伤害我们的机会。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;即使是那些钦佩并与苏格拉底在结交的人也不能避免误入歧途。&lt;/strong&gt; Charmides 和 Critias在苏格拉底被审判时都在场，明显没有从与苏格拉底的对话中受益 (他们是404 到403BC期间统治雅典的三十僭主之一)。 阿尔西比亚得也没有受益，他在后来的伯罗奔尼撒战争中背叛了雅典。（在以阿尔西比亚得名字命名的对话中，虽然这篇对话的作者不是柏拉图，但也值得仔细研究，柏拉图《会饮》中他的形象也同样如此 212C-223D）。阿尔西比亚得正如在柏拉图《会饮》中被描绘的那样，当着苏格拉底的面承认他对他的方式生活感到羞处（216B-ĉ）。但即使是这样，他也没能使自己严肃对待哲学。Charmides, Critias和 Alcibiades为未经省察的人生的困难提供了进一步例子，当我们不进行道德探寻时，我们也能发现这些困难。如果 像克里托和尤西弗罗 这样的人没有像Critias, Charmides, 和 Alcibiades一样成为违法犯罪者，那可真是走运。&lt;/p&gt;

&lt;p&gt;柏拉图《Ion》中，与作品同名对话者提供了一个明显的例子：他屈服于理性思考的能力，容纳别人的思想和情感。Ion是一个rhapsode——专业的荷马史诗朗诵和解说者。但他承认，他并不懂得荷马史诗的主题；甚至，他说，荷马史诗的思想浸入了他的灵魂，并通过他再浸入听众的灵魂。苏格拉底认为，灵感的传递开始与激发了诗人创作的的缪斯女神：创作诗句的作家为了接受来自神的影响抛开自己的智慧，反过来rhapsode的灵感来自于他读的诗，和听众听到了来自rhapsode的吟诵，也放弃了他们本身的思想，被听到的溢美之词所掌控（533d-35A）。在一定程度上，诗歌爱好者生活在诗歌的影响下，没有仔细省察其中的含义，他们是把自己的生活交由别人来掌握（如希波克拉底）。如果他们走运，激发他们的诗句的观点是真实的，那么他们就把自己交给了一个好的指引者。但这并不是过自己人生的方式。&lt;/p&gt;

&lt;p&gt;柏拉图的《高尔吉亚篇》探讨另一种模式：任由一个人的灵魂不加批判地受到别人的影响。雅典政治生活的成功需要一个人做任何吸引民众的事（502E，513B-C）：一个必须获得公众赞同的人，一个能用演讲取悦大众的人。一个人必须像糕点师一样，根据味蕾单独选择配料，而不考虑食用者的健康。Callicles是最后一个也是最大胆的的一个苏格拉底的对话者，他批评前两次对话者高尔吉亚和波卢什回答苏格拉底的方式，总是表现出他们不愿意说的感到羞愧的一面：他们自己不相信自己所说，并且按照设计好的方式在回答（482C-E）。&lt;/p&gt;

&lt;p&gt;人们在公共场合表现出表面的忠诚，实际上是在日常社会生活交往中表现的虚假的自我——在理想国第二章也有提到，&lt;strong&gt;当格劳孔提到，绝大多数人在隐身的状态下（戴着盖斯的戒指），都会去尝试做坏事，即使他们自己在公众面前会谴责这种不正当行为。&lt;/strong&gt; 在与他人的日常交往中，他们躲在一个虚假的自我之后，为了达到目的他们戴着面具。他们不但不知道自己应该拥有什么，什么值得拥有，还给他人留下自己缺乏且渴望之物的虚假印象。他们没有真正的自我——一个自己设计的自我，经过剖析的自我——所以他们的社会面具后隐藏的除了对别人的被动模仿外，什么都不是。柏拉图介于苏格拉底式人生与未经省察的人生之间的思考，是对普通人的毫不偏颇的刻画的来源。&lt;/p&gt;

&lt;p&gt;然而，在苏格拉底式对话中有一段，属于对普通人决定如何行动的标准，完全符合普通人的欲望。在柏拉图的《普罗塔哥拉》中，苏格拉底认为大多数人只凭借未来的快乐和痛苦来做决定（352B-56C）。他们认为快乐和痛苦是人类生活中的两个最强大的力量，他们把快乐作为唯一好的东西，痛苦是唯一不好的东西。因此，当他们拒绝追求快乐，或愿意接受痛苦，那只是因为他们进行了理性的计算，以便在今后获得最大的快乐，或者最少的痛苦。苏格拉底没有攻击《普罗塔哥拉》中提到的决策标准；在这篇对话中，他只是努力在证明一个这样做决定的人，懂得未来的快乐和痛苦是人生的关键因素，并且我们不能够和我们可以做到最好的行为产生对立。但令人惊讶的是，这里苏格拉底把普通人赋予了他对话者本没有的特点：一个人做采纳的决定表现出了他们想要的事物。&lt;/p&gt;

&lt;p&gt;然而，不能作出这样疯狂的假设，即仅仅因为有了真正的决策标准，人生就会变好。标准必须是正确的，因此一个人必须找到原因，为什么这些快乐和痛苦在人生中这样重要。这不是一个大多数人都想知道的问题。苏格拉底描述的他们的视野是情绪化的：&lt;strong&gt;他们认为，与欲望，恐惧，爱，快乐，和痛苦比较起来，知识是对人生的影响不那么重要。&lt;/strong&gt; 这种态度使他们不能思考真正的人生目标。即使这种知识是能够获取的，他们认为这对自己的行为也不会有什么影响。因此，他们向他们追求快乐的欲望投降，尽力避免痛苦。他们认为唯一值得他们付诸行动的是如何得到这些好东西的最佳组合。然而他们却不会想知道，是否有更好的方法去作出决定，这些好的东西是否有坏处，不会对将来的快乐和痛苦产生影响。因此，像大多与苏格拉底对话的人一样，他们冒着巨大的风险：一个人经历的属于自己的快乐和痛苦不是正确的决策知道——如果有其他一类事物也是有好有坏——那么大多数人的处境会很糟糕。&lt;/p&gt;

&lt;h1 id=&#34;大多数人都是其他人:f856045648ed1256c7d09f14d1393f16&#34;&gt;“ 大多数人都是其他人”&lt;/h1&gt;

&lt;p&gt;奥斯卡·王尔德的《深渊书简》写道：“大多数人都是其他人，他们的想法是别人的意见，他们的生活是在模仿，他们的激情都是虚假的。”，这和经过省察的人生有了类似的表达情感，也是一种接受苏格拉底式人生要求的方法。有人可能会对王尔德提出反对：那为什么我的观点不会也是别人的观点呢？如果我的想法就是别人的想法会不会非常可怖呢？我相信这个命题成立，那还存在什么异议——仅仅是因为别人认同我，我就应该停止这样做？&lt;/p&gt;

&lt;p&gt;王尔德抓住了要点，但如果我们拿他说，永远不会形成和抛弃智慧的标准（即衡量看法的能力）是致命的缺点，不假思索地接受其他人的话语、行为和感受。想要变得像另外一个人是很严重的错误，如果这仅仅是一个模仿他人的不假思索的行为，而不思考这是否值得模仿。如果我爱一幅画，它应该是因为我感受到了我自己的眼睛看到到的感情，并且从内心出发认为这值得欣赏。别人喜欢一幅画，能够使我趋向认为这幅画有某些地方值得欣赏的；但是如果我并没发现这幅画好在哪里，并且没有如实得表达出来，那么我并没有得到他们的真实看法（当然假定别人的看法是真实的没有误导性的）。同样的观点不仅适用于画作，同时也适用于对好坏的评价。如果我们只是模仿他人的思想和行为，但却不加以评判思想的质量，那么我们则是不是用自己的心灵在面对世界。&lt;strong&gt;我们成为了其他人，失去了自我。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;阿尔西比亚得这样描述苏格拉底：再以没有人能像他一样。（会饮 221C）。但是，尽管苏格拉底未经省察的人生不存在未经思索的模仿，但这没有要求我们为了不寻常而变得不寻常。苏格拉底也没有这种动机，认为真实的自我只有通过评价决定这个单独标准来评判——虽然正如我们看见的那样，他没有批评那些有羞耻心的人或是害怕别人异样的眼神的人。&lt;strong&gt;经过省察的人生值得度过，因为对于正确行为的标准我们知之甚少；所以出于我们的无知，我们的人生没有那么好。必须通过自己的努力、凭借自己的看法去发现正确的标准。但一旦我们发现了那些标准，我们都会变得相似。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;告诫我们不要过未经省察的人生的意思是&lt;strong&gt;通过自身努力才能发现真理&lt;/strong&gt;。相反，它最好被看成是基于人类发展和人类本性：人类天生就被渴望快乐憎恨痛苦。他们追求权力和地位，因为财富，权力，地位能带来快乐。这是成为雅典重要的价值观并不是偶然。（申辩 29d-e）；这是全人类社会的共同的重要价值。虽然每一个孩子都会接受的道德教育，学会如何与他人沟通，学会如“好”，“正义”和“可耻”这些共同的道德规范。教育会留下在道德认识上留下巨大空白，这个空白能够被进一步的求索来填补；因为童年形成的对正义和善良的概念依然是初步的。他们接受的有限教育和强大的心理力量做着产生了冲突，心理想要快乐，权力和地位，并且引导这行为。那些感到理所当然需要进一步的教育的人常常不知道如何满足这种需要：他们通过美丽的诗句和名誉权利，把自己的想法表达给喜欢用华丽肤浅方式的人。或者，他们干脆放弃屈服于心理因素，不再思考，并把其对渴望快乐和憎恨厌恶痛苦的强烈欲望作为正确行为的唯一标准。&lt;/p&gt;

&lt;p&gt;经过省察的人生的巨大价值在于，有一种可信的方式来找出我们头脑中的错误和教育里的缺陷，来填补关于好、正义、善良的概念空白。苏格拉底把它作为一个永远不会结束的过程。无论一个人在的道德认识上取得了多大的进步，学习依然是无止尽的，永远需要对于德性和其他道德规范的讨论。科学的思考方式已经普及。正如我们现在认识到的，科学永远从一个问题跳到另一个问题，每个解决方案的都会产生新的研究领域。我们可以说，苏格拉底在寻求一个道德的科学。这不是通过一次性回答所有的问题来终结所有的道德规范的讨论。无论我们学习了多少，学习都是无休止的，甚至可以温故而知新。那些在这个不断后退的目标方面取得进展的人，扩展了知识深度避免了在生活中范严重的错误。&lt;/p&gt;

&lt;h1 id=&#34;德性-知识-和良好的意愿:f856045648ed1256c7d09f14d1393f16&#34;&gt;德性，知识，和良好的意愿&lt;/h1&gt;

&lt;p&gt;这就是为什么我们应该过经过省察的人生。但这不是苏格拉底唯一坚持的观点。他说，我们也应该努力成为优秀的人，成为公正，勇敢 sophron（“克制”，“自制”，“稳健”，“节制”）的人，拥有所有的德性。这些劝诫互相有关系吗，或他们相互独立？——甚至可能互相之间有干扰？在《普罗塔哥拉》中，苏格拉底认为，它们是一个统一体；它们都有各自单独的一面，在正确的理解下是不会产生冲突的。&lt;strong&gt;德性的统一性所揭示的事实是，在寻求知识和智慧的不同道路上，它们之间相关联系。&lt;/strong&gt; 为了获得某种德性，一个人在人生的某一面必须做到足够好：例如，要变得勇敢，就要面对恐惧，并以正确的方式来处理恐惧。但要做到这一点，必须问自己人生和境况的哪些方面会有恐惧。是死亡吗？我们真的害怕它的到来吗？这不是我们的童年本身的道德修养能够让我们有能力来回答的一个问题。可以这样回答，省察自己的人生，找到真正有害的事情，这才是值得害怕的。这种方式反过来是不能被成功应用的，除非你了解对人来说，什么是好的；知道什么是好的，和知道什么是坏的是两个独立的问题。所以真正的勇气（不是愿意承担风险的愚蠢行为）不能从苏格拉底那种从事哲学探究的人身上拿走。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;同样的道理也适用于德性的品质：它是一个人拥有的好品格，并且它的好只能通过伦理探寻来发现。&lt;/strong&gt;在生命的初期，我们对真正的人生目标没有理解的时候，德性不要求被拥有；但很久之后，当我们知道了我们的人生目标是什么以及怎样去实现的时候，就不一样了。这就是苏格拉底在《普罗塔哥拉》中提出的，德性是智慧和知识的形式。&lt;strong&gt;这并不能得出这样的结论，即拥有德性的人没有感情。一个勇敢的人会害怕应该害怕的东西，有节制的人能通过恰当的方式体验快乐。&lt;/strong&gt; 但这些情绪反应将随着不同场合的改变流露。每个人都有幼稚情绪，这种情绪不只在童年才有，也终会被正义的情感所消灭和取代。&lt;/p&gt;

&lt;p&gt;在“美诺篇”中 ，苏格拉底认为，在以下几个方面，德性是知识的一种。考虑一个通常被认为是好的东西，德性除外：例如健康、美丽或强壮。这些优点从长远和总体的角度来看的确是好的，但肯定存在健康、美丽或强壮也有不利的方面的情况。例如，一个强大和健康的人会高估自己的力量和体能，承担可能致命的任务。男孩英俊的面孔会导致别人采用损害他智力发展的方式对待他。这些看起来好的东西是否真的好取决于它们怎样被运用，然后如何运用这些好的东西并没有与其相辅相成。要知道如何使用这些天资才能使一个人真正受益，是需要一定的检验和实践的问题。对于只能算作一般的资质的智力水平也是一样，在不同情况下都有利有弊。根据苏格拉底的说法，好坏的差异在于一个人对智慧的运用——对于如何利用好的食物的知识。（类似的论证请参见 Euthydemus 278d-82A）。&lt;/p&gt;

&lt;p&gt;他的理由和康德的《道德形而上学的基本原理》开篇十分类似。康德在开篇中写道：“除了善意，世界上在也没有一种能被称得上经过认证的好可以被构想出来，甚至在世界之外也没有”。他举出了很多例子：心灵品质如聪慧，和勇气，不同情形下的好“能够产生害处，“如果使用这些好的意愿不好的话”对于其他的幸运——权力，财富，名誉，健康，满足——也是如此。&lt;/p&gt;

&lt;p&gt;他们将变成“傲慢与自大，如果在心中没有一个良好的意愿的话。”康德的结论是，没有什么比但良好的意愿更具备“内在的无条件的价值”。3没有什么情况下良好的意愿会导致不好和损害。&lt;/p&gt;

&lt;p&gt;但康德在开篇提出的与苏格拉底的“美诺篇”中的观点类似， 但却没有得出一个好人应当进行道德探寻的结论——每天都像苏格拉底一样同别人探讨德性是什么。康德认为，普通的道德代理人不需要靠哲学帮忙来将规范概念应用到特定情况中，他们只需要防范规范刺激下动机的恶化。一个心智健全的成年人知道道义之路在哪。需要倍加努力的不是知道如何行事，而是有一个恰当的动机去行事，&lt;strong&gt;因为一个人的意愿可以强大到驱使一个人单纯的动机，并且克服所有强烈的动机。&lt;/strong&gt;对康德来说，完善一个人的灵魂是一个 净化心灵的过程，而不是哲学训练的过程。像苏格拉底一样，他得到了灵魂深处核心的价值的东西——但这些东西并不包含复杂抽象的哲学思考。只有在灵魂之外的堕落这一点上，两个哲学家的观点是一致的。&lt;/p&gt;

&lt;p&gt;跟随康德的引导，二十世纪最有影响力的道德哲学家之一WD罗斯认为任何受过教育有道德观念的人能够列出一个完整的职责表和一个完整的包括好坏事物的表。4 需要哲学技能和观察的是理解什么是一件事成为责任、成为好的。但是这样的职业与教导普通成年人应该如何度过人生没有关系。他认为，现实事物中的困难在于，当责任有冲突时知道该怎么做或者好坏发生冲突，没有哪方能胜过谁时。在这种情况下，做什么取决于每种情况下的细节，&lt;strong&gt;抽象的道德理论可以说不能引导普通民众的道德意识。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在一个方面，功利主义——唯一的职责是利益最大化—— - 比康德哲学更接近苏格拉底哲学。因为功利主义采取了修正的态度来对待道德常识框架，即孩子们从他们的父母和社区来学习。 孩子们通常没有被教育要尽可能多做善事。他们 之前设定的目标 是更加温和，而且往往他们学习的道德由各种各样的规则组成：不打架，不撒谎，不拿别人的东西、爱护兄弟姐妹，尊重长老。功利主义的主要思想——个人应该关注全世界的幸福，个人不能作恶无论这是否导致了更大的善——是大多数功利主义只有在阅读哲学和与教师，学生，朋友谈话时才话能发现。为了成为一个功利主义的人，必须先经过一段时间的自我反省：我们必须要通过做一些伤害，来讯问一个人是否真的认为最大的好的政策是附着在更广泛接受的规则的复杂的网络的进步。一个人也必须问自己最重要的问题：什么是真的好？但在另一个方面，功利主义完全不赞同苏格拉底被省察的人生的要求。因为，正如我们所看到的，苏格拉底的禁令是基于“每天都在探讨德性”或者可以给人类带来最大利益的话题。苏格拉底认为，道德谈话的主要内容是永远不会枯竭的；道德生活的主要问题永远不能得到一次性解决，因为每一个新的理解会带来新的问题。这是 功利主义强烈反对 的想法 。他们认为功利主义者的程式永远不能加以改进。他们 认为， 一旦人们了解到什么是好的， 就没有必要继续思考了：一个人应该仅仅尽可能产生多的，并且这不是一个需要进行的哲学探究的做法。我们需要互相探讨我们的行为会产生什么影响；但是我们不需要谈论道德。古代伦理学的苏格拉底式角色&lt;/p&gt;

&lt;p&gt;古代主要道德哲学家​站在苏格拉底的立场的至少有：他们认为如果哲学观念基于系统和抽象的思考，并进一步超越了我们童年时期学习的共同道德观，就很在某种程度上，影响一个人的思想，过上糟糕的人生，给别人造成巨大损害。他们认为，最重要的是我们必须达成一个共同理解，超过小孩子所认为的好。这是 的希腊伦理学的主要概念，苏格拉底式的对话展示了没有理解什么是好快是不可能理解德性，并且奠定了其核心基础。(Laches and Charmides 在其中所扮演的角色十分重要.) 柏拉图在《理想国》中的观点是，最高的智慧是对好的形式的认识——需要多年的科学训练来获得。亚里士多德说，学习道德的学生要从努力成为一个更好的人开始，为了做到这一点，他一定要了解人生首要的好。就像一个射手瞄准目标，他将能够切中要害——以他应该的方式生活及做决定——通过哲学论证，来发现对他重要的是什么。伊壁鸠鲁认为好就是明显且是被普遍接受的快乐；但他们认为的快乐在种类和价值上有极大不同，&lt;strong&gt;唯有哲学思考能确定哪些是最适合我们去追求的。&lt;/strong&gt;斯多葛学派认为，&lt;strong&gt;只有接受目的论的宇宙结构可以帮助我们摆脱破坏我们的幸福的幼稚情绪。&lt;/strong&gt;Pyrrhonian怀疑是唯一苏格拉底式的：他们自豪自己是唯一的哲学学校，&lt;strong&gt;继续从事苏格拉底后对智慧的探寻，避开一切定论的意见。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于所有的这些古代的思想家和学者，只有系统和抽象的思考才能揭示我们的生活中恰当行为的核心。如果我们只根据现成的第一手资料来做决定，不假思索，我们则会会误入歧途，对所有的成年人都是这样。这些苏格拉底的跟随者认为，我们必须采纳这两种看法中的一种：（柏拉图说）我们必须把让少数专家来完善和系统化我们的规范用词，并按照他们的理论制定方法，把道德意识渗入普通人的心中；或（苏格拉底说）我们每个人必须竭尽所能靠自己经历这个过程。无论哪种方式，我们在日常行为决策中都会用错道德规范的概念，除非这些概念是通过恰当使用的过程——即通过哲学理论——中形成的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记4 正则化</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04_%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0_logistic%E5%9B%9E%E5%BD%92_%E6%AD%A3%E8%A7%84%E5%8C%96/</link>
      <pubDate>Tue, 17 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04_%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0_logistic%E5%9B%9E%E5%BD%92_%E6%AD%A3%E8%A7%84%E5%8C%96/</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;正则化-regularization:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化 Regularization&lt;/h1&gt;

&lt;p&gt;为了和正规方程(normal equation)里&amp;rdquo;正规&amp;rdquo;区分开来，这里Regularization都译作“正则化”，有些地方也用的是“正规化”。以下内容来自&lt;a href=&#34;http://en.wikipedia.org/w/index.php?title=Regularization_(mathematics&#34;&gt;wikipedia&lt;/a&gt;)：&lt;/p&gt;

&lt;p&gt;正则化是指通过引入额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。正则化的一种理论解释是它试图引入&lt;a href=&#34;http://en.wikipedia.org/wiki/Occam%27s_razor&#34;&gt;奥卡姆剃刀原则&lt;/a&gt;。而从贝叶斯的观点来看，正则化则是在模型参数上引入了某种先验的分布。&lt;/p&gt;

&lt;p&gt;机器学习中最常见的正则化是$L_1$和$L_2$正则化。正则化是在学习算法的损失(成本)函数$E(X,Y)$的基础上在加上一项正则化参数项：$E(X,Y)+\alpha|w|$，其中$w$是参数向量，$\alpha$是正则项的参数值，需要在实际训练中调整。正则化在许多模型中都适用，对于线性回归模型来说，采用$L_1$正则化的模型叫作lasso回归，采用$L_2$的叫作ridge回归。对于logistic回归，神经网络，支持向量机，随机条件场和一些矩阵分解方法，正则化也适用。在神经网络中，$L_2$正则化又叫作“权重衰减”(weight decay)。$L_1$正则化能产生稀疏模型，因此在特征选择中很有用，但是$L_1$范式不可微，所以需要在学习算法中修改，特别是基于梯度下降的算法。&lt;/p&gt;

&lt;h1 id=&#34;过拟合问题:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;过拟合问题&lt;/h1&gt;

&lt;p&gt;欠拟合(也叫做高偏差(high bias))是指不能很好地拟合数据，一般是因为模型函数太简单或者特征较少。过拟合问题是指过于完美拟合了训练集数据，而对新的样本失去了一般性，不能有效预测新样本，这个问题也叫做高方差(high variances)。造成过拟合的原因可能是特征量太多或者模型函数过于复杂。线性回归和logistic回归都存在欠拟合和过拟合的问题。&lt;/p&gt;

&lt;p&gt;要解决过拟合的问题，通常有两种方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;减少特征数量&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;手动筛选特征&lt;/li&gt;
&lt;li&gt;采用特征筛选算法&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正则化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保留所有的特征，但尽可能使参数$\theta_j$尽量小。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;正则化在很多特征变量对目标值只有很小影响的情况下非常有用。&lt;/p&gt;

&lt;h1 id=&#34;成本函数:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;成本函数&lt;/h1&gt;

&lt;p&gt;在原有的成本函数的基础上加上使参数$\theta_j$正则化的项：&lt;/p&gt;

&lt;div&gt;
$$\min \frac1{2m}(\sum_{i=1}^m (h_{\theta}(x^{(i)}-y^{(i)}) + \lambda \sum_{j=1}^n \theta_j^2))$$
&lt;/div&gt;

&lt;p&gt;其中$\lambda$叫做正则化参数，决定了参数正则化项的影响大小。引入正则化参数项，可以避免模型过拟合的问题。如果$\lambda$的值设置过大，可能会使模型函数出现欠拟合的问题。&lt;/p&gt;

&lt;h1 id=&#34;正则化线性回归:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化线性回归&lt;/h1&gt;

&lt;h2 id=&#34;梯度下降法:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;梯度下降法&lt;/h2&gt;

&lt;p&gt;常数项$\theta_0$不用正则化，因此更新策略为：&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; \text{Repeat}\ \lbrace \newline
&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
&amp; \rbrace
\end{align*}
&lt;/div&gt;

&lt;p&gt;上面的式子可以写成：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j(1 - \alpha \frac{\lambda}m)-\alpha (\frac1m \sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}) $$
&lt;/div&gt;

&lt;p&gt;注意到$(1 - \alpha \frac{\lambda}m) &amp;lt; 1$，直观上可以理解为将式子中第一项$\theta_j$值减小了一点，第二项还是和无正则化的更新策略的第二项一致。&lt;/p&gt;

&lt;h2 id=&#34;正规方程:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正规方程&lt;/h2&gt;

&lt;p&gt;正则化正规方程求解$\theta$为：&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline
&amp; \text{where}\ \ L = 
\begin{bmatrix}
 0 &amp; &amp; &amp; &amp; \newline
 &amp; 1 &amp; &amp; &amp; \newline
 &amp; &amp; 1 &amp; &amp; \newline
 &amp; &amp; &amp; \ddots &amp; \newline
 &amp; &amp; &amp; &amp; 1 \newline
\end{bmatrix}
\end{align*}
&lt;/div&gt;

&lt;p&gt;$L$是(n+1)*(n+1)的矩阵，除了右上角的值为0外，对角线上其他值都为1。直观上理解，不包括右上角$X_0$项，$L$是一个单位矩阵。&lt;/p&gt;

&lt;p&gt;当$m\leq n$时，$X^TX$不可逆，但加入$\lambda L$后，$X^TX$也变得可逆了。&lt;/p&gt;

&lt;h1 id=&#34;正则化logistic回归:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化logistic回归&lt;/h1&gt;

&lt;h2 id=&#34;成本函数-1:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;成本函数&lt;/h2&gt;

&lt;p&gt;加上正则化参数$\theta$项：&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$
&lt;/div&gt;

&lt;p&gt;注意$\sum_{j=1}^n \theta_j^2$中参数的下标时从1到n，没有包括常数项$\theta_0$&lt;/p&gt;

&lt;h2 id=&#34;梯度下降:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;梯度下降&lt;/h2&gt;

&lt;p&gt;和线性回归一样，$\theta_0$和其他参数要分开更新：&lt;/p&gt;

&lt;div&gt;
    $$\begin{align*}
&amp; \text{Repeat}\ \lbrace \newline
&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
&amp; \rbrace
\end{align*}$$
&lt;/div&gt;

&lt;p&gt;这和正则化线性回归的更新策略是一样的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记3 有监督学习 分类 logistic回归</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%20%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20%E5%88%86%E7%B1%BB%20logistic%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Thu, 12 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%20%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20%E5%88%86%E7%B1%BB%20logistic%E5%9B%9E%E5%BD%92/</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;分类问题:662363920bb15b8d466d06a6774c21df&#34;&gt;分类问题&lt;/h1&gt;

&lt;p&gt;分类问题和回归问题不同的是，分类问题的预测值$y$只能取离散值，而非连续值。首先来看一个二类分类问题，预测值$y$只能取0或1。0又被称作负例(negative class)，1被称作正例(positive class)。通常也用&amp;rdquo;-&amp;ldquo;,&amp;rdquo;+&amp;ldquo;符号来表示。对于一个样本集输入$x^{(i)}$，对应的目标值$y^{(i)}$也被为标注(lable)。&lt;/p&gt;

&lt;h2 id=&#34;logistic回归:662363920bb15b8d466d06a6774c21df&#34;&gt;logistic回归&lt;/h2&gt;

&lt;p&gt;也可以用线性回归的方法运用到分类问题上，但是这样做很容易得到不好的结果。稍微改变一下我们的假设函数$h_\theta(x)$，使其的取值在{0,1}范围内：&lt;/p&gt;

&lt;div&gt;
$$h_\theta(x) = g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$$
$$g(z)=\frac1{1+e^{-z}}$$
&lt;/div&gt;

&lt;p&gt;$g(z)$叫做logistic函数，也叫做sigmoid函数。$g(z)$的函数图像如下：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/logistic-function.png&#34; alt=&#34;logistic-function&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;当$z\rightarrow \infty$时，$g(z)$趋近于1；当$z\rightarrow -\infty$时，$g(z)$趋近于0。因此$h(x)$的取值在0到1范围内。&lt;/p&gt;

&lt;p&gt;求$g(z)$的导数可得：&lt;/p&gt;

&lt;div&gt;
$$g&#39;(z)=g(z)(1-g(z))$$
&lt;/div&gt;

&lt;p&gt;下面是对分类问题作出的一些假设，预测函数$h_\theta(x)$将给出样本目标值分类为1的概率：&lt;/p&gt;

&lt;div&gt;
$$P(y=1|x;\theta) = h_{\theta}(x)$$
$$P(y=0|x;\theta) = 1-h_{\theta}(x)$$
$$p(y|x;\theta) = (h_{\theta}(x)^y(1-h_{\theta}(x)^{1-y}))$$
&lt;/div&gt;

&lt;p&gt;那么$\theta$的似然函数为：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
L(\theta) =&amp; p(\overrightarrow y|X;\theta)\\
=&amp; \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=&amp; \Pi_{i=1}^m (h_\theta(x^{(i)})^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}})
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;求log似然函数:&lt;/p&gt;

&lt;div&gt;
$$l(\theta)=\log L(\theta)=\sum_{i=1}^m y^{(i)}\log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))$$
&lt;/div&gt;

&lt;p&gt;求最大似然估计，同样可以采用梯度下降的方法，更新$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta+\alpha \nabla_\theta l(\theta)$$
&lt;/div&gt;

&lt;p&gt;这里是求最大值，因此更新$\theta$是加上$l(\theta)$的偏导。&lt;/p&gt;

&lt;p&gt;解之得到：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j+\alpha (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$$
&lt;/div&gt;

&lt;p&gt;这和之前在线性回归模型得到的LMS更新策略一样，这并不是巧合，而是因为线性回归和logistic回归都属于广义线性模型(GLM models)。&lt;/p&gt;

&lt;h2 id=&#34;perceptron学习算法:662363920bb15b8d466d06a6774c21df&#34;&gt;perceptron学习算法&lt;/h2&gt;

&lt;p&gt;有趣的是，如果这里不采用logistic函数，而是采用一种简单粗暴的只考虑阈值的函数g(z)：&lt;/p&gt;

&lt;div&gt;
$$g(z) = \begin{cases}1,if z\geq 0\\0,if z &lt; 0\end{cases}$$
&lt;/div&gt;

&lt;p&gt;我们得到的更新$\theta$的策略和采用logistic函数得到的策略是一致。这种算法叫做感知器(perceptron)学习算法，感知器原指一种用来刻画大脑神经元的粗糙模型。虽然表面上看这种简单粗暴的方式和其他算法得到的结果是一样的，但是这是一种和logistic回归以及最小二乘线性回归非常不同的一类算法，它不能推导出有意义的概率解释，也不能通过极大似然估计得到。&lt;/p&gt;

&lt;h2 id=&#34;牛顿法:662363920bb15b8d466d06a6774c21df&#34;&gt;牛顿法&lt;/h2&gt;

&lt;p&gt;为了求$f(\theta)=0$时$\theta$的取值，牛顿法每次更新$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-\frac{f(\theta)}{f&#39;(\theta)}$$
&lt;/div&gt;

&lt;p&gt;要最大化似然函数$l(\theta)$的值，使其导数$l&amp;rsquo;(\theta)＝0$。更新策略为：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-\frac{l&#39;(\theta)}{l&#39;&#39;(\theta)}$$
&lt;/div&gt;

&lt;p&gt;当$\theta$为向量时，推广更一般的牛顿法，这种方法也叫做牛顿－拉普森法(Newton-Raphson method)：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-H^{-1} \nabla_\theta l(\theta)$$
&lt;/div&gt;

&lt;p&gt;$\nabla_\theta l(\theta)$是$l(\theta)$对于$\theta$的偏导。$H$是$(n+1)*(n+1)$的矩阵，叫做Hessian：&lt;/p&gt;

&lt;div&gt;
$$H_{ij}=\frac{\delta^2 l(\theta)}{\delta \theta_i \delta \theta_j}$$
&lt;/div&gt;

&lt;p&gt;牛顿法收敛的速度通常比批量梯度下降要快，但是牛顿法每次迭代的计算量更大，每次迭代重新计算Hessian矩阵，需要$O(n^2)$的时间复杂度。但在n没有很大的情况下，牛顿法是更有效率的。将牛顿法用于logistic回归的log似然函数$l(\theta)$得到的方法也被称为Fisher scoring。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92_%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Wed, 11 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92_%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A/</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:a41c550b5852f048f913986db76cdf89&#34;&gt;有监督学习&lt;/h1&gt;

&lt;h2 id=&#34;局部加权线性回归-locally-weighted-linear-regression:a41c550b5852f048f913986db76cdf89&#34;&gt;局部加权线性回归(Locally weighted linear regression)&lt;/h2&gt;

&lt;p&gt;参数学习算法(parametric learning algorithm)：参数个数固定&lt;/p&gt;

&lt;p&gt;非参数学习算法(non-parametric learning algorithm)：参数个数随样本增加&lt;/p&gt;

&lt;p&gt;特征选择对参数学习算法非常重要，否则会出现下面的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欠拟合(underfitting)：特征过少，模型过于简单，高偏差(high bias)，不能很好拟合训练集&lt;/li&gt;
&lt;li&gt;过拟合(overfitting)：特征过多，模型过于复杂，高方差(high variance)，过于拟合训练集，不能很好预测新样本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于非参数学习算法来说，并不需要进行精心的特征选择，局部加权线性回归就是这样。&lt;/p&gt;

&lt;p&gt;局部加权回归又叫做Loess，其成本函数为：&lt;/p&gt;

&lt;div&gt;
$$\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$$
&lt;/div&gt;

&lt;p&gt;$w^{(i)}$是非负权重，通常定义$w^{(i)}$如下：&lt;/p&gt;

&lt;div&gt;
$$w^{(i)}=exp(- \frac{(x^{(i)}-x)^2}{2\tau^2})$$
&lt;/div&gt;

&lt;p&gt;权值取决于用于预测的输入变量$x$的值：离$x$越近的样本，权值越接近1；越远的样本，越接近0。$\tau$被称为带宽(bandwidth)参数，决定了以$x$为中心，样本权重递减的速度。$\tau$越大，递减速度越慢；$\tau$越小，递减速度越快。&lt;/p&gt;

&lt;p&gt;参数学习算法的参数是固定的，一经学习得到不再改变。而非参数学习算法的参数并不固定，每次预测都要重新学习一组新的参数，并且要一直保留完整的训练样本集。当样本集很大时，局部加权回归的计算开销会很大，Andrew Moore提出的KD-tree方法可以在大数据集上的计算更高效。&lt;/p&gt;

&lt;h2 id=&#34;回归模型的概率解释:a41c550b5852f048f913986db76cdf89&#34;&gt;回归模型的概率解释&lt;/h2&gt;

&lt;p&gt;遇到一个回归问题，为什么采用线性回归，又为什么采用最小二乘作为成本函数？其实最小二乘回归中蕴含了非常自然的概率假设。假设目标值和输入值之间满足以下关系：&lt;/p&gt;

&lt;div&gt;
$$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$$
&lt;/div&gt;

&lt;p&gt;其中，$\epsilon^{(i)}$是误差项，包含了模型未考虑到的影响目标值的因素和随机噪声。假设误差项独立同分布，服从均值为0，方差为$\sigma^2$的高斯分布（正态分布），即$\epsilon^{(i)}\sim N(0,\sigma^2)$。$\epsilon^{(i)}$的密度函数为：&lt;/p&gt;

&lt;div&gt;
$$p(\epsilon^{(i)})=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$
&lt;/div&gt;

&lt;p&gt;也就是说：&lt;/p&gt;

&lt;div&gt;
$$p(y^{(i)}|x^{(i)};\theta)=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
&lt;/div&gt;

&lt;p&gt;或者也可以这样写：$(y^{(i)}|x^{(i)};\theta) \sim N(\theta^Tx^{(i)},\sigma^2)$。&lt;/p&gt;

&lt;p&gt;为什么采用正态分布？一种原因是数学上处理起来比较便利，二是因为根据中心极限定理，独立的随机变量的和，即多种随机误差的累积，其总的影响是接近正态分布的。&lt;/p&gt;

&lt;p&gt;$p(y^{(i)}|x^{(i)};\theta)$的含义是给定条件$x^{(i)}$，参数设定为$\theta$时，$y^{(i)}$的分布值。不能将$\theta$看作是概率条件，因为$\theta$不是随机变量，而是实际存在的真实值，虽然我们不知道真实值到底是多少。因此在以上的式子中，用了分号而不是逗号来区分$x^{(i)}$和$\theta$。&lt;/p&gt;

&lt;p&gt;定义$\theta$的似然(likelihood)函数：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
L(\theta)=&amp; p(y|X;\theta)\\
=&amp; \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=&amp; \Pi_{i=1}^m \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;似然(likelihood)和概率(probability)实际上是一个东西，但是似然函数是对参数$\theta$定义的，为了加以区分，使用了似然这一术语。我们可以说参数的似然，数据的概率，但不能说数据的似然，参数的概率。&lt;/p&gt;

&lt;p&gt;极大似然估计的含义是选择参数$\theta$，使参数的似然函数最大化，也就是说选择参数使得已有样本数据出现的概率最大。&lt;/p&gt;

&lt;p&gt;为方便求解，再定义函数$l(\theta)$：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
l(\theta) =&amp; \log L(\theta) \\
=&amp; \sum_{i=1}^m \log \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\
=&amp; m\log (\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2)
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;可得，要最大化似然函数$L(\theta)$，也就是最大化$l(\theta)$，也就是最小化：&lt;/p&gt;

&lt;div&gt;
$$\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$$
&lt;/div&gt;

&lt;p&gt;这个式子刚好是最小二乘法中定义的成本函数$J(\theta)$。总结一下，最小二乘回归模型刚好就是在假设了误差独立同服从正态分布后，得到的最大似然估计。注意到，正态分布中的方差$\sigma^2$的取值对模型并没有影响。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记1 有监督学习 线性回归 LMS算法 正规方程</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_LMS%E7%AE%97%E6%B3%95_%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_LMS%E7%AE%97%E6%B3%95_%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:b33d274e87081502d65882ed2d51cd57&#34;&gt;有监督学习&lt;/h1&gt;

&lt;p&gt;先理清几个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$x^{(i)}$表示&amp;rdquo;输入&amp;rdquo;变量(&amp;ldquo;input&amp;rdquo; variables)，也称为特征值(features)。&lt;/li&gt;
&lt;li&gt;$y^{(i)}$表示&amp;rdquo;输出&amp;rdquo;变量(&amp;ldquo;output&amp;rdquo; variables)，也称为目标值(target)。&lt;/li&gt;
&lt;li&gt;一对$(x^{(i)},y^{(i)})$称为一个训练样本(training example)，用作训练的数据集就是就是一组$m$个训练样本${(x^{(i)},y^{(i)});i=1,&amp;hellip;,m}$，被称为训练集(training set)。&lt;/li&gt;
&lt;li&gt;$X$表示输入变量的取值空间，$Y$表示输出变量的取值空间。那么$h:X \rightarrow Y$是训练得到的映射函数，对于每个取值空间X的取值，都能给出取值空间Y上的一个预测值。函数$h$的含义为假设(hypothesis)。&lt;/li&gt;
&lt;li&gt;图形化表示整个过程：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/supervised-learning.png&#34; alt=&#34;supervised-learning&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当预测值y为连续值时，则有监督学习问题是回归(regression)问题；预测值y为离散值时，则为分类(classification)问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;线性回归-linear-regression:b33d274e87081502d65882ed2d51cd57&#34;&gt;线性回归(Linear Regression)&lt;/h2&gt;

&lt;p&gt;先简单将y表示为x的线性函数：&lt;/p&gt;

&lt;div&gt;
$$h(x) = \sum_{i=0}^{n}\theta _ix_i=\theta^Tx$$
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;$\theta$ 称为参数(parameters)，也叫做权重(weights)，参数决定了$X$到$Y$的射映空间。&lt;/li&gt;
&lt;li&gt;用$x_0=1$来表示截距项(intercept term)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有了训练集，如果通过学习得到参数$\theta$？&lt;/p&gt;

&lt;p&gt;一种方法是，让预测值$h(x)$尽量接近真实值y，定义成本函数(cost function):&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = \frac12\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)^2$$
&lt;/div&gt;

&lt;p&gt;这实际上就是最小二乘成本函数，我们把这个回归模型叫做普通最小二乘回归模型(ordinary least squares regression model)。&lt;/p&gt;

&lt;h2 id=&#34;lms算法:b33d274e87081502d65882ed2d51cd57&#34;&gt;LMS算法&lt;/h2&gt;

&lt;p&gt;为了找到使成本函数$J(\theta)$最小的参数$\theta$，采用搜索算法：给定一个$\theta$的初值，然后不断改进，每次改进都使$J(\theta)$更小，直到最小化$J(\theta)$的$\theta$的值收敛。&lt;/p&gt;

&lt;p&gt;考虑梯度下降(gradient descent)算法：从初始$\theta$开始，不断更新：&lt;/p&gt;

&lt;p&gt;$$\theta_j:=\theta_j-\alpha \frac{\delta}{\delta\theta_j}J(\theta)$$&lt;/p&gt;

&lt;p&gt;注意，更新是同时对所有$j=0,&amp;hellip;,n$的$\theta_j$值进行。$\alpha$被称作学习率(learning rate)，也是梯度下降的长度，若$\alpha$取值较小，则收敛的时间较长；相反，若$\alpha$取值较大，则可能错过最优值。&lt;/p&gt;

&lt;p&gt;假设我们只有一个训练样本$(x,y)$，此时$J(\theta) = \frac12(h_{\theta}(x)-y)^2$，求偏导项得到：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
\frac{\delta}{\delta\theta_j}J(\theta) =&amp; \frac{\delta}{\delta\theta_j}\frac12(h_{\theta}(x)-y)^2\\
=&amp; (h_{\theta}(x)-y)*\frac{\delta}{\delta\theta_j}(h_{\theta}(x)-y)\\
=&amp; ((h_{\theta}(x)-y))*\frac{\delta}{\delta\theta_j}(\sum_{i=0}^{n}\theta_ix_i-y)\\
=&amp; (h_{\theta}(x)-y)*x_j
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;每次按照以下式子更新$\theta_j$的值：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))*x_j^{(i)}$$
&lt;/div&gt;

&lt;p&gt;这种更新方法叫做LMS更新策略(Least Mean Squares update rule)，也叫做Widrow-Hoff 学习策略。&lt;/p&gt;

&lt;p&gt;采用LMS方法，参数更新的次数和误差项$(y^{(i)}-h_{\theta}(x^{(i)}))$成正比。也就是说，如果预测值与真实值的误差项较小，则参数调整改变不会很大，相反，如果误差项较大，参数进行的调整更大。&lt;/p&gt;

&lt;p&gt;如果训练集不只一个训练样本，可以采用以下方法更新参数：&lt;/p&gt;

&lt;p&gt;Repeat until convergence{&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$\theta_j:=\theta_j+\alpha\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)})x_j^{(i)})$&lt;/code&gt; (for every j)&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;实际上，这里的求和刚好是$\frac{\delta J(\theta)}{\delta\theta_j}$的值。这种方法每一步更新都会遍历每所有的训练样本，因此被称作批量梯度下降(batch gradient descent)。&lt;/p&gt;

&lt;p&gt;梯度下降法通常容易受局部最优值的影响，但这里的最优问题只有一个全局最优值，没有局部最优值。因此梯度下降总是收敛到全局最优解（学习率$\alpha$不能取太大，否则错过最优值）。&lt;/p&gt;

&lt;p&gt;除了批量梯度下降，还有一种方法叫做随机梯度下降(stochastic gradient descent)，也叫做增量梯度下降(incremental gradient descent)。其更新策略为：&lt;/p&gt;

&lt;p&gt;Loop{&lt;/p&gt;

&lt;p&gt;for i=1 to m,{&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}$&lt;/code&gt; (for every j).&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;随机梯度下降和批量梯度下降不同点在于，批量梯度下降每一步更新$\theta$值，都需要遍历全部的训练样本，而随机梯度下降在遇到每个训练样本时，更新$\theta$之后继续处理下一个样本，每个样本只遍历一次，算法的学习时间比批量梯度下降快很多。但是，随机梯度下降可能永远不会收敛到全局最优值，而是在成本函数$J(\theta)$最优值周围附近摇摆。但是在实际问题中，接近最优值的参数值可能已经是足够好的结果了，特别是对于数据量非常大的训练集来说，随机梯度下降是比批量梯度下降更好的选择。&lt;/p&gt;

&lt;p&gt;在实际使用梯度下降算法时，将输入变量归一到同一取值范围，能够减少算法的迭代次数。这是因为$\theta$在小的取值范围内会下降很快，但在大的取值范围内会下降较慢。并且当输入变量取值范围不够均衡时，$\theta$更容易在最优值周围波动。采用特征缩放(feature scaling)和均值归一化(mean normalization)可以避免这些问题。选择学习率$\alpha$的值，要观察$J(\theta)$值在每次迭代后的变化。已经证明了如果$\alpha$的取值足够小，则$J(\theta)$每次迭代后的值都会减少。如果$J(\theta)$在某次迭代后反而增加了，说明学习率$\alpha$的值应该减小，因为错过了最优值。Andrew Ng推荐的一个经验是每次将$\alpha$减少3倍。&lt;/p&gt;

&lt;h2 id=&#34;正规方程-the-normal-equations:b33d274e87081502d65882ed2d51cd57&#34;&gt;正规方程(The normal equations)&lt;/h2&gt;

&lt;p&gt;梯度下降是最小化$J(\theta)$的一种方式，正规方程是另一种求解参数$\theta$的方法，这种方法可以直接求出最优值参数结果，不需要迭代更新，也不需要事先对数据进行归一化预处理。这种方法实际上是直接求出$J(\theta)$的导数，并令其为0。&lt;/p&gt;

&lt;div&gt;
$$J(\theta)=\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)}))^2=\frac12(X\theta-\overrightarrow y)^T(X\theta-\overrightarrow y)$$

$$\nabla_{\theta}J(\theta)=0$$ 
&lt;/div&gt;

&lt;p&gt;解之，&lt;/p&gt;

&lt;div&gt;
$$\nabla_{\theta}J(\theta) = X^TX\theta-X^T\overrightarrow y=0$$
&lt;/div&gt;

&lt;p&gt;得到正规方程：&lt;/p&gt;

&lt;div&gt;
$$X^TX\theta=X^T\overrightarrow y$$
&lt;/div&gt;

&lt;p&gt;求解$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta=(X^TX)^{-1}X^T\overrightarrow y$$
&lt;/div&gt;

&lt;p&gt;正规方程求解$\theta$的时间复杂度为$O(n^3)$，n是特征数量。当特征数量很大时，正规方程求解会很慢。Andrew Ng给出的一个经验参考是：当n&amp;gt;10,000时，采用梯度下降比正规方程更好。&lt;/p&gt;

&lt;p&gt;比较一下正规方程和梯度下降：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;正规方程&lt;/th&gt;
&lt;th&gt;梯度下降&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;不需要调整参数&lt;/td&gt;
&lt;td&gt;需要调整参数$\alpha$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;不需要迭代&lt;/td&gt;
&lt;td&gt;需要迭代更新$\theta$值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n较大时效率低&lt;/td&gt;
&lt;td&gt;n较大时效率也不错&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;正规方程还会遇到$X^TX$不可逆的情况，通常这是因为输入变量中存在线性相关的变量或者是因为特征太多($n\geq m$)。解决方法是去掉线性相关的冗余变量，或者删掉一些特征。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hugo静态网站生成器中文教程</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E7%94%A8Hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Sat, 31 Jan 2015 00:30:03 CST</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/%E7%94%A8Hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>

&lt;h1 id=&#34;前言:d605f9890f3528aea462ac7515ece633&#34;&gt;前言&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://gohugo.io&#34;&gt;Hugo&lt;/a&gt;是什么？官方文档是这样介绍它的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hugo is a general-purpose website framework. Technically speaking, Hugo is a static site generator.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hugo是一种通用的网站框架。严格来说，Hugo应该被称作静态网站生成器。&lt;/p&gt;

&lt;p&gt;静态网站生成器从字面上来理解，就是将你的内容生成静态网站。所谓“静态”的含义其实反映在网站页面的生成的时间。一般的web服务器（WordPress, Ghost, Drupal等等）在收到页面请求时，需要调用数据库生成页面（也就是HTML代码），再返回给用户请求。而静态网站则不需要在收到请求后生成页面，而是在整个网站建立起之前就将所有的页面全部生成完成，页面一经生成便称为静态文件，访问时直接返回现成的静态页面，不需要数据库的参与。&lt;/p&gt;

&lt;p&gt;采用静态网站的维护也相当简单，实际上你根本不需要什么维护，完全不用考虑复杂的运行时间，依赖和数据库的问题。再有也不用担心安全性的问题，没有数据库，网站注入什么的也无从下手。&lt;/p&gt;

&lt;p&gt;静态网站最大好处就是访问快速，不用每次重新生成页面。当然，一旦网站有任何更改，静态网站生成器需要重新生成所有的与更改相关的页面。然而对于小型的个人网站，项目主页等等，网站规模很小，重新生成整个网站也是非常快的。Hugo在速度方面做得非常好，Dan Hersam在他这个&lt;a href=&#34;https://www.udemy.com/build-static-sites-in-seconds-with-hugo/&#34;&gt;Hugo教程&lt;/a&gt;里提到，5000篇文章的博客，Hugo生成整个网站只花了6秒，而很多其他的静态网站生成器则需要几分钟的时间。我的博客目前文章只有几十篇，用Hugo生成整个网站只需要0.1秒。官方文档提供的数据是每篇页面的生成时间不到1ms。&lt;/p&gt;

&lt;p&gt;我认为对于个人博客来说，应该将时间花在内容上而不是各种折腾网站。Hugo会将Markdown格式的内容和设置好模版一起，生成漂亮干净的页面。挑选折腾好一个喜爱的模版，在Sublime Text里用Markdown写博客，再敲一行命令生成同步到服务器就OK了。整个体验是不是非常优雅简单还有点geek的味道呢？&lt;/p&gt;

&lt;p&gt;Hugo是用&lt;a href=&#34;http://golang.org/&#34;&gt;Go语言&lt;/a&gt;写的，为什么使用Go，作者&lt;a href=&#34;http://spf13.com&#34;&gt;Steve Francia&lt;/a&gt;的原话是：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I looked at existing static site generators like Jekyll, Middleman and nanoc. All had complicated dependencies to install and took far longer to render my blog with hundreds of posts than I felt was acceptable. I wanted a framework to be able to get rapid feedback while making changes to the templates, and the 5+-minute render times was just too slow. In general, they were also very blog minded and didn’t have the ability to have different content types and flexible URLs.&lt;/p&gt;

&lt;p&gt;I wanted to develop a fast and full-featured website framework without dependencies. The Go language seemed to have all of the features I needed in a language. I began developing Hugo in Go and fell in love with the language. I hope you will enjoy using (and contributing to) Hugo as much as I have writing it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;总结他的一下大意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;吐槽脸：Jekyll以及那一堆静态网站生成器安装麻烦（依赖多），速度又慢，内容类型单一，url死板&lt;/li&gt;
&lt;li&gt;挽袖子状：Go挺萌的符合我对语言的一切幻想，就用它重写一个吧&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我为啥用Hugo？除了以上提到的原因，很重要的一点是&lt;a href=&#34;http://gohugo.io&#34;&gt;Hugo主页&lt;/a&gt;很漂亮，看了一圈静态网站生成器的主页，一眼就被Hugo的美到了，首页的照片里的那个格子小本子应该是&lt;a href=&#34;http://www.paperthinks.com&#34;&gt;Paperthinks&lt;/a&gt;，我正好也在用，有种刚好看到自己桌面的感觉。&lt;/p&gt;

&lt;h1 id=&#34;安装:d605f9890f3528aea462ac7515ece633&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;如果说速度快是Hugo的第一大优点，那么安装简单应该就是Hugo的第二大优点。对于Mac用户，没有brew的话先安装brew，在命令行里敲：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后再敲一行安装Hugo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew new Hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然你也可以在&lt;a href=&#34;https://github.com/spf13/hugo/releases&#34;&gt;这里&lt;/a&gt;直接下载对应系统的binary文件，解压就行了。&lt;/p&gt;

&lt;h1 id=&#34;了解hugo:d605f9890f3528aea462ac7515ece633&#34;&gt;了解Hugo&lt;/h1&gt;

&lt;p&gt;首先建立自己的网站，mysite是网站的路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo new site mysite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后进入该路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd mysite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在该目录下你可以看到以下几个目录和&lt;code&gt;config.toml&lt;/code&gt;文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ▸ archetypes/ 
 ▸ content/
 ▸ layouts/
 ▸ static/
   config.toml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;config.toml&lt;/code&gt;是网站的配置文件，包括&lt;code&gt;baseurl&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;copyright&lt;/code&gt;等等网站参数。&lt;/p&gt;

&lt;p&gt;这几个文件夹的作用分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;archetypes：包括内容类型，在创建新内容时自动生成内容的配置&lt;/li&gt;
&lt;li&gt;content：包括网站内容，全部使用markdown格式&lt;/li&gt;
&lt;li&gt;layouts：包括了网站的模版，决定内容如何呈现&lt;/li&gt;
&lt;li&gt;static：包括了css, js, fonts, media等，决定网站的外观&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hugo提供了一些完整的主题可以使用，下载这些主题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone --recursive https://github.com/spf13/hugoThemes themes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时现成的主题存放在&lt;code&gt;themes/&lt;/code&gt;文件夹中。&lt;/p&gt;

&lt;p&gt;现在我们先熟悉一下Hugo，创建新页面：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo new about.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入&lt;code&gt;content/&lt;/code&gt;文件夹可以看到，此时多了一个markdown格式的文件&lt;code&gt;about.md&lt;/code&gt;，打开文件可以看到时间和文件名等信息已经自动加到文件开头，包括创建时间，页面名，是否为草稿等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
+++
date = &amp;quot;2015-02-01T18:19:54+08:00&amp;quot;
draft = true
title = &amp;quot;about&amp;quot;

+++

# 关于我
- 2010  HR@RUC
- 2014  CS@ICT, CAS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我在页面中加入了一些内容，然后运行Hugo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ Hugo server -t hyde --buildDrafts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;-t&lt;/code&gt;参数的意思是使用hyde主题渲染我们的页面，注意到&lt;code&gt;about.md&lt;/code&gt;目前是作为草稿，即&lt;code&gt;draft&lt;/code&gt;参数设置为&lt;code&gt;true&lt;/code&gt;，运行Hugo时要加上&lt;code&gt;--buildDrafts&lt;/code&gt;参数才会生成被标记为草稿的页面。
在浏览器输入localhost:1313，就可以看到我们刚刚创建的页面。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/hugo-server-1.png&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;注意观察当前目录下多了一个文件夹&lt;code&gt;public/&lt;/code&gt;，这里面是Hugo生成的整个静态网站，如果使用Github pages来作为博客的Host，你只需要将&lt;code&gt;public/&lt;/code&gt;里的文件上传就可以，这相当于是Hugo的输出。&lt;/p&gt;

&lt;h1 id=&#34;主题选择:d605f9890f3528aea462ac7515ece633&#34;&gt;主题选择&lt;/h1&gt;

&lt;p&gt;进入&lt;code&gt;themes/hyde&lt;/code&gt;文件夹，可以看到熟悉的文件夹名，和主题相关的文件主要是在&lt;code&gt;layouts/&lt;/code&gt;和&lt;code&gt;static/&lt;/code&gt;这两个文件内，选择好一个主题后，可以将&lt;code&gt;themes/&lt;/code&gt;中的文件夹直接复制到&lt;code&gt;mysite/&lt;/code&gt;目录下，覆盖原来的&lt;code&gt;layouts/&lt;/code&gt;, &lt;code&gt;static/&lt;/code&gt;文件夹，此时直接使用\$Hugo server就可以看到主题效果，修改主题也可以直接修改其中的css, js, html等文件。&lt;/p&gt;

&lt;p&gt;我的博客模版是在Hugo作者spf13的&lt;a href=&#34;http://spf13.com&#34;&gt;博客&lt;/a&gt;基础上修改的。第一步，先去他的博客网站源码&lt;a href=&#34;https://github.com/spf13/spf13.com&#34;&gt;主页&lt;/a&gt;把整个项目clone下来&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git@github.com:spf13/spf13.com.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把项目中的&lt;code&gt;static/&lt;/code&gt;和&lt;code&gt;layouts/&lt;/code&gt;文件复制到自己网站的目录下替换原来的文件夹。再次运行Hugo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ Hugo server --buildDrafts -w
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这次没有选择主题，如果选择了主题会将当前的主题覆盖掉。参数&lt;code&gt;-w&lt;/code&gt;意味监视watch，此时如果修改了网站内的信息，会直接显示在浏览器的页面上，不需要重新运行\$hugo server，方便我们进行修改。这是采用了spf13主题的页面：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/hugo-server-2.png&#34; alt=&#34;hugo-server-2&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;我们尝试在他的主题基础上修改，找到&lt;code&gt;/layouts/partials/subheader.html&lt;/code&gt;文件:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;header id=&amp;quot;header&amp;quot;&amp;gt;
    &amp;lt;figure&amp;gt;
      &amp;lt;a href=&amp;quot;/&amp;quot; border=0 id=&amp;quot;logolink&amp;quot;&amp;gt;&amp;lt;div class=&amp;quot;icon-spf13-3&amp;quot; id=&amp;quot;logo&amp;quot;&amp;gt; &amp;lt;/div&amp;gt;&amp;lt;/a&amp;gt;
    &amp;lt;/figure&amp;gt;
    &amp;lt;div id=&amp;quot;byline&amp;quot;&amp;gt;by Steve Francia&amp;lt;/div&amp;gt;
    &amp;lt;nav id=&amp;quot;nav&amp;quot;&amp;gt;
    {{ partial &amp;quot;nav.html&amp;quot; . }}
    {{ partial &amp;quot;social.html&amp;quot; . }}
    &amp;lt;/nav&amp;gt;
&amp;lt;/header&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将by Steve Francia换成by myname，再次回到浏览器，可以看到左边侧栏已经发生变化了，你可以根据自己的需要修改对应的文件，当然得懂一点css, html。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/hugo-server-change.png&#34; alt=&#34;hugo-server-change&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;评论功能:d605f9890f3528aea462ac7515ece633&#34;&gt;评论功能&lt;/h1&gt;

&lt;p&gt;个人博客当然不能没有评论，Hugo默认支持&lt;a href=&#34;https://disqus.com/&#34;&gt;Disqus&lt;/a&gt;的评论，需要在模版中添加以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ template &amp;quot;_internal/disqus.html&amp;quot; . }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spf13在&lt;code&gt;/layouts/partials/disqus.html&lt;/code&gt;中已经添加好了。&lt;/p&gt;

&lt;p&gt;只需要去Disqus注册一个账号，然后在&lt;code&gt;config.toml&lt;/code&gt;里加上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;disqusShortname = &amp;quot;yourdisqusShortname&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意&lt;code&gt;-w&lt;/code&gt;参数是不能监测&lt;code&gt;config.toml&lt;/code&gt;里参数变化的，因此需要重新运行Hugo，进入localhost:1313/about，可以看到评论功能。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/comments.png&#34; alt=&#34;comments&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;代码高亮:d605f9890f3528aea462ac7515ece633&#34;&gt;代码高亮&lt;/h1&gt;

&lt;p&gt;作为码农，代码高亮对于写博客来说当然必不可少。有两种方法：第一种是在生成页面时就生成好代码高亮过的页面；第二种是使用js，用户加载页面时浏览器再进行渲染。&lt;/p&gt;

&lt;p&gt;第一种方法需要使用&lt;a href=&#34;http://pygments.org/&#34;&gt;Pygments&lt;/a&gt;，一个python写的工具。&lt;/p&gt;

&lt;p&gt;安装Pygments：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install Pygments
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;没有pip的先下载 &lt;a href=&#34;https://bootstrap.pypa.io/get-pip.py&#34;&gt;https://bootstrap.pypa.io/get-pip.py&lt;/a&gt; ，然后安装pip：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python get-pip.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pygments的调用采用shortcodes实现，spf13里也写好了，在&lt;code&gt;/layouts/shortcode/highlight.html&lt;/code&gt;里&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ $lang := index .Params 0 }}
{{ highlight .Inner $lang }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要使代码高亮，在你的代码外面加上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ % highlight python %}}
your code here.
{{ % /highlight %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里为了避免以上两行被识别为代码高亮的标识，在&lt;code&gt;{{&lt;/code&gt;和&lt;code&gt;%&lt;/code&gt;之间多加了一个空格，实际使用的时候需要把空格去掉。&lt;/p&gt;

&lt;p&gt;第二种方法比较简单，在&lt;code&gt;layouts/partials/header_includes.html&lt;/code&gt;中加上：&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://yandex.st/highlightjs/8.0/highlight.min.js&#34;&gt;&lt;/script&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;https://yandex.st/highlightjs/8.0/styles/default.min.css&#34;&gt;
  &lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;这里使用了&lt;a href=&#34;http://yandex.ru/&#34;&gt;Yandex&lt;/a&gt;的&lt;a href=&#34;http://highlightjs.org/&#34;&gt;Highlight.js&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;其他的可以实现代码高亮的js库还有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://highlightjs.org/&#34;&gt;Highlight.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://craig.is/making/rainbows&#34;&gt;Rainbow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://alexgorbatchev.com/SyntaxHighlighter/&#34;&gt;Syntax Highlighter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://code.google.com/p/google-code-prettify/&#34;&gt;Google Prettify&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;插入图片:d605f9890f3528aea462ac7515ece633&#34;&gt;插入图片&lt;/h1&gt;

&lt;p&gt;图片文件放在&lt;code&gt;static/media&lt;/code&gt;文件中，插入图片：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ % img src=&amp;quot;/media/example.jpg&amp;quot; alt=&amp;quot;example&amp;quot; %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意这里的&lt;code&gt;{{&lt;/code&gt;和&lt;code&gt;%&lt;/code&gt;之间也加上了空格，避免这行代码起作用，实际使用也需要把空格去掉。&lt;/p&gt;

&lt;h1 id=&#34;使用mathjax:d605f9890f3528aea462ac7515ece633&#34;&gt;使用Mathjax&lt;/h1&gt;

&lt;p&gt;在需要渲染公式的页面加入以下代码，比如&lt;code&gt;layouts/_default/single.html&lt;/code&gt;文件，这个文件是对于所有post进行页面生成的模版，如果你希望所有页面都对公式渲染的话，可以加入&lt;code&gt;layouts/partials/footer.html&lt;/code&gt;文件里，保证所有生成的页面都有这几行代码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mathjax和Markdown会有冲突问题，&lt;a href=&#34;http://doswa.com/2011/07/20/mathjax-in-markdown.html&#34;&gt;这里&lt;/a&gt;提供了解决方案。&lt;/p&gt;

&lt;h1 id=&#34;用github-pages作为网站的host:d605f9890f3528aea462ac7515ece633&#34;&gt;用github pages作为网站的Host&lt;/h1&gt;

&lt;p&gt;Github pages分为两种：一种是项目主页，每个项目都可以有一个；另一种是用户主页，一个用户只能有一个。&lt;/p&gt;

&lt;p&gt;因为用户主页只能有一个，所以建议使用项目主页托管，不过我这里采用了用户主页，反正我也只用一个博客，使用个人主页作为Host也相对更简单一点。&lt;/p&gt;

&lt;p&gt;我们需要创建两个单独的repo，一个用于放Hugo的输入文件，即除了&lt;code&gt;public/&lt;/code&gt;文件夹之外的所有文件，另一个放我们生成的静态网站，也就是&lt;code&gt;public/&lt;/code&gt;的内容。&lt;/p&gt;

&lt;p&gt;步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在Github上创建repo &lt;code&gt;&amp;lt;your-project&amp;gt;-hugo&lt;/code&gt;，托管Hugo的输入文件。&lt;/li&gt;
&lt;li&gt;创建repo &lt;code&gt;&amp;lt;username&amp;gt;.github.io&lt;/code&gt;，用于托管&lt;code&gt;public/&lt;/code&gt;文件夹，注意这里的repo名字一定要用自己的用户名，才会被当作是个人主页。&lt;/li&gt;
&lt;li&gt;clone your-project&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ git clone &amp;lt;&amp;lt;your-project&amp;gt;-hugo-url&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;进入your-project 目录&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ cd &amp;lt;your-project&amp;gt;-hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;删掉public目录（这个目录每次运行Hugo都会再次生成，不用担心）&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ rm -rf public
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;把public/目录添加为submodule 与&lt;username&gt;.github.io同步&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ git submodule add git@github.com:&amp;lt;username&amp;gt;/&amp;lt;username&amp;gt;.github.io.git public
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;添加.gitignore文件，文件中写&lt;code&gt;public/&lt;/code&gt;，在同步&lt;code&gt;&amp;lt;your-project&amp;gt;-hugo&lt;/code&gt;时会忽略public文件夹&lt;/li&gt;
&lt;li&gt;下面是写好的一个script &lt;code&gt;deploy.sh&lt;/code&gt;，拷贝过去直接就能用，记得chmod +x deploy.sh加上运行权限。&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi

# Push Hugo content 
git add -A
git commit -m &amp;quot;$msg&amp;quot;
git push origin master


# Build the project. 
hugo # if using a theme, replace by `hugo -t &amp;lt;yourtheme&amp;gt;`

# Go To Public folder
cd public
# Add changes to git.
git add -A

# Commit changes.

git commit -m &amp;quot;$msg&amp;quot;

# Push source and build repos.
git push origin master

# Come Back
cd ..

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等一小会儿（10分钟左右），你就能在&lt;a href=&#34;http://username.github.io/&#34;&gt;http://username.github.io/&lt;/a&gt; 这个页面看到你的网站了！每次更新网站或者写了新文章，只需要运行./deploy.sh 发布就搞定了，简单吧？&lt;/p&gt;

&lt;p&gt;Github pages还支持域名绑定，三个步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在&lt;code&gt;&amp;lt;username&amp;gt;.github.io&lt;/code&gt; repo的跟目录下添加&lt;code&gt;CNAME&lt;/code&gt;文件，文件里写上你的域名，不用加http://的开头。&lt;/li&gt;
&lt;li&gt;记下&lt;a href=&#34;http://username.github.io/&#34;&gt;http://username.github.io/&lt;/a&gt; 的ip地址。&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ ping username.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;在你的域名管理中加上两条A记录，分别是www和@，记录指向&lt;a href=&#34;http://username.github.io/&#34;&gt;http://username.github.io/&lt;/a&gt; 的ip地址，也需要等一小会儿生效。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;更改字体服务商:d605f9890f3528aea462ac7515ece633&#34;&gt;更改字体服务商&lt;/h1&gt;

&lt;p&gt;我的博客模版里用的字体是从googleapis里获取的，国内访问会下载失败，把字体库改成360的。
找到&lt;code&gt;layouts/partials/head_includes.html&lt;/code&gt;文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;link href=&#39;http://fonts.googleapis.com/css?family=Fjalla+One|Open+Sans:300&#39; rel=&#39;stylesheet&#39; type=&#39;text/css&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将其中的googleapis替换为useso就行了。&lt;/p&gt;

&lt;p&gt;教程会根据我的博客遇到的问题继续更新。&lt;/p&gt;

&lt;h1 id=&#34;参考:d605f9890f3528aea462ac7515ece633&#34;&gt;参考&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://gohugo.io/overview/introduction/&#34;&gt;Hugo docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ipn.li/kernelpanic/3/&#34;&gt;《内核恐慌》静态网站生成器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/build-static-sites-in-seconds-with-hugo/&#34;&gt;Build Static Sites in Seconds with Hugo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/articles/setting-up-a-custom-domain-with-github-pages/&#34;&gt;Setting up a custom domain with GitHub Pages&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://nanshu.wang/about/</link>
      <pubDate>Fri, 30 Jan 2015 15:33:51 CST</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/about/</guid>
      <description>

&lt;h2 id=&#34;关于我:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;关于我&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;2010  HR@RUC&lt;/li&gt;
&lt;li&gt;2014  CS@ICT, CAS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;坐标北京，90后，女。&lt;/p&gt;

&lt;p&gt;本科的时候开始写博客，缘由是下定决心转CS专业，记录一下自己的学习和生活。现在看来，学习的内容有点太少了，生活的吐槽倒是一大堆。所以为了当一个好的程序媛，干脆弃掉原来的点点博客，折腾一个属于自己的地盘，记录一些学习笔记和自己的想法，就当督促自己吧。&lt;/p&gt;

&lt;p&gt;电邮: nanshu.wang@gmail.com&lt;/p&gt;

&lt;h2 id=&#34;喜欢:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;喜欢：&lt;/h2&gt;

&lt;p&gt;过山车 童话 Modern Family 刘慈欣 羽毛球 凉面 马尔克斯 Wes Anderson 恋爱的犀牛 安房直子 栀子花 22 42 GEEK笑点 三重镇 郑渊洁 Battlestar Galactica 抹茶味八喜冰淇淋 关东煮的蟹粉包 茄子 滑滑梯 Pixar 宫崎骏 智力游戏 Dixit 德州扑克 搜索引擎 豆瓣 油茶 粤菜 回锅肉 排骨香肠 番茄鸡蛋炒饭 小吊梨汤 咖啡厅 苏格拉底 Markdown Manictime 纪念碑谷 Popcap 红楼梦 Shameless Mac 东区全日早餐 早茶 三明治 滑雪 长跑 Sublime Text 鱼豆腐 油泼面 王小波 蒋勋 Python 卜东波老师 李钟硕 Running man 颜文字 韩语 Aaron Swartz Wunderlist 番茄土豆 Wikipedia 集邮&lt;/p&gt;

&lt;h2 id=&#34;不喜欢:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;不喜欢：&lt;/h2&gt;

&lt;p&gt;大部分肉类 过分亲密 用office工作 恐怖片 孤独感 话不投机&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scikit-Learn机器学习介绍（中文翻译）</title>
      <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/scikit-learn-docs-translation-1/</link>
      <pubDate>Tue, 02 Dec 2014 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%98%85%E8%8A%9C/scikit-learn-docs-translation-1/</guid>
      <description>

&lt;p&gt;翻译自：&lt;a href=&#34;http://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34;&gt;http://scikit-learn.org/stable/tutorial/basic/tutorial.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;以后可能会根据自己的学习慢慢翻译其他的章节，水平有限，不足之处请指正。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本章内容
在本章中，我们会介绍在使用scikit-learn中遇到的&lt;a href=&#34;http://en.wikipedia.org/wiki/Machine_learning&#34;&gt;机器学习&lt;/a&gt;(machine learning)术语，以及一个简单的机器学习例子。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;机器学习-问题设定:935e1a74f960fd04b26b502f6058f057&#34;&gt;机器学习：问题设定&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;一般来说，机器学习问题可以这样来理解：我们有n个&lt;a href=&#34;http://en.wikipedia.org/wiki/Sample_(statistics)&#34;&gt;样本&lt;/a&gt;(sample)的数据集，想要预测未知数据的属性。
如果描述每个样本的数字不只一个，比如一个多维的条目（也叫做&lt;a href=&#34;http://en.wikipedia.org/wiki/Multivariate_random_variable&#34;&gt;多变量数据&lt;/a&gt;(multivariate data)），那么这个样本就有多个属性或者&lt;strong&gt;特征&lt;/strong&gt;。
我们可以将学习问题分为以下几类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Supervised_learning&#34;&gt;有监督学习&lt;/a&gt;(unsupervised learning)是指数据中包括了我们想预测的属性，有监督学习问题有以下两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Classification_in_machine_learning&#34;&gt;分类&lt;/a&gt;（classification）:样本属于两个或多个类别，我们希望通过从已标记类别的数据学习，来预测未标记数据的分类。例如，识别手写数字就是一个分类问题，其目标是将每个输入向量对应到有穷的数字类别。从另一种角度来思考，分类是一种有监督学习的离散（相对于连续）形式，对于n个样本，一方有对应的有限个类别数量，另一方则试图标记样本并分配到正确的类别。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Regression_analysis&#34;&gt;回归&lt;/a&gt;(regression):如果希望的输出是一个或多个连续的变量，那么这项任务被称作*回归*，比如用年龄和体重的函数来预测三文鱼的长度。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Unsupervised_learning&#34;&gt;无监督学习&lt;/a&gt;(unsupervised learning)的训练数据包括了输入向量X的集合，但没有相对应的目标变量。这类问题的目标可以是发掘数据中相似样本的分组，被称作&lt;a href=&#34;http://en.wikipedia.org/wiki/Cluster_analysis&#34;&gt;聚类&lt;/a&gt;(Clustering)；也可以是确定输入样本空间中的数据分布，被称作&lt;a href=&#34;http://en.wikipedia.org/wiki/Density_estimation&#34;&gt;密度估计&lt;/a&gt;（density estimation）;还可以是将数据从高维空间投射到两维或三维空间，以便进行数据可视化。&lt;a href=&#34;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&#34;&gt;这里&lt;/a&gt;是Scikit-Learn的无监督学习主页。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;训练集和测试集
机器学习是关于如何从数据学习到一些属性并且用于新的数据集。这也是为什么机器学习中评估算法的一个习惯做法是将手头已有的数据集分成两部分：一部分我们称作&lt;strong&gt;训练集&lt;/strong&gt;（training set），用来学习数据的属性；另一部分叫做&lt;strong&gt;测试集&lt;/strong&gt;（testing set），用来测试这些属性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;加载样例数据集:935e1a74f960fd04b26b502f6058f057&#34;&gt;加载样例数据集&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;scikit-learn有一些标准数据集，比如用于分类的&lt;a href=&#34;http://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;iris&lt;/a&gt;和&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits&#34;&gt;digits&lt;/a&gt;数据集，和用于回归的&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Housing&#34;&gt;波士顿房价&lt;/a&gt;(boston house prices)数据集。
下面，我们会用shell里的Python解释器来加载&lt;code&gt;iris&lt;/code&gt;和&lt;code&gt;digits&lt;/code&gt;数据集。&lt;code&gt;$&lt;/code&gt;表示shell提示符，&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;表示Python解释器提示符：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ python
  &amp;gt;&amp;gt;&amp;gt; from sklearn import datasets
  &amp;gt;&amp;gt;&amp;gt; iris = datasets.load_iris()
  &amp;gt;&amp;gt;&amp;gt; digits = datasets.load_digits()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据集类似字典对象，包括了所有的数据和关于数据的元数据（metadata）。数据被存储在&lt;code&gt;.data&lt;/code&gt;成员内，是一个&lt;code&gt;n_samples*n_features&lt;/code&gt;的数组。在有监督问题的情形下，一个或多个因变量（response variables）被储存在&lt;code&gt;.target&lt;/code&gt;成员中。有关不同数据集的更多细节可以在&lt;a href=&#34;http://scikit-learn.org/stable/datasets/index.html#datasets&#34;&gt;这里&lt;/a&gt;被找到。
例如，在digits数据集中，&lt;code&gt;digits.data&lt;/code&gt;是可以用来分类数字样本的特征：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; print(digits.data)  # doctest: +NORMALIZE_WHITESPACE
  [[  0.   0.   5. ...,   0.   0.   0.]
   [  0.   0.   0. ...,  10.   0.   0.]
   [  0.   0.   0. ...,  16.   9.   0.]
   ...,
   [  0.   0.   1. ...,   6.   0.   0.]
   [  0.   0.   2. ...,  12.   0.   0.]
   [  0.   0.  10. ...,  12.   1.   0.]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;digits.target&lt;/code&gt;给出了digits数据集的真实值，即每个数字图案对应的我们想预测的真实数字：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; digits.target
  array([0, 1, 2, ..., 8, 9, 8])
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;数据数组的形式
数据是一个2维&lt;code&gt;n_samples*n_features&lt;/code&gt;的数组，尽管原始数据集可能会有不同的形式。在digits数据集中，每个原始样本是一个8*8的数组，可以用以下方式访问：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;    &amp;gt;&amp;gt;&amp;gt; digits.images[0]
    array([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],
         [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],
         [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],
         [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],
         [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],
         [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],
         [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],
         [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/plot_digits_classification.html#example-plot-digits-classification-py&#34;&gt;这个简单的例子&lt;/a&gt;说明了如何从原始问题里将数据形式化，以便scikit-learn使用。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;学习和预测:935e1a74f960fd04b26b502f6058f057&#34;&gt;学习和预测&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;在digits数据集中，我们的任务是给定一个图案，预测其表示的数字是什么。我们的样本有10个可能的分类（数字0到9)，我们将拟合一个&lt;a href=&#34;http://en.wikipedia.org/wiki/Estimator&#34;&gt;预测器&lt;/a&gt;(estimator)来&lt;strong&gt;预测&lt;/strong&gt;(predict)未知样本所属的分类。
在scikit-learn中，分类的预测器是一个Python对象，来实现&lt;code&gt;fit(X, y)&lt;/code&gt;和 &lt;code&gt;predict(T)&lt;/code&gt;方法。
下面这个预测器的例子是class&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;，实现了&lt;a href=&#34;http://en.wikipedia.org/wiki/Support_vector_machine&#34;&gt;支持向量机分类&lt;/a&gt;。创建分类器需要模型参数，但现在，我们暂时先将预测器看作是一个黑盒：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; from sklearn import svm
  &amp;gt;&amp;gt;&amp;gt; clf = svm.SVC(gamma=0.001, C=100.)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;选择模型参数
在这个例子里我们手动设置了&lt;code&gt;gamma&lt;/code&gt;值。可以通过这些工具例如&lt;a href=&#34;http://scikit-learn.org/stable/modules/grid_search.html#grid-search&#34;&gt;网格搜索&lt;/a&gt;（grid search）和&lt;a href=&#34;http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation&#34;&gt;交叉验证&lt;/a&gt;（cross validation）来自动找到参数的最佳取值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;给预测器取个名字叫做&lt;code&gt;clf&lt;/code&gt;（claasifier）。现在预测器必须来&lt;strong&gt;拟合&lt;/strong&gt;（fit）模型，也就是说，它必须从模型中&lt;strong&gt;学习&lt;/strong&gt;（learn）。这个过程是通过将训练集传递给&lt;code&gt;fit&lt;/code&gt;方法来实现的。我们将除了最后一个样本的数据全部作为训练集。通过Python语法&lt;code&gt;[:-1]&lt;/code&gt;来选择训练集，这会生成一个新的数组，包含了除最后一个条目的&lt;code&gt;digits.data&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; clf.fit(digits.data[:-1], digits.target[:-1])  # doctest: +NORMALIZE_WHITESPACE
  SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
    gamma=0.001, kernel=&#39;rbf&#39;, max_iter=-1, probability=False,
    random_state=None, shrinking=True, tol=0.001, verbose=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在你可以预测新值了，具体来说，我们可以询问分类器，&lt;code&gt;digits&lt;/code&gt;数据集里最后一个图案所代表的数字是什么，我们并没有用最后一个数据来训练分类器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; clf.predict(digits.data[-1])
  array([8])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最一个图案如下：
&lt;img src=&#34;http://scikit-learn.org/stable/_images/plot_digits_last_image_0011.png&#34; alt=&#34;此处输入图片的描述&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;如你所见，这项任务很具有挑战性：这个图案的分辨率很差。你能和分类器得到一致结果吗？
一个更复杂的分类问题的例子在这里:&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/plot_digits_classification.html#example-plot-digits-classification-py&#34;&gt;识别手写数字&lt;/a&gt;（Recognizing hand-written digits），供学习参考。&lt;/p&gt;

&lt;h2 id=&#34;模型持久性-model-persistence:935e1a74f960fd04b26b502f6058f057&#34;&gt;模型持久性（Model persistence）&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;可以采用Python内建的持久性模型&lt;a href=&#34;http://docs.python.org/library/pickle.html&#34;&gt;pickle&lt;/a&gt;来保存scikit的模型:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; from sklearn import svm
  &amp;gt;&amp;gt;&amp;gt; from sklearn import datasets
  &amp;gt;&amp;gt;&amp;gt; clf = svm.SVC()
  &amp;gt;&amp;gt;&amp;gt; iris = datasets.load_iris()
  &amp;gt;&amp;gt;&amp;gt; X, y = iris.data, iris.target
  &amp;gt;&amp;gt;&amp;gt; clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
    kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None,
    shrinking=True, tol=0.001, verbose=False)

  &amp;gt;&amp;gt;&amp;gt; import pickle
  &amp;gt;&amp;gt;&amp;gt; s = pickle.dumps(clf)
  &amp;gt;&amp;gt;&amp;gt; clf2 = pickle.loads(s)
  &amp;gt;&amp;gt;&amp;gt; clf2.predict(X[0])
  array([0])
  &amp;gt;&amp;gt;&amp;gt; y[0]
  0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在scikit的特定情形下，用joblib&amp;rsquo;s来代替pickle（&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;）会更吸引人，在大数据下效率更高，但只能pickle到磁盘而不是字符串：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; from sklearn.externals import joblib
  &amp;gt;&amp;gt;&amp;gt; joblib.dump(clf, &#39;filename.pkl&#39;) # doctest: +SKIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以在之后重新加载pickled模型（可以在另一个Python程序里）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; clf = joblib.load(&#39;filename.pkl&#39;) # doctest:+SKIP
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;注意：
joblib.dump返回一个文件名列表。每个包含在&lt;code&gt;clf&lt;/code&gt;对象中独立的numpy数组是在文件系统中是按顺序排列的一个独立文件。当用joblib.load重新加载模型时，所有文件必须在同一个目录下。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;注意pickle有一些安全性和维护性问题。请参考&lt;a href=&#34;http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence&#34;&gt;模型持久性&lt;/a&gt;章节获得更多关于scikit-learn模型持久性的信息。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>孤独的反思——读《孤独六讲》</title>
      <link>http://nanshu.wang/%E8%A7%82%E5%8F%B9/%E5%AD%A4%E7%8B%AC%E7%9A%84%E5%8F%8D%E6%80%9D/</link>
      <pubDate>Mon, 15 Sep 2014 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%A7%82%E5%8F%B9/%E5%AD%A4%E7%8B%AC%E7%9A%84%E5%8F%8D%E6%80%9D/</guid>
      <description>&lt;p&gt;第一次知道蒋勋，是默存同学送给我的一本《蒋勋的卢浮宫》。我对西方美术完全没有了解，这本书一开始我也很不以为然。蒋勋用他自己的美学视角，写的一本卢浮宫导游书。读一遍下来我一直以为蒋勋是一位年轻的艺术家，因为他的文字就像是一个人在你身旁慢慢说话，没有任何艰深难懂的地方，把我所以为很难懂的美术讲成了一个个平常的故事，不夸张也不造作，有一种原来美术是这样来理解的恍然大悟。&lt;/p&gt;

&lt;p&gt;后来，开始慢慢听《蒋勋说红楼梦》，才知道他原来主要是研究文学。也多亏他，我才意识到《红楼梦》是多么精彩而伟大的小说，以前走马观花的阅读失去了多少趣味。他讲话，总是觉察不出他的年龄，常常诧异怎么年轻人的心态他这么懂。我几乎和上一辈的人没有过深入的交谈和对话，但读蒋勋的文字就常常很高兴，觉得上一辈的也有有趣的，懂我们的人。&lt;/p&gt;

&lt;p&gt;毕业旅行去了台北，在诚品书店找到了《孤独六讲》，便买回来读。陌生的竖排繁体的排版，读起来却没有什么障碍。我记得一个大学同学，常常在地铁上读竖排的漫画书，他说竖排最适合在地铁里看，因为可以一只手拿书左右滚着看，另一只手可以去扶扶手。&lt;/p&gt;

&lt;p&gt;书的开头一段是坐在诚品书店里读的，里面写到他常常看到报上的新闻，会想到当事人的心里的孤独，全世界都在谈论你，却没有人愿意去你的心底看一看的感受，是多么巨大的孤独。当时我特别惊讶，因为我也有过同样的感觉，会想要去理解别人内心的孤独感，我从来没对人说过，却在这本书里读到了。这本从头读到尾，这样的感觉都一直伴随着我。&lt;/p&gt;

&lt;p&gt;蒋勋是赞颂孤独的，他一开头就说&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“孤独没有什么不好。使孤独变得不好，是因为你害怕孤独。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在情欲孤独里，他认为青少年时期必须经过一个孤独的阶段，必须在这个阶段里感受到完整的自我，才能发展出完整的感情。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“当你在暗恋一个人时，你的生命正在转换，从中发现出完美的自我。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;他提到，儒家文化一直以来是阻挡孤独和完整自我形成的最大障碍，这种集体主义的思想抹杀了独立个体的感情，不允许孤独存在。我接受我的情欲孤独，是因为我的感情要代表我的自我，而不要去代表社会所认同的我。&lt;/p&gt;

&lt;p&gt;语言孤独让我感到害怕，你看见一千张嘴在那里说话，却没有一只耳朵在听。你发出了声音，却永远地消失在无尽的黑暗里。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“看一本小说，不要看它写了什么，要看他没有写什么。如同你听朋友说话，不要听他讲了什么，要听他没有讲什么。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我每次放假回家，都不愿意和父母去饭后散步，不是因为我懒也不是因为不愿意陪伴父母，而是我不愿意每次遇到父母的朋友我都必须毕恭毕敬地打声毫无意义的招呼。语言的孤独在于我们嘴里的话都没有了意义，只有马斯洛需求层次最低一层的寒暄。最近班级聚餐时，邻座坐着一个和我气味相投的男生，和他很有话聊的时候，语言仿佛才有了意义，不是室友碎碎念周末进城见了谁吃了什么的话，而是平时的孤独感仿佛有人可以理解。&lt;/p&gt;

&lt;p&gt;革命孤独对于我是全新的话题。大概是被应试教育荼毒的原因，一提起革命我就老是想起高中的历史课本，里面一本正经告诉你革命伟大在哪里，我们该如何感恩，却从没有提起过革命者的思想，他们为何会在年轻时为革命献身，他们理解的革命是什么？读了秋瑾的故事，理解她的内心，才会体会其中的感人，才会觉得会掉下泪来。他们从时代走出，将生命置之度外的超脱，是他们背负的巨大的孤独。蒋勋提到了托尔斯泰临终前留下的一封信，信上说：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“我决定放弃我的爵位，我决定放弃我的土地，我决定要土地上所有的农奴恢复自由人身份。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;他写到：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“我觉得这是托尔斯泰最了不起的作品，他让我们看到革命是对自己的革命，他所要颠覆的不是外在的体制和阶级，而是颠覆内在的道德不安感。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这时我才反应过来，这些作品之所以令我着迷，是因为它在颠覆一些我不认同的东西，说出了我想说出的话。&lt;/p&gt;

&lt;p&gt;暴力孤独说的不仅仅是字面上的暴力，因为暴力的形式有很多，语言可以是暴力，感情可以是暴力，爱也可能是暴力。暴力是一种美学，因为暴力的源自于我们生存的本能，在弱肉强食，优胜劣汰中保存在了我们基因里。蒋勋用他的一篇小说《妇人明月的手指》的创作来说明文明社会中的暴力，这本小说读来让人不寒而栗，手指被砍断的暴力反而并没有大学生，出租车司机，警察的表现更让人害怕。而这是法律所不能管辖的暴力，也是我们所有人无时无刻无意识会施加的暴力。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“当你强势到某一个程度时，你不会意识到强势到了某个程度，不管是阶级，国家或是族群，本身就会构成暴力。但要产生这些自觉，并不是那么容易。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;蒋勋认为思维孤独是最大的一个孤独，因为思维孤独是最难以坚持和保留的。我们的儒家文化里没有哲学的思辨，只有已经规定好的是非伦理。我常常会质疑很多周围朋友的观点：比如女生不用太努力，比如什么年龄一定要结婚要有小孩，还有些是我还不敢说出来的观点。而这样的讨论往往会归总到一句大家耳熟能详的熟语，放佛几千年传来下的话一定没错似的，我竟然找不到反驳的理由。我佩服那些敢作那0.01%的人，害怕永远只做99.99%的人，就像书里说到在某个特殊时期，&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“有人讲说要怎么样怎么样的时候，你先不要动，先观察，然后发现有一半以上的人都这样讲的话，你就开始这样子讲，然后你千万不要变成那样最后的几个和最前面的几个，因为可能倒霉，靠错边就不好了。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;不知道这一段在大陆出版的书里有没有被删去呢？还有一段很喜欢的话：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“孤独一定要慢，当你急迫地从A点到B点时，所有的思考都停止。生命很简单，也是从A点到B点，由生到死。如果你一生都很忙碌，就表示你一生什么都没有看到，快速地从A点到了B点。难道生命的开始就是为了死亡吗？还是为了活着的每一分每一秒。与孤独相处的时候，可以多一点思维的空间，生命的过程会不会更细腻一点？”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;伦理孤独是很多时候让我感到痛苦的部分，因为我对这个社会的伦理划分的方式并不满意，或者我认为我们应该可以有自己的伦理选择，才能谈得上个人的完整。蒋勋是这样说的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“我一直期盼我们的社会能建立一个新的伦理，是以独立的个人为单位，先成为一个可以充分思考，完整的个人，再进而谈其他的相对伦理的关系。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我明白了我为什么会对《霍乱时期的爱情》里费尔明娜和《归来》里冯婉瑜抛开子女的爱情感到伟大，是因为我认为的爱情，是纯粹的，抛开伦理的，甚至是最重要的亲子观的。我向往的是不带有伦理的感情，那种自由，透明，没有束缚的表达，而不是伦理关系下迎合社会的结合。&lt;/p&gt;

&lt;p&gt;这本书谈论的是个人，个人的思想，个人的孤独。同样是谈论人，却比上课学的那些心理学，组织行为学更有用，更打动人。作为社会人，我们通常关注的和在社会关系网中和别人的连系的那些表示关系的边，却忽略了人首先是作为独立的点的存在，才会发展出边。我们忽略了自己的和别人的独立性，忽略了在社会中我们的孤独，让孤独变成不可以谈论，不可以表现，不可以赞颂的东西。但我们需要孤独，需要它来使自己更完美。整本书讲到的是我们作为社会人的悲哀，作为社会人的孤独。情欲，革命，暴力等的本质都是孤独。必须承认孤独，才能成为社会人。蒋勋在伦理孤独的最后说：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;期盼每个人都能在破碎重整的过程中找回自己的伦理孤独。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;希望我能在破碎重整的过程中找回自我。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>离开学还有十一天</title>
      <link>http://nanshu.wang/%E6%BD%87%E6%B9%98/%E7%A6%BB%E5%BC%80%E5%AD%A6%E8%BF%98%E6%9C%89%E5%8D%81%E4%B8%80%E5%A4%A9/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E6%BD%87%E6%B9%98/%E7%A6%BB%E5%BC%80%E5%AD%A6%E8%BF%98%E6%9C%89%E5%8D%81%E4%B8%80%E5%A4%A9/</guid>
      <description>&lt;p&gt;最近过着一种十分规律的生活：早睡早起，早午晚都去同样的地方吃饭，走同样上下班的路，进同样的电梯，按下同样的7楼，和同样的同伴说同样的话题。头回开始觉得，重复同样的生活并没有什么不好，甚至有点不想离开，去面对新学期新课程新同学新宿舍的新鲜劲。毕业已经有快2个月了，朋友圈里充斥着大家新生活的状态，或兴奋或无奈或沮丧或否极泰来，总之都带有一种move on的气氛。而面对我的新生活，我却更愿意待在回忆里不肯走出来。大概是因为心中还有郁结堆在那里，没有和珍惜的人好好道别，更是被自己的话一语中的，像是karma一样，明明是高中的翻版，我的角色对调了而已。&lt;/p&gt;

&lt;p&gt;好几个夜晚陷入了回忆而失眠，耳机里是将勋娓娓道来的红楼梦，再也回不去的曾经的美好如潮水般涌来。似乎人生的每个阶段都在惋惜上个阶段，初中惋惜童年，高中惋惜初中朋友，大学惋惜高中同学，大学毕业又开始惋惜大学最好的朋友。&lt;/p&gt;

&lt;p&gt;想像黛玉那样活，偏执，极致，宁为玉碎，不为瓦全。&lt;/p&gt;

&lt;p&gt;却只能像宝钗那样，失去立场，周全，说不出真心话。&lt;/p&gt;

&lt;p&gt;在家里说“自由的灵魂”这样的话，也会被妈妈取笑。只好抓住妈妈回忆自己年轻时是如何摆脱被安排的命运的时机，把自己的想法类比着全倒出来。妹妹说看我高中的日记本，突然就不想读书了。我却也只有模仿大人的语气，告诉她世俗价值观的对错。
果然多了好多长大了才有的想法，可是我还是想拒绝长大啊。&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;新学期课程不管怎么学，拿好成绩&lt;/li&gt;
&lt;li&gt;自己充电的计划不放松&lt;/li&gt;
&lt;li&gt;不间断看csapp&lt;/li&gt;
&lt;li&gt;下个月之内搞定GRE3000&lt;/li&gt;
&lt;li&gt;每天一道算法题&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>大学42句</title>
      <link>http://nanshu.wang/%E6%BD%87%E6%B9%98/%E5%A4%A7%E5%AD%A642%E5%8F%A5/</link>
      <pubDate>Sat, 29 Mar 2014 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E6%BD%87%E6%B9%98/%E5%A4%A7%E5%AD%A642%E5%8F%A5/</guid>
      <description>&lt;p&gt;3年前在&lt;a href=&#34;http://www.asyanyang.com/&#34;&gt;这个学姐&lt;/a&gt;的博客里看到反映我大学经历的50句话。就想着等我快毕业时，也要写这么一篇。最近失眠得厉害，必须到深夜大家都沉沉睡去时，才慢慢回忆起这些闪在脑海中的话。我一向觉得我长期记忆能力好差，要想凑够数量足够多的句子还要费上很长一段时间。但昨天的脑洞一开，这些句子简直是自己长了翅膀飞出来的一样，闭上眼睛，仿佛说话的人就在耳边。这些句子的主人，有最亲密的好友，也有仅一面之缘的陌生人，有老师，有同学，有父母，有朋友，有恋人，也有我自己。有的话恐怕当事人已经不再记得，只有我知道这些句子在当时是如何惊起心中的波澜，对我多么宝贵。本来也想凑够50句，但第42句已经是前几天毕业采访的时候了，况且42是一个多么符合GEEK气质的数字啊，凑不够也就作罢。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Carpe diem.&lt;/li&gt;
&lt;li&gt;精神使人活。&lt;/li&gt;
&lt;li&gt;曾经我因为看《红楼梦》太入迷，抑郁了好几年。&lt;/li&gt;
&lt;li&gt;你们组的时间已经到了，余下的不用讲了。&lt;/li&gt;
&lt;li&gt;我们家的一个时代结束了，有你二妈的时代永远不会回来了。&lt;/li&gt;
&lt;li&gt;当然记得你的声音啊。&lt;/li&gt;
&lt;li&gt;彩虹扣子，只送给你的呀。&lt;/li&gt;
&lt;li&gt;少年，你的生活已经足够好了。&lt;/li&gt;
&lt;li&gt;这是我的保留节目。&lt;/li&gt;
&lt;li&gt;我也有些醉了，说最后一句话。&lt;/li&gt;
&lt;li&gt;我签一下字，剩下的你自己写吧。&lt;/li&gt;
&lt;li&gt;送你一对耳钉，美丽的背后都有疼痛。&lt;/li&gt;
&lt;li&gt;留下吧，我们都留下吧。&lt;/li&gt;
&lt;li&gt;即使所有人都不行，你还有我呢。&lt;/li&gt;
&lt;li&gt;应该早一点认识你的。&lt;/li&gt;
&lt;li&gt;这一切的理论和论点都不成立的话，为什么我们要生活在一个巨大的谎言之下。&lt;/li&gt;
&lt;li&gt;我最大的梦想就是等赚够了钱，就回四川开一家饭馆。&lt;/li&gt;
&lt;li&gt;你还年轻，应该去闯，去经历。爸爸妈妈都支持你。&lt;/li&gt;
&lt;li&gt;你为什么总是这样奇怪，考虑下我的感受。&lt;/li&gt;
&lt;li&gt;我只是一台会哭的机器。&lt;/li&gt;
&lt;li&gt;舒舒姐姐生日快乐。&lt;/li&gt;
&lt;li&gt;果然经历过地震心态都不一样。&lt;/li&gt;
&lt;li&gt;这都没什么所谓。&lt;/li&gt;
&lt;li&gt;每天晚上要读一点《黄金时代》才能睡着。&lt;/li&gt;
&lt;li&gt;每次到这种时候我才知道自己真的是有多爱你。&lt;/li&gt;
&lt;li&gt;小姑娘怎么想的，你们院专业多好啊。&lt;/li&gt;
&lt;li&gt;这是我婚礼上讲过的故事。&lt;/li&gt;
&lt;li&gt;你现在就该玩。&lt;/li&gt;
&lt;li&gt;如果有一天他回头，我也不知道会怎么做。&lt;/li&gt;
&lt;li&gt;字我已经签了，跟我来有几句话要说。&lt;/li&gt;
&lt;li&gt;第一次这样称呼你，总之谢谢。&lt;/li&gt;
&lt;li&gt;为了我女朋友，就该拼命去奋斗。&lt;/li&gt;
&lt;li&gt;街上一个人都没有，我就把耳机开到最大，跑调非常严重的唱歌。&lt;/li&gt;
&lt;li&gt;大学能遇到一个可以交心的朋友，真庆幸。&lt;/li&gt;
&lt;li&gt;我没什么目标，不知道要干嘛。&lt;/li&gt;
&lt;li&gt;因为你们说都来过青岛了，我很感动。&lt;/li&gt;
&lt;li&gt;分开的想法大于复合吧。&lt;/li&gt;
&lt;li&gt;Qualified or not, merely depends on time, just make sure that you are right on the track.&lt;/li&gt;
&lt;li&gt;原来你并没有我想像的那样独立。&lt;/li&gt;
&lt;li&gt;我觉得我还有好多没有经历。&lt;/li&gt;
&lt;li&gt;Thank You Letter&lt;/li&gt;
&lt;li&gt;我从一个怀疑生活的人变成了一个相信命运的人。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>近期计划</title>
      <link>http://nanshu.wang/%E6%BD%87%E6%B9%98/%E8%BF%91%E6%9C%9F%E8%AE%A1%E5%88%92/</link>
      <pubDate>Fri, 21 Mar 2014 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E6%BD%87%E6%B9%98/%E8%BF%91%E6%9C%9F%E8%AE%A1%E5%88%92/</guid>
      <description>&lt;p&gt;说来惭愧，尘埃落定之后便抛弃了记录学习过程的习惯。最近实在是有点颓废，记一下好勉励自己。&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;搞定毕业论文&lt;/li&gt;
&lt;li&gt;edX的课程跟上，不要拖过deadline&lt;/li&gt;
&lt;li&gt;找暑期实习&lt;/li&gt;
&lt;li&gt;不要看闲书，看正经书&lt;/li&gt;
&lt;li&gt;背GRE单词&lt;/li&gt;
&lt;li&gt;学习《韩国语》第一册&lt;/li&gt;
&lt;li&gt;少刷微博少逛淘宝&lt;/li&gt;
&lt;li&gt;第二次去青岛，girls&amp;rsquo; spring break, 深夜啤酒！&lt;/li&gt;
&lt;li&gt;从开学以来保持了记账的好习惯，果然开支控制得很好，但是天气好起来就想买小裙子的心情无法抑制&lt;/li&gt;
&lt;li&gt;第二个习惯是听电台，IT公论简直是涨姿势的好途径，好喜欢Rio&lt;/li&gt;
&lt;li&gt;最近想追的：Ellen show巨好笑， 简直要爱上Ellen Degeneres了T T；韩国版的爸爸我们去哪儿也好好看&amp;gt;&amp;lt;&lt;/li&gt;
&lt;li&gt;每天刷instagram也要爱上Jesse Tyler了星星眼&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>《恋爱的犀牛》</title>
      <link>http://nanshu.wang/%E8%A7%82%E5%8F%B9/%E6%81%8B%E7%88%B1%E7%9A%84%E7%8A%80%E7%89%9B/</link>
      <pubDate>Thu, 09 Jan 2014 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/%E8%A7%82%E5%8F%B9/%E6%81%8B%E7%88%B1%E7%9A%84%E7%8A%80%E7%89%9B/</guid>
      <description>&lt;p&gt;看现场的话剧比书本上的对话明朗了许多，说到最后这也不过是一个“我爱你，你不爱我”的故事。用最近从《程序员的数学》上学来的分组方法，爱情的分组不外乎是一下三种：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;我爱你，你也爱我&lt;/li&gt;
&lt;li&gt;我爱你，你不爱我&lt;/li&gt;
&lt;li&gt;你爱我，我也爱你&lt;/li&gt;
&lt;li&gt;你爱我，我不爱你&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其中，1,3都是Happy ending（如果没有婆媳矛盾，身世之谜种种阻挠），然而大部分陷入爱情的迷阵里的青年，都在2,4里苦苦挣扎：有的有幸在2,4的循环里进入了1,3的接口，终止了自己的不幸；有的却在这个循环里咒骂爱情的无情，不得逃脱。&lt;/p&gt;

&lt;p&gt;以前我总觉得，聪明的人一定会冲着2,4move on：要么从2到1，积极努力不行再换个人；要么从4到3，自己的信再铁石心肠也会有被打动的一天吧。然而《恋爱的犀牛》却不是讲着完全相反的故事，爱着明明的马路不停得重复这句告白：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;一切白的东西和你相比都成了黑墨水而自惭形秽，一切无知的鸟兽因为不能说出你的名字而绝望万分。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;爱着成飞的明明在受伤时说：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;我还要对他顺从到哪一天，我真不知道我还有什么事不能为他做！这个可恨的人！我要是不爱他了，该多好。我眼睛里带着爱情就像是脑门上带着奴隶的印记，他走到哪儿我就要跟到哪儿！你能想象吗？只要跟着他我就满足了。真是发疯，怎么样才能不再爱他呢？嗯？这样下去我会受不了的！可我要是不爱他了，活着还有什么意思呢？我从来没见过像他这样的男人，我下了多少次决心，可一看见他，完蛋了…….&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;他们两人固执得不肯从2,4里跳出死循环，死死抓住那句“我爱你”或“我不爱你”，其余什么都不要。大概大部分人都是自私而愚蠢的，才会造成怎么多爱情悲剧的存在：“我”是自私的，“爱”或“不爱”是愚蠢的，“你”和“我”都是悲剧的。没有一个好下场的故事，为什么又要死死把自己扣进那个循环呢。&lt;/p&gt;

&lt;p&gt;大概“爱的伟大”可以解释这个现象。“爱”是人类最纯洁，最无私，最高尚的一种情感，在柏拉图的《会饮》里，Agathon对爱神进行了最美妙的赞美：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Therefore, Phaedrus, I say of Love that he is the fairest and best in himself, and the cause of what is fairest and best in all other things. And there comes into my mind a line of poetry in which he is said to be the god who&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Gives peace on earth and calms the stormy deep,&lt;/p&gt;

&lt;p&gt;Who stills the winds and bids the sufferer sleep.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is he who empties men of disaffection and fills them with affection, who makes them to meet together at banquets such as these: in sacrifices, feasts, dances, he is our lord-who sends courtesy and sends away discourtesy, who gives kindness ever and never gives unkindness; the friend of the good, the wonder of the wise, the amazement of the gods; desired by those who have no part in him, and precious to those who have the better part in him; parent of delicacy, luxury, desire, fondness, softness, grace; regardful of the good, regardless of the evil: in every word, work, wish, fear-saviour, pilot, comrade, helper; glory of gods and men, leader best and brightest: in whose footsteps let every man follow, sweetly singing in his honour and joining in that sweet strain with which love charms the souls of gods and men. Such is the speech, Phaedrus, half-playful, yet having a certain measure of seriousness, which, according to my ability, I dedicate to the god.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;既然古希腊的这位伟大的喜剧家都对爱进行了这样的最高赞誉，那爱一定是没错的，是会带来美德和幸福的。爱既然没错，我的“爱”也一定没错，我的“爱”驱使我做的事也一定没错。爱是多么正确和美妙啊，大部分人一定是这样想的，因为我们老能听见电视里不肯放手的男二悲催的台词：&lt;em&gt;我爱你难道有错吗？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;奇怪的是，《恋爱的犀牛》里，明明出场时却是这样一段质疑爱的话：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;我是说“爱”！那感觉是从哪来的？从心脏、肝脾、血管，哪一处内脏里来的？也许那一天月亮靠近了地球，太阳直射北回归线，季风送来海洋的湿气使你皮肤润滑，蒙古形成的低气压让你心跳加快。或者只是你来自你心里的渴望，月经周期带来的骚动，他房间里刚换的灯泡，他刚吃过的橙子留在手指上的清香，他忘了刮的胡子刺痛了你的脸……这一切作用下神经末梢麻酥酥的感觉，就是所说的爱情……&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;倘若爱情的产生仅仅是周围环境而引起的内啡肽的化学变化，爱的本质似乎就没有Agathon说的那样高尚。一瞬间而且大部分紧靠外貌被吸引的异性，慢慢作用催生出的那个东西，被称之为爱。既然爱可以慢慢滋长，它也必然可以慢慢消退。由此永远陷入爱情2,4的死循环的判断是不靠谱的。世上一定要这样的人，《霍乱时期的爱情》里阿里萨就是这样，等待了几十年的爱情，而且非得等到心爱人老公死掉了才有机会，这样的浪漫主义其实是可悲的。&lt;/p&gt;

&lt;p&gt;《恋爱的犀牛》里大仙说：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;爱情跟喜剧、体育、流行音乐没什么不同，是为了让人活得轻松愉快的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这话听上去又轻浮又没责任心，然而比起那些陷入循环要死要活的爱情来说，我宁愿轻松一点，也不愿把原本应该被歌颂的爱变成我的苦难。&lt;/p&gt;

&lt;p&gt;《会饮》里还说了，&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Love is simply the name for the desire and pursuit of the whole.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Whole是需要两人在一起的，别固执也别强求。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
