<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>蘅芜s on Nanshu&#39;s blog </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://nanshu.wang/%E8%98%85%E8%8A%9C/</link>
    <language>en-us</language>
    <author>Nanshu Wang</author>
    <copyright>Copyright (c) 2015, Nanshu Wang; all rights reserved.</copyright>
    <updated>Wed, 01 Jul 2015 00:00:00 UTC</updated>
    
    <item>
      <title>机器学习第一战——阿里天池移动推荐算法比赛经验总结攻略</title>
      <link>http://nanshu.wang/post/2015-07-01</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-07-01</guid>
      <description>

&lt;p&gt;历时98天的阿里移动推荐算法终于结束了，有种终于下了贼船的感觉。总的来说，整个比赛的体验并不好：时间太长，资源不够，打酱油的队友/男票，运气成分大，学到的干货少。阿里还有两场的比赛都不想参加了，觉得没必要把宝贵的研二时间都撘进去。虽然学到的干货不多，但这毕竟是在Data Science道路上进军的第一场实战，还是有必要好好总结一下。&lt;/p&gt;

&lt;h1 id=&#34;比赛统计:e22b26d8c6b3a4529619561860cf7538&#34;&gt;比赛统计&lt;/h1&gt;

&lt;p&gt;初赛名次：21
复赛名次：22
Python代码行数：2320
SQL代码行数：6656
天池平台数据表数：1006
线下结果数：490&lt;/p&gt;

&lt;h1 id=&#34;初识比赛:e22b26d8c6b3a4529619561860cf7538&#34;&gt;初识比赛&lt;/h1&gt;

&lt;p&gt;虽然比赛的名字叫做移动推荐算法，但本质上也是一个机器学习的问题。比赛给出了一个月（2014.11.18~2014.12.18）的用户对商品操作的行为数据，需要预测12.19号这天的购买行为，评分采用经典的F1值计算。行为数据中包括了时间、地点、用户、商品、行为类型、商品类别六个要素，特征提取的思路就是围绕这六个要素进行。题目还给出了一个商品子集，只需要提交对商品子集购买行为的预测结果。&lt;/p&gt;

&lt;p&gt;大致看了去年前十选手的比赛总结，比赛流程大致分为4个模块：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;特征提取&lt;/li&gt;
&lt;li&gt;训练集构造&lt;/li&gt;
&lt;li&gt;模型学习调参&lt;/li&gt;
&lt;li&gt;模型融合&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这4个模块并不是相互独立的，比如特征提取和训练集构造通常是一起完成，构造训练集也会用到简单模型学习来平衡正负样本数。&lt;/p&gt;

&lt;p&gt;比赛分为两个赛季：
1. 初赛3.20-4.25，仅提供1万用户数据，采用本地调试提交。
2. 复赛4.30-7.1，有500万用户数据，使用阿里天池平台提交。&lt;/p&gt;

&lt;p&gt;初赛和复赛是需要区别对待的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;工具：初赛本地调试，学习模型和特征提取并不拘泥，可以用任意熟悉的工具。然而复赛需要使用天池平台，工具受限制。&lt;/li&gt;
&lt;li&gt;模型or规则？：初赛数据量很小，很可能模型效果并不好，事实证明，规则实际上是简单粗暴效果佳的。复赛则不可能寄希望于规则，老老实实搞模型吧。&lt;/li&gt;
&lt;li&gt;时间分配：初赛只用进前500名即可，不用投入太多的精力。到复赛前期越快熟悉环境越好，尽量前期多做，比赛资源并不充足，后期资源会非常有限，速度很慢。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;初赛:e22b26d8c6b3a4529619561860cf7538&#34;&gt;初赛&lt;/h1&gt;

&lt;p&gt;初赛的环境使用了mySQL+Python，选择Python的原因是数据挖掘的包很齐全也容易上手：numpy、scipy、pandas，还有机器学习包scikit-learn，对付比赛足够用了。&lt;/p&gt;

&lt;h2 id=&#34;特征提取:e22b26d8c6b3a4529619561860cf7538&#34;&gt;特征提取&lt;/h2&gt;

&lt;p&gt;根据行为数据的各个字段，很容易可以将特征分为以下几类：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;用户-商品UI特征&lt;/li&gt;
&lt;li&gt;用户特征&lt;/li&gt;
&lt;li&gt;商品特征&lt;/li&gt;
&lt;li&gt;类别特征&lt;/li&gt;
&lt;li&gt;地理位置特征&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;特征类别主要是计数和4行为转化率，比如用户1行为计数，4行为计数/1行为计数等等。&lt;/p&gt;

&lt;p&gt;第一赛季并没有想到时间特征，只是根据时间划分训练集，第二赛季才发现时间特征非常重要。&lt;/p&gt;

&lt;p&gt;为了便于特征提取，我用两种方法存放数据，一种是存入SQL表中：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;建立表logUser，user_id为主键,
字段名：
user_id,item_start,item_end&lt;/p&gt;

&lt;p&gt;建立表logItem,item_idx为主键，
字段名：
item_idx,item_id,bhv_start,bhv_end&lt;/p&gt;

&lt;p&gt;建立表logBhv,bhv_idx为主键，
字段名：
bhv_idx, behavior_type,user_geohash,item_category,time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这样存放可以方便地提取UI特征，但事实证明这种方法实际上很慢，因为需要大量的SQL查询操作，不如用SQL的group by操作。幸而第一赛季数据量并不大，提取训练集并不是瓶颈。&lt;/p&gt;

&lt;p&gt;第二种方法是将数据按天分开，便于提取user、item、geo、category的全局特征，这部分特征我全部使用了awk脚本提取。&lt;/p&gt;

&lt;h2 id=&#34;构造训练集:e22b26d8c6b3a4529619561860cf7538&#34;&gt;构造训练集&lt;/h2&gt;

&lt;p&gt;线下训练集使用12.18号购买行为标注样本，严格使用18号之前的数据提取特征（包括全局特征），样本为18号之前n天有交互行为的UI对。
线上预测集则使用19号之前n天所有的交互行为的UI对。&lt;/p&gt;

&lt;p&gt;由于正例负例不平衡，需要对训练集进行抽样，抽样比例是在1:5到1:20这个区间里找最优结果。&lt;/p&gt;

&lt;h2 id=&#34;模型训练:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型训练&lt;/h2&gt;

&lt;p&gt;主要使用了Adaboost，Random forrest，Logistic Regression三种模型，直接调用Scikit-lSearn提供的模型训练。其中树型模型的测试结果最好。&lt;/p&gt;

&lt;p&gt;然而第一赛季受到数据量的限制，实际上规则比模型好用多了，比如购物车+时间规则：前一天晚上8点后加入购入车的UI，就可以做到比单个模型结果好很多。&lt;/p&gt;

&lt;h2 id=&#34;模型融合:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型融合&lt;/h2&gt;

&lt;p&gt;第一赛季比赛前期一直死磕模型，成绩一直提不上去，改成规则后才大呼坑爹。&lt;/p&gt;

&lt;p&gt;初赛的最优成绩使用了规则+模型混合的方法，使用规则得到候选集后，再从候选集中去掉多个模型预测的top k负例交集。&lt;/p&gt;

&lt;h1 id=&#34;复赛:e22b26d8c6b3a4529619561860cf7538&#34;&gt;复赛&lt;/h1&gt;

&lt;p&gt;第二赛季要使用天池平台，第一赛季的代码都用不上，得全部推翻重写。天池平台的工具有SQL、Map Reduce、UDF，算法平台也提供了经典的机器学习模型。&lt;/p&gt;

&lt;p&gt;SQL容易学习，但写起来复杂容易出错，没有可读性，不提供参数设置，复用比较麻烦。Map Reduce和UDF学习成本高，可以实现复杂逻辑，但天池对这两个工具的限制很多，文档对用户非常不友好。&lt;/p&gt;

&lt;p&gt;大部分特征、训练集和模型基本需求如计算f1等等都是通过SQL实现，少部分特征和其他复杂功能使用了UDF和Map Reduce。比较有用的是Map Reduce实现了特征information gain的计算，用来进行特征筛选，这要感谢队友/男票&lt;a href=&#34;http://www.xgezhang.com&#34;&gt;xge&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;特征提取-1:e22b26d8c6b3a4529619561860cf7538&#34;&gt;特征提取&lt;/h2&gt;

&lt;p&gt;整个复赛的特征工程一共进行了10次更改，更改大多是增加新特征，从最初的22维增加到最后的380维。特征构建还是一赛季的思路，不过根据一赛季的结果，筛选掉了一部分无用特征。除了简单计数和4行为转化特征，在第一赛季基础上增加的特征有：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;时间特征

&lt;ul&gt;
&lt;li&gt;UI行为距离标注日的小时数&lt;/li&gt;
&lt;li&gt;用时间衰减来计算加权后的行为计数&lt;/li&gt;
&lt;li&gt;多次行为操作之间的时间间隔(Map Reduce实现)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;不同时间粒度的UI特征

&lt;ul&gt;
&lt;li&gt;按照标注日前1天，3天，7天，30天为粒度提取&lt;/li&gt;
&lt;li&gt;按照标注日前4小时，8小时，16小时为粒度提取&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加地理特征

&lt;ul&gt;
&lt;li&gt;用户商品的距离&lt;/li&gt;
&lt;li&gt;用户地理用最后操作位置填充&lt;/li&gt;
&lt;li&gt;商品地理用购买该商品的用户位置填充&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;计数特征去重

&lt;ul&gt;
&lt;li&gt;商品特征计数对用户去重&lt;/li&gt;
&lt;li&gt;类别特征计数对商品去重&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加交叉特征

&lt;ul&gt;
&lt;li&gt;用户&amp;amp;UI特征交叉&lt;/li&gt;
&lt;li&gt;商品&amp;amp;UI特征交叉&lt;/li&gt;
&lt;li&gt;类别&amp;amp;UI特征交叉&lt;/li&gt;
&lt;li&gt;用户&amp;amp;商品特征交叉&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;特征增加对分数提升是最显著的，在特征选择中也需要注意：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;不要盲目添加大量特征，最好一部分一部分添加，这样会对添加的特征效果有个大体的认识。&lt;/li&gt;
&lt;li&gt;添加的特征要能从现实逻辑上解释得通，最好是直观上影响购买行为的特征。&lt;/li&gt;
&lt;li&gt;特征筛选或许有用，但最好添加特征时就尽量加入有用的特征，不要妄图从一大堆特征中再筛选。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;训练集构造:e22b26d8c6b3a4529619561860cf7538&#34;&gt;训练集构造&lt;/h2&gt;

&lt;p&gt;构造训练集前对数据进行了一个初步的统计，第二天的购买行为中仅有32%的UI是有过交互的，剩下64%的购买行为都是当天的偶发购买。前1天有过交互的占16%，前2天21%，前2天23%，前4天25%。也就是说，有一半的正样本是前1天的，因此&lt;strong&gt;仅仅使用前1天的交互UI构造训练集&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;考虑到仅仅需要对商品子集进行预测，为了使训练集和预测集保持一致，又可以减少训练集的规模，所以&lt;strong&gt;只使用了商品子集内的商品构造训练集&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;在比赛前期，这样构造训练集是有效的，缩短了模型训练时间，加快了特征迭代的工作。但是当特征增加不能再提升分数时，应该想到，这样构造训练集存在很大的问题，丢掉了很多正例样本。和其他队伍交流来看，这实际上是比赛后期的一个瓶颈，限制了特征的发挥。&lt;/p&gt;

&lt;p&gt;正确的做法是：在特征工程比较完善之后，最好是比赛中期，使用不止1天的交互和商品全集构造训练集，采用简单模型过滤掉大量负样本。这点我到比赛后期才开始改，平台资源已经不够了，所以并没有做好，也是比赛的一大遗憾。&lt;/p&gt;

&lt;p&gt;可以通过不同的标注日来得到不同的训练集，注意要预留出相同规模的验证集的测试集，到模型融合时会有用。&lt;/p&gt;

&lt;h2 id=&#34;模型学习-调参:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型学习+调参&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;逻辑回归&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;逻辑回归（Logistic Regression）训练速度预测速度很快，预测效果较差，可以用来进行负样本筛选，模型融合也有用。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;随机森林&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;随机森林学习速度一般，预测速度很慢，主要用于模型融合。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GBDT二分类&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GBDT训练速度慢，预测速度一般，单模型的效果最优。唯一的调参经验是：树越多越好，但速度也越慢。并且特征不一样，最优的参数也不一样，调参时注意每次最好只改变一个参数值，否则不容易看出参数的影响。不要使用枚举参数的办法调参，太浪费时间。&lt;/p&gt;

&lt;h2 id=&#34;模型融合-1:e22b26d8c6b3a4529619561860cf7538&#34;&gt;模型融合&lt;/h2&gt;

&lt;p&gt;总结一下道听途说模型融合的方法：
1. 取不同模型top k并集
2. 取不同模型top k交集
3. 直接将预测概率相加
4. 将预测概率作为特征用lr训练
5. 将预测概率作为特征加入验证集，再讲验证集当做训练集训练。&lt;/p&gt;

&lt;p&gt;对我来说以上5种方法都是然并卵，不论使用哪种融合方式，总没有单模型的结果好。具体的原因还没有找到，或许是因为没有对训练集进行抽样，GBDT模型对正负样本比例并不敏感，所以我直接用了所有的负样本，也算是这次比赛的第二大遗憾吧。&lt;/p&gt;

&lt;h1 id=&#34;其他收获:e22b26d8c6b3a4529619561860cf7538&#34;&gt;其他收获&lt;/h1&gt;

&lt;p&gt;与其说是收获，不如说是一些犯过的错误，总结出来自勉：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;不要过早优化
主要是模型调参和模型融合，这两部分应当留到比赛中后期来做，过早优化耽误时间也没有意义。&lt;/li&gt;
&lt;li&gt;不要短视
比赛每天都在更新名次，但不要只关注短期的分数，更要为长期做打算。比如虽然调参、获得更多的训练集会提升暂时的分数，但是也失去了快速迭代特征的机会。要时刻记住这是一场长时间的比赛，终点才是胜利，中间领跑并不说明问题。&lt;/li&gt;
&lt;li&gt;合理安排时间精力
初赛不需要投入全部精力，复赛前期尽量多做，否则后期没有资源。&lt;/li&gt;
&lt;li&gt;不要妄下结论
对于不确定的猜想，一定要用数据验证，比如训练集用商品全集还是子集的区别，是否需要抽样等等。尽量多试多做，要去尝试各种可能性。&lt;/li&gt;
&lt;li&gt;不要懒，不要懒，不要懒
重要的事情说三遍。有时会觉得麻烦，于是采用了写起来简单但效率低的方法。有时明明应该改成另一种方法，又觉得不改也没影响就算了吧。有时觉得今天太晚了就不做了，马虎总是不检查提交结果，于是又浪费了很多次机会。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记7 高偏差/低偏差，学习曲线，模型选择</title>
      <link>http://nanshu.wang/post/2015-05-17</link>
      <pubDate>Sun, 17 May 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-05-17</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;p&gt;原文：&lt;a href=&#34;https://share.coursera.org/wiki/index.php/ML:Advice_for_Applying_Machine_Learning&#34;&gt;https://share.coursera.org/wiki/index.php/ML:Advice_for_Applying_Machine_Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;面对一个机器学习问题，我们提取好特征，挑选好训练集，选择一种机器学习算法，然后学习预测得到了第一步结果。然而我们不幸地发现，在测试集上的准确率低得离谱，误差高得吓人，要提高准确率、减少误差的话，下一步该做些什么呢？&lt;/p&gt;

&lt;p&gt;可以采用以下的方法来减少预测的误差：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获得更多的训练样本&lt;/li&gt;
&lt;li&gt;减少特征的数量&lt;/li&gt;
&lt;li&gt;增加特征的数量&lt;/li&gt;
&lt;li&gt;使用多项式特征&lt;/li&gt;
&lt;li&gt;增大或减小正则化参数$\lambda$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但不要盲目在这些可行的方法里随便选一种来提升模型，需要用一些诊断模型的技术来帮助我们选择使用哪种策略。&lt;/p&gt;

&lt;h1 id=&#34;1-评估假设:876321dc83c64489eb74f98905ca718c&#34;&gt;1.评估假设&lt;/h1&gt;

&lt;p&gt;即使模型假设对于训练集的误差很低，若存在过拟合，模型的预测也同样会不准确。&lt;/p&gt;

&lt;p&gt;给定一份训练集，我们可以将数据分成两部分：训练集和测试集。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;使用训练集最小化$J(\Theta)$得到$\Theta$参数&lt;/li&gt;
&lt;li&gt;计算测试集的误差：&lt;/li&gt;
&lt;/ol&gt;

&lt;div&gt;
$$J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$$
&lt;/div&gt;

&lt;p&gt;3.计算分类错误率（即0/1分类错误率）&lt;/p&gt;

&lt;div&gt;
$$err(h_\Theta(x),y) =
\begin{matrix}
1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline
0 &amp; \mbox otherwise 
\end{matrix}$$
&lt;/div&gt;

&lt;p&gt;测试集的平均误差为：&lt;/p&gt;

&lt;div&gt;
$$\large
\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})$$
&lt;/div&gt;

&lt;p&gt;也就是测试集上分类错误的样本的比例。&lt;/p&gt;

&lt;h1 id=&#34;2-模型选择与训练-验证-测试集:876321dc83c64489eb74f98905ca718c&#34;&gt;2.模型选择与训练/验证/测试集&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;学习算法若仅仅对训练集拟合较好，并不能说明其假设也是好的。&lt;/li&gt;
&lt;li&gt;训练集上的假设误差通常要比其他数据集上得到的误差要小。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了在假设上选择模型，可以测试模型的多项式的次数来观察误差结果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;无验证集&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对不同的多项式次数的模型通过训练集得到最优化参数$\Theta$。&lt;/li&gt;
&lt;li&gt;找到在预测集上误差最小的模型的多项式次数$d$。&lt;/li&gt;
&lt;li&gt;使用测试集估计泛化误差$J_{test}(\Theta^{(d)})$。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在这个例子中，我们用测试集训练得到的一个变量，即多项式次数$d$，但这样做会使其他数据集的误差更大。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，我们引入了第三种数据集，即交叉验证集(Cross Validation Set)，来作为选择$d$的中间数据集。这样，测试集会给出一个准确，非乐观估计的误差结果。&lt;/p&gt;

&lt;p&gt;例如，将数据集分成三份：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;训练集：60%&lt;/li&gt;
&lt;li&gt;交叉验证集：20%&lt;/li&gt;
&lt;li&gt;测试集：20%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于这三个数据集我们可以计算三个不同误差值：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;有验证集&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对不同的多项式次数的模型通过训练集得到最优化参数$\Theta$。&lt;/li&gt;
&lt;li&gt;找到在验证集上误差最小的模型的多项式次数$d$。&lt;/li&gt;
&lt;li&gt;使用测试集估计泛化误差$J_{test}(\Theta^{(d)})$。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;使用验证集则避免了使用测试集来确定多项式次数$d$。&lt;/p&gt;

&lt;h1 id=&#34;3-诊断偏差-vs-方差:876321dc83c64489eb74f98905ca718c&#34;&gt;3.诊断偏差 vs. 方差&lt;/h1&gt;

&lt;p&gt;我们来讨论一下多项式次数$d$和过拟合以及欠拟合之间的关系。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;我们需要区分导致预测结果差的原因是偏差还是方差。&lt;/li&gt;
&lt;li&gt;高偏差也就是欠拟合，高方差也就是过拟合。我们需要在这两者之间找到一个黄金分割。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;随着多项式次数$d$的增加，训练集的误差会&lt;strong&gt;减少&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;同时，交叉验证集的误差会随着$d$的增加而&lt;strong&gt;减少&lt;/strong&gt;，但在$d$增加到某一点之后，会随着$d$的增加而&lt;strong&gt;增加&lt;/strong&gt;，形成一个凸曲线&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高偏差（欠拟合）：$J_{train}(\Theta)$和$J_{CV}(\Theta)$都较高，并且$J_{CV}(\Theta) \approx J_{train}(\Theta)$。&lt;/li&gt;
&lt;li&gt;高方差（过拟合）：$J_{train}(\Theta)$较低，且$J_{CV}(\Theta)$比$J_{train}(\Theta)$高得多。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以用下图来表示：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/300px-Features-and-polynom-degree.png&#34; alt=&#34;Features-and-polynom-degree&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;4-正则化和偏差-方差:876321dc83c64489eb74f98905ca718c&#34;&gt;4.正则化和偏差/方差&lt;/h1&gt;

&lt;p&gt;下面来分析正则化参数$\lambda$。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda$较大：高偏差（欠拟合）&lt;/li&gt;
&lt;li&gt;$\lambda$不大不小：正好&lt;/li&gt;
&lt;li&gt;$\lambda$较小：高方差（过拟合）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;较大的$\lambda$参数会惩罚$\Theta$参数，即简单化结果函数的曲线，造成欠拟合。&lt;/p&gt;

&lt;p&gt;$\lambda$和训练集以及验证集的关系如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda$较小：$J_{train}(\Theta)$较低，且$J_{CV}(\Theta)$较高（高方差/过拟合）。&lt;/li&gt;
&lt;li&gt;$\lambda$不大不小：$J_{train}(\Theta)$和$J_{CV}(\Theta)$都较低，并且$J_{CV}(\Theta) \approx J_{train}(\Theta)$。&lt;/li&gt;
&lt;li&gt;$\lambda$较大：$J_{train}(\Theta)$和$J_{CV}(\Theta)$都较高（高偏差/欠拟合）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图说明了$\lambda$值和假设之间的关系：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/300px-Features-and-lambda.png&#34; alt=&#34;Features-and-lambda&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;为了选择模型和正则化参数$lambda$，我们需要：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;列出$\lambda$测试的值，比如 $\lambda \in \lbrace0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24\rbrace$；&lt;/li&gt;
&lt;li&gt;选择一个$\lambda$的值进行计算；&lt;/li&gt;
&lt;li&gt;创建模型集，比如按照多项式次数或其他指标来创建；&lt;/li&gt;
&lt;li&gt;选择一个模型来学习$\Theta$值；&lt;/li&gt;
&lt;li&gt;用所选的模型学习得到$\Theta$值，使用选择的$\lambda$值计算$J_{train}(\Theta)$（为下一步学习参数$\Theta$）；&lt;/li&gt;
&lt;li&gt;使用学习（带$\lambda$）得到的参数$\Theta$计算不带正则项或是$\lambda=0$的训练误差$J_{train}(\Theta)$；&lt;/li&gt;
&lt;li&gt;使用学习（带$\lambda$）得到的参数$\Theta$计算不带正则项或是$\lambda=0$的交叉验证误差$J_{CV}(\Theta)$；&lt;/li&gt;
&lt;li&gt;对模型集合所有$\lambda$取值重复上述步骤，选择使交叉验证集误差最小的组合；&lt;/li&gt;
&lt;li&gt;如果需要使用图形化结果来帮助决策的话，可以绘制$\lambda$和$J_{train}(\Theta)$的图像，以及$\lambda$和$J_{CV}(\Theta)$的图像；&lt;/li&gt;
&lt;li&gt;使用最好的$\Theta$和$\lambda$组合，在测试集上进行预测计算$J_{test}(\Theta)$的值来验证模型对问题是否有好的泛化能力。&lt;/li&gt;
&lt;li&gt;为了帮助选择最好的多项式次数和$\lambda$的值，可以采用学习曲线来诊断。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;5-学习曲线:876321dc83c64489eb74f98905ca718c&#34;&gt;5.学习曲线&lt;/h1&gt;

&lt;p&gt;训练3个样本很容易得到0误差，因为我们永远可以找到一条二次曲线完全经过3个点。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当训练集越来越大时，二次函数的误差也会增加。&lt;/li&gt;
&lt;li&gt;误差值会在训练集大小m增加到一定程度后慢慢平缓。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;高偏差的情况&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;小训练集&lt;/strong&gt;：$J_{train}(\Theta)$较低，$J_{CV}(\Theta)较高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;大训练集&lt;/strong&gt;：$J_{train}(\Theta)$和$J_{CV}(\Theta)都较高，并且$J_{train}(\Theta) \approx J_{CV}(\Theta)$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果学习算法有高偏差的问题，那么获取更多的训练数据并不会有很多改进。&lt;/p&gt;

&lt;p&gt;对于高方差的问题，对于训练集大小有如下关系：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;高方差的情况&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;小训练集&lt;/strong&gt;：$J_{train}(\Theta)$较低，$J_{CV}(\Theta)较高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;大训练集&lt;/strong&gt;：$J_{train}(\Theta)$会略微增加，$J_{CV}(\Theta)会略微降低，并且$J_{train}(\Theta) &amp;lt; J_{CV}(\Theta)$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果学习算法有高方差的问题，那么获取更多的训练数据是有用的。&lt;/p&gt;

&lt;p&gt;下图展示了训练集大小和高偏差/高方差问题之间的关系。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/500px-High-variance-high-bias.png&#34; alt=&#34;High-variance-high-bias&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;6-再次考虑如何选择提升模型的下一步:876321dc83c64489eb74f98905ca718c&#34;&gt;6.再次考虑如何选择提升模型的下一步&lt;/h1&gt;

&lt;p&gt;决策过程可以分解成以下几点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获得更多的训练样本

&lt;ul&gt;
&lt;li&gt;解决高方差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;减少特征的数量

&lt;ul&gt;
&lt;li&gt;解决高方差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加特征的数量

&lt;ul&gt;
&lt;li&gt;解决高偏差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;使用多项式特征

&lt;ul&gt;
&lt;li&gt;解决高偏差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;增加正则参数$\lambda$

&lt;ul&gt;
&lt;li&gt;解决高偏差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;减少正则参数$\lambda$

&lt;ul&gt;
&lt;li&gt;解决高方差&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;7-诊断神经网络:876321dc83c64489eb74f98905ca718c&#34;&gt;7.诊断神经网络&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;参数较少的神经网络很容易欠拟合，但同时计算也较容易。&lt;/li&gt;
&lt;li&gt;参数较多的大型神经网络更容易过拟合，但同时计算量较大。在这种情况下可以使用正则化（增加$\lambda$）来避免过拟合问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用单个隐藏层是一个较好地开始默认设置。你可以使用验证集在多个隐藏层上训练神经网络。&lt;/p&gt;

&lt;h1 id=&#34;8-模型选择总结:876321dc83c64489eb74f98905ca718c&#34;&gt;8.模型选择总结&lt;/h1&gt;

&lt;p&gt;以下是机器学习诊断的一些总结&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;选择多项式次数M&lt;/li&gt;
&lt;li&gt;如何选择模型中得参数$\Theta$（即模型选择）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有3种方式解决：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;获取更多数据（非常困难）&lt;/li&gt;
&lt;li&gt;选择拟合数据最好且没有过拟合的模型（非常困难）&lt;/li&gt;
&lt;li&gt;通过正则化来减少过拟合的机会&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;偏差：近似误差（预测值和期望值之间的差值）&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高偏差 = 欠拟合（BU）&lt;/li&gt;
&lt;li&gt;$J_{train}(\Theta)$和$J_{CV}(\Theta)都较高，并且$J_{train}(\Theta) \approx J_{CV}(\Theta)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;方差：有限数据集之间的估计误差值&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高方差 = 过拟合（VO）&lt;/li&gt;
&lt;li&gt;$J_{train}(\Theta)$较低，并且$J_{train}(\Theta) &amp;lt;&amp;lt; J_{CV}(\Theta)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;偏差-方差权衡的直觉&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;复杂模型=&amp;gt;数据敏感=&amp;gt;受训练集X变化的影响=&amp;gt;高方差，低偏差&lt;/li&gt;
&lt;li&gt;简单模型=&amp;gt;更死板=&amp;gt;不受训练集X变化的影响=&amp;gt;低方差，高偏差&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;机器学习的最重要的目标之一：找到一个模型在偏差-方差的权衡之间刚刚好。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;正则化影响&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda$值较小（过拟合）使模型容易受噪声影响，导致高方差。&lt;/li&gt;
&lt;li&gt;$\lambda$值较大（欠拟合）会将参数值接近于0，导致高偏差。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;模型复杂度影响&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;多项式次数较低的模型（模型复杂度低）有高偏差和低方差。在这种情况下，模型拟合总是很差。&lt;/li&gt;
&lt;li&gt;多项式次数较高的模型（模型复杂度高）拟合训练集极好，拟合测试集极差。导致训练集上低偏差，但高方差。&lt;/li&gt;
&lt;li&gt;在现实中，我们想要选择一个模型在以上两种情况之间，既然可以很好地拟合数据，也有很好地泛化能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用诊断时的一些典型经验法则&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取更多地训练样本可以解决高方差问题，不能解决高偏差问题。&lt;/li&gt;
&lt;li&gt;减少特征数量可以解决高方差问题，不能解决高偏差问题。&lt;/li&gt;
&lt;li&gt;增加特征数量可以解决高偏差问题，不能解决高方差问题。&lt;/li&gt;
&lt;li&gt;增加多项式特征和交互特征（特征和特征交互）解决高偏差问题，不能解决高方差问题。&lt;/li&gt;
&lt;li&gt;当使用梯度下降时，减少正则化参数$\lambda$值可以解决高方差问题，增加$\lambda$值可以解决高偏差问题。&lt;/li&gt;
&lt;li&gt;当使用神经网络时，小型神经网络更容易欠拟合，大型神经网络更容易过拟合。交叉验证是选择神经网络大小的一种方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://class.coursera.org/ml/lecture/index&#34;&gt;https://class.coursera.org/ml/lecture/index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE555/Chap9.Part2.pdf&#34;&gt;http://www.cedar.buffalo.edu/~srihari/CSE555/Chap9.Part2.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.stephenpurpura.com/post/13052575854/managing-bias-variance-tradeoff-in-machine-learning&#34;&gt;http://blog.stephenpurpura.com/post/13052575854/managing-bias-variance-tradeoff-in-machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap3/Bias-Variance.pdf&#34;&gt;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap3/Bias-Variance.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记5 神经网络2 参数学习</title>
      <link>http://nanshu.wang/post/2015-03-26</link>
      <pubDate>Thu, 26 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-03-26</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;成本函数-cost-function:8735cce9d7fd270391f5ec199a69102a&#34;&gt;成本函数(Cost Function)&lt;/h1&gt;

&lt;p&gt;以下是我们会用到的一些变量：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$L$表示神经网络的层数&lt;/li&gt;
&lt;li&gt;$s_l$表示第$l$层的神经单元数(不包括偏差(bias)单元)&lt;/li&gt;
&lt;li&gt;$K$表示输出单元数(分类数)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当有多个输出类别时，采用$h_\Theta(x)_k$表示第$k$个输出的假设结果。&lt;/p&gt;

&lt;p&gt;神经网络的成本函数是logistic回归中的成本函数更普遍的一种形式。&lt;/p&gt;

&lt;p&gt;logistic回归中的成本函数为：&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)}\log(h_{\theta}) + (1-y^{(i)}) \log(1-h_{\theta}(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2$$
&lt;/div&gt;

&lt;p&gt;神经网络的成本函数稍微复杂一点：&lt;/p&gt;

&lt;div&gt;
$$\begin{gather*}
J(\Theta) = - \frac{1}{m} \left[ \sum_{i=1}^m \sum_{k=1}^K y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2
\end{gather*}$$
&lt;/div&gt;

&lt;p&gt;和logistic回归相比，第一个方括号内多了一层累加求和，即对多类输出节点的成本函数进行累加，会遍历所有的输出节点。&lt;/p&gt;

&lt;p&gt;正则化的部分则是将每一层的参数都考虑了进去(包括偏差单元相关的参数值)。和logistic回归一样，每一项都进行了平方。&lt;/p&gt;

&lt;p&gt;注意:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;双层累加中对于输出层所有单元的logistic回归的成本函数进行求和&lt;/li&gt;
&lt;li&gt;三层累加中则是对于整个神经网络中单独的$\Theta$参数的平方进行求和&lt;/li&gt;
&lt;li&gt;三层累加中的$i$和训练集中的$i$并不一样&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;向后传播-backpropagation-算法:8735cce9d7fd270391f5ec199a69102a&#34;&gt;向后传播(Backpropagation)算法&lt;/h1&gt;

&lt;p&gt;向后传播是神经网络中表示最小化成本函数的术语，就像logistic回归和线性回归中的梯度下降(gradient descent)一样。&lt;/p&gt;

&lt;p&gt;我们的目标是计算：&lt;/p&gt;

&lt;div&gt;
$$\min_\Theta J(\Theta)$$
&lt;/div&gt;

&lt;p&gt;即我们想最小化成本函数$J$，得到最优化参数$\Theta$。&lt;/p&gt;

&lt;p&gt;观察我们用于计算$J(\Theta)$偏导的等式：&lt;/p&gt;

&lt;div&gt;
$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)$$
&lt;/div&gt;

&lt;p&gt;在向后传播中我们需要对每个节点计算：&lt;/p&gt;

&lt;div&gt;
$$\delta_j^{(l)} \text{＝第} l \text{层的节点}j \text{的误差}$$
&lt;/div&gt;

&lt;p&gt;回忆一下，$a_j^{(i)}$表示第$l$层的节点$j$的激活结果。对于最后一层，误差的计算为：&lt;/p&gt;

&lt;div&gt;$$\large
\delta^{(L)} = a^{(L)} - y$$&lt;/div&gt;

&lt;p&gt;其中，$L$表示层数，$a^{(L)}$是最后一层激活单元的向量化表示。因此对于最后一层，误差的计算就是简单的真实结果和预测结果的差值。&lt;/p&gt;

&lt;p&gt;为了得到除了最后一层以外其他层的误差值，我们采用以下等式从右往左地向后计算：&lt;/p&gt;

&lt;div&gt;$$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ g&#39;(z^{(l)})$$&lt;/div&gt;

&lt;p&gt;也就是用$\Theta$去乘后一层的误差值，再对每一个元素乘上$g&amp;rsquo;$，也就是激活函数$g$输入值为$z^{(l)}$时的导数。&lt;/p&gt;

&lt;p&gt;$g&amp;rsquo;$也可以写成：&lt;/p&gt;

&lt;div&gt;$$g&#39;(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})$$&lt;/div&gt;

&lt;p&gt;那么，对于内部节点来说，完整的向后传播计算等式为：&lt;/p&gt;

&lt;div&gt;$$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})$$&lt;/div&gt;

&lt;p&gt;其中，证明比较复杂，不过实现向后传播的算法也不用知道证明的细节。&lt;/p&gt;

&lt;p&gt;对于每个训练样本$t$，可以采用激活值和误差值来计算偏导项：&lt;/p&gt;

&lt;div&gt;$$\dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}} = \frac{1}{m}\sum_{t=1}^m a_j^{(t)(l)} {\delta}_i^{(t)(l+1)}$$&lt;/div&gt;

&lt;p&gt;上述式子忽略了正则项，后面会再详细说明。&lt;/p&gt;

&lt;p&gt;注意，其中$\delta^{l+1}$和$a^{l+1}$都是有$s_{l+1}$个元素的向量。类似地，$a^{(l)}$是有$s_l$个元素的向量。这两个向量相乘会产生和$\Theta^{(l)}$同样维度的$s_{l+1} * s_l$的矩阵。这个计算过程也就是得到了$\Theta^{(l)}$中每个元素的梯度项。&lt;/p&gt;

&lt;p&gt;现在，将以上过程串起来，我们得到整个向后传播算法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;给定训练集$\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$&lt;/li&gt;
&lt;li&gt;设$\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace \forall (l,i,j)$&lt;/li&gt;
&lt;li&gt;对于每个训练集 $\text{for } t = 1 \text{ to } m$:

&lt;ul&gt;
&lt;li&gt;$a^{(1)} := x^{(t)}$&lt;/li&gt;
&lt;li&gt;采用向前传播算法计算$a^{(l)}, l = 2,3,&amp;hellip;,l$&lt;/li&gt;
&lt;li&gt;计算$\delta^{(L)} = a^{(L)} - y^{(t)}$&lt;/li&gt;
&lt;li&gt;计算$\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$
&lt;div&gt;$$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)}) .* a^{(l)} .* (1 - a^{(l)})$$&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;$\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$，向量运算表示为：&lt;div&gt;$$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$$&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;$D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$s&lt;/li&gt;
&lt;li&gt;$D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$D^{(l)}_{i,j}$即最后我们所求的偏导。&lt;/p&gt;

&lt;h1 id=&#34;向后传播的直观理解:8735cce9d7fd270391f5ec199a69102a&#34;&gt;向后传播的直观理解&lt;/h1&gt;

&lt;p&gt;成本函数为：&lt;/p&gt;

&lt;div&gt;
$$
\begin{gather*}
J(\theta) = - \frac{1}{m} \left[ \sum_{t=1}^m \sum_{k=1}^K y^{(t)}_k \ \log (h_\theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \theta_{j,i}^{(l)})^2
\end{gather*}
$$
&lt;/div&gt;

&lt;p&gt;如果我们只考虑一个训练样本$(t=1)$，并且忽略正则项，那么成本函数为：&lt;/p&gt;

&lt;div&gt;
$$cost(t) =y^{(t)} \ \log (h_\theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\theta(x^{(t)}))$$
&lt;/div&gt;

&lt;p&gt;直观上可以将以上的等式近似看作：&lt;/p&gt;

&lt;div&gt;$$cost(t) \approx (h_\theta(x^{(t)})-y^{(t)})^2$$&lt;/div&gt;

&lt;p&gt;$\delta_j^{(l)}$是$a_j^{(l)}$的误差。&lt;/p&gt;

&lt;p&gt;严格来说，$\delta$值实际上是成本函数的偏导：&lt;/p&gt;

&lt;div&gt;$$\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)}} cost(t)$$&lt;/div&gt;

&lt;p&gt;注意偏导是成本函数切线的斜率，斜率越陡峭越不准确。&lt;/p&gt;

&lt;h1 id=&#34;梯度检查-gradient-checking:8735cce9d7fd270391f5ec199a69102a&#34;&gt;梯度检查(Gradient Checking)&lt;/h1&gt;

&lt;p&gt;梯度检查能够保证我们的向后传播算法的正确性。&lt;/p&gt;

&lt;p&gt;我们可以用以下方法近似地估算成本函数的偏导：&lt;/p&gt;

&lt;div&gt;$$\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$$&lt;/div&gt;

&lt;p&gt;$\Theta$是很多个矩阵构成，可以对$\Theta_j$的偏导进行以下估算：&lt;/p&gt;

&lt;div&gt;$$\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$$&lt;/div&gt;

&lt;p&gt;$\epsilon$的取值必须较小，才能得到接近真实的值。但如果$\epsilon$的值太小，计算可能会有精度问题而无法得到正确值。Andrew给的经验值时$\epsilon = 10^{-4}$。&lt;/p&gt;

&lt;p&gt;如果可以验证梯度近似值和$\delta$大致相等，那么向后传播算法是正确的。在真正计算中不需要计算梯度近似值，因为其计算非常慢。&lt;/p&gt;

&lt;h1 id=&#34;随机初始化:8735cce9d7fd270391f5ec199a69102a&#34;&gt;随机初始化&lt;/h1&gt;

&lt;p&gt;在神经网络中，不能将$\theta$权重初始化为0，否则在向后传播时，所有的节点会重复更新一样的值。&lt;/p&gt;

&lt;p&gt;因此需要随机初始化权重：&lt;/p&gt;

&lt;p&gt;将$\Theta_{ij}^{(l)}$赋为$[-\epsilon,\epsilon]$范围内的一个随机数：&lt;/p&gt;

&lt;div&gt;$$\epsilon = \dfrac{\sqrt{6}}{\sqrt{\mathrm{Loutput} + \mathrm{Linput}}}$$&lt;/div&gt;

&lt;div&gt;$$\Theta^{(l)} =  2 \epsilon \; \mathrm{rand}(\mathrm{Loutput}, \mathrm{Linput} + 1)    - \epsilon$$&lt;/div&gt;

&lt;p&gt;其中，$Loutput$和$Linput+1$是$\Theta$参数的维度。这里的$\epsilon$和梯度检查中的$\epsilon$没有关系。&lt;/p&gt;

&lt;h1 id=&#34;神经网络总结:8735cce9d7fd270391f5ec199a69102a&#34;&gt;神经网络总结&lt;/h1&gt;

&lt;p&gt;首先，选择神经网络的结构，确定神经网络的层数和每层的隐藏单元的个数。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;输入层单元数等于特征$x^(i)$的维度&lt;/li&gt;
&lt;li&gt;输出层的单元数等于分类个数&lt;/li&gt;
&lt;li&gt;默认设置：1个隐藏层。如果有多于1个隐藏层，则隐藏层的单元个数都一样多。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;训练神经网络&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;随机初始化权重&lt;/li&gt;
&lt;li&gt;实现向前传播计算$h_{\theta}(x^{(i)})$&lt;/li&gt;
&lt;li&gt;实现成本函数&lt;/li&gt;
&lt;li&gt;实现向后传播计算偏导值&lt;/li&gt;
&lt;li&gt;采用梯度检查来确认向后传播的正确性后，再取消梯度检查。&lt;/li&gt;
&lt;li&gt;采用梯度下降或其他已有的最优化函数来最小化成本函数，得到$\theta$权重值。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于每个训练样本都循环采用向前和向后传播算法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i = 1:m,
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>python numpy 计算自相关系数</title>
      <link>http://nanshu.wang/post/2015-03-15</link>
      <pubDate>Sun, 15 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-03-15</guid>
      <description>&lt;p&gt;在分析时间序列时，通常需要计算一个序列的自相关系数。自相关(&lt;a href=&#34;http://en.wikipedia.org/wiki/Autocorrelation&#34;&gt;Autocorrelation&lt;/a&gt;)又叫做序列相关，通常采用自相关系数来发现序列的重复规律，周期等信息。&lt;/p&gt;

&lt;p&gt;我们有序列$X:x_1,x_2,x_3,&amp;hellip;,x_n$，设$X_{s,t}$为$s$时刻开始，$t$时刻结束的序列：$x_s,x_{s+1}&amp;hellip;,x_{t-1},x_t$。$\mu_{s,t}$为序列$X_{s,t}$的均值，$\sigma_{s,t}$为序列$X_{s,t}$的标准差。那么一阶自相关系数为：&lt;/p&gt;

&lt;div&gt;

$$R(1) = \frac{E(X_{2,n}-\mu_{2,n})(X_{1,n-1}-\mu_{1,n-1})}{\sigma_{2,n}\sigma_{1,n-1}}$$

&lt;/div&gt;

&lt;p&gt;同理$k$阶自相关系数为：&lt;/p&gt;

&lt;div&gt;

$$R(k) = \frac{E(X_{k+1,n}-\mu_{k+1,n})(X_{1,n-k}-\mu_{1,n-k})}{\sigma_{k+1,n}\sigma_{1,n-k}}$$

&lt;/div&gt;

&lt;p&gt;python的numpy库里没有直接计算序列自相关系数的函数，但有计算两个不同序列的相关系数函数： &lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html&#34;&gt;correlate&lt;/a&gt;。给定两个序列$X,Y$，correlation(X,Y) = $\sum XY$。可以利用correlate函数计算$X$的自相关性：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autocorrelation(x,lags):#计算lags阶以内的自相关系数，返回lags个值，分别计算序列均值，标准差
	n = len(x)
	x = numpy.array(x)
	result = [numpy.correlate(x[i:]-x[i:].mean(),x[:n-i]-x[:n-i].mean())[0]\
		/(x[i:].std()*x[:n-i].std()*(n-i)) \
		for i in range(1,lags+1)]
	return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常在实际中，很多时间序列的均值和标准受时间变化的影响较小，可以看作是恒定的，此时：&lt;/p&gt;

&lt;div&gt;

$$R(k) = \frac{E(X_{k+1,n}-\mu)(X_{1,n-k}-\mu)}{\sigma^2}$$

&lt;/div&gt;

&lt;p&gt;同样可以利用correlate函数实现：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autocorrelation(x,lags):#计算lags阶以内的自相关系数，返回lags个值，将序列均值、标准差视为不变
	n = len(x)
	x = numpy.array(x)
	variance = x.var()
	x = x-x.mean()
	result = numpy.correlate(x, x, mode = &#39;full&#39;)[-n+1:-n+lags+1]/\
		(variance*(numpy.arange(n-1,n-1-lags,-1)))
	return result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考：&lt;a href=&#34;http://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation&#34;&gt;http://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记5 神经网络1 模型表达</title>
      <link>http://nanshu.wang/post/2015-03-03-2</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-03-03-2</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;神经网络:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经网络&lt;/h1&gt;

&lt;h2 id=&#34;非线性假设:d35c8128725ed53542064246ed42d2c0&#34;&gt;非线性假设&lt;/h2&gt;

&lt;p&gt;在特征变量数较大的情况下，采用线性回归会很难处理，比如我的数据集有3个特征变量，想要在假设中引入所有特征变量的平方项：&lt;/p&gt;

&lt;div&gt;
$$g(\theta_0 + \theta_1x_1^2 + \theta_2x_1x_2 + \theta_3x_1x_3  + \theta_4x_2^2 + \theta_5x_2x_3  + \theta_6x_3^2 )$$
&lt;/div&gt;

&lt;p&gt;共有6个特征，假设我们想知道选取其中任意两个可重复的平方项有多少组合，采用允许重复的组合公式计算$\frac{(n+r-1)!}{r!(n-1)!}$，共有$\frac{(3 + 2 - 1)!}{(2!\cdot (3-1)!)} = 6$种特征变量的组合。对于100个特征变量，则共有$\frac{(100 + 2 - 1)!}{(2\cdot (100-1)!)} = 5050$个新的特征变量。&lt;/p&gt;

&lt;p&gt;可以大致估计特征变量的平方项组合个数的增长速度为$\mathcal{O}(\frac{n^2}2)$，立方项的组合个数的增长为$\mathcal{O}(n^3)$。这些增长都十分陡峭，让实际问题变得很棘手。&lt;/p&gt;

&lt;p&gt;在变量假设十分复杂的情况下，神经网络提供了另一种机器学习算法。&lt;/p&gt;

&lt;h1 id=&#34;神经元和大脑:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经元和大脑&lt;/h1&gt;

&lt;p&gt;神经网络是对大脑工作方式的一种简单模拟。有证据表明，大脑对所有的功能（如视觉，触觉，听觉等）都采用了一种“学习算法”。将听觉皮层和视觉神经连接到一起，听觉皮层可以学会“看见”。这种理论叫作“neuroplasticity”，已经得到了很多例子和实验验证。&lt;/p&gt;

&lt;h1 id=&#34;模型表达:d35c8128725ed53542064246ed42d2c0&#34;&gt;模型表达&lt;/h1&gt;

&lt;p&gt;简单来说，每个神经元都有输入（树突dendrites）和输出（轴突axons）。在模型中，输入就是我们的特征变量，输出就是模型假设的结果。&lt;/p&gt;

&lt;p&gt;在神经网络中，分类问题通常采用logistic函数，也叫做sigmoid激活函数(sigmoid activation function)。$\theta$参数有时也被称为权重(weights)。&lt;/p&gt;

&lt;p&gt;下面是一种简单的神经网络：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/neuron_model.jpg&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;第一层是输入节点(nodes)，第二层是输出节点，也就是我们假设函数的结果$h_{\theta}(x)$。&lt;/p&gt;

&lt;p&gt;第一层叫作“输入层”(input layer)，最后一层叫作“输出层”(output layer)，输入层和输出层之间还可以有多层，统称为“隐藏层”(hidden layer)。如下图中，第二层就叫隐藏层。隐藏层节点表示为$a^2_0 \cdots a^2_n$，被称作“激活单元(activation units)”。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/neuron_network_3layers.jpg&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;$a_i^{(j)}$表示第$j$层的$i$单元被“激活”，$\Theta^{(j)}$表示从第$j$层到第$j+1$层的权重矩阵。&lt;/p&gt;

&lt;p&gt;上图的神经网络中，激活单元的计算分别为：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline
a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline
h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;每一层都有自己的权重矩阵$\Theta^{(j)}$，如果第$j$层有 $s_j$ 个单元，第$j+1$层有$s_{j+1}$个单元，则$\Theta^{(j)}$是$s_{j+1} \times (s_j+1)$的矩阵。&amp;rdquo;+1&amp;rdquo;是因为第$j$层包括一个&amp;rdquo;偏差节点(bias nodes)“，$x_0$和$\Theta_0^{(j)}$。换句话说，输出节点不包括偏差节点，但输入节点会包括偏差节点。&lt;/p&gt;

&lt;p&gt;举个例子，第一层有2个输入节点，第二层有4个激活单元。$\Theta^{(1)}$的维度为$4 \times 3$。&lt;/p&gt;

&lt;p&gt;下面将以上模型表达向量化。&lt;/p&gt;

&lt;p&gt;采用$z^{(i)}_k$表示$g$函数的输入，那么有：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
a_1^{(2)} = g(z_1^{(2)}) \newline
a_2^{(2)} = g(z_2^{(2)}) \newline
a_3^{(2)} = g(z_3^{(2)}) \newline
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;给定第$j$层的节点$k$，变量$z$等于：&lt;/p&gt;

&lt;div&gt;
$$
z_k^{(j)} = \Theta_{k,0}^{(j-1)}x_0 + \Theta_{k,1}^{(j-1)}x_1 + \cdots + \Theta_{k,n}^{(j-1)}x_n 
$$
&lt;div&gt;

$x$和$z^{(j)}$的向量表示为：

&lt;div&gt;
$$
\begin{align*}
x = 
\begin{bmatrix}
x_0 \newline
x_1 \newline
\cdots \newline
x_n
\end{bmatrix} &amp;
z^{(j)} = 
\begin{bmatrix}
z_1^{(j)} \newline
z_2^{(j)} \newline
\cdots \newline
z_n^{(j)}
\end{bmatrix}
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;因此，$z^{(j}) = \Theta^{(j-1)} x$&lt;/p&gt;

&lt;p&gt;令$x = a^{(j-1)}$，则$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$&lt;/p&gt;

&lt;p&gt;最后的结果为&lt;/p&gt;

&lt;div&gt;
$$
h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) = g(\Theta^{(j)}a^{(j)})
$$
&lt;/div&gt;

&lt;h1 id=&#34;神经网络拟合逻辑运算:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经网络拟合逻辑运算&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ AND\ x_2$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;神经网络模型：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
g(z^{(2)})
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;其中，$x_0$为偏差，恒等于1。&lt;/p&gt;

&lt;p&gt;权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
-30 &amp; 20 &amp; 20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;仅当$x_1$和$x_2$同时为1时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \newline
\newline
&amp; x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline
&amp; x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline
&amp; x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline
&amp; x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1
\end{align*}
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ NOR\ x_2$, $NOR$为$NOT\ OR$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;模型同上，权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
10 &amp; -20 &amp; -20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;仅当$x_1$和$x_2$同时为0时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ OR\ x_2$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;模型同上，权重参数为：&lt;/p&gt;

&lt;div&gt;
$\Theta^{(1)} = 
\begin{bmatrix}
-10 &amp; 20 &amp; 20
\end{bmatrix}$
&lt;/div&gt;

&lt;p&gt;当$x_1$和$x_2$不同时为0时，$h_{\Theta}(x) = 1$。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$x_1\ XNOR\ x_2$，$XNOR$为$NOT\ XOR$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将前面得到的模型组合起来$x_1\ XNOR\ x_2 = (x_1\ AND\ x_2)\ OR\ (x_1\ XOR\ x_2)$&lt;/p&gt;

&lt;p&gt;模型如下：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_1^{(2)} \newline
a_2^{(2)} 
\end{bmatrix} \rightarrow
\begin{bmatrix}
a^{(3)}
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;第一层到第二层的$\Theta^{(1)}$分别表示$AND$和$NOR$操作：&lt;/p&gt;

&lt;div&gt;
$$
\Theta^{(1)} = 
\begin{bmatrix}
-30 &amp; 20 &amp; 20 \newline
10 &amp; -20 &amp; -20
\end{bmatrix}
$$
&lt;/div&gt;  

&lt;p&gt;第二层到第三层的$\Theta^{(2)}$表示$OR$操作：&lt;/p&gt;

&lt;div&gt;
$$
\Theta^{(2)} = 
\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}
$$
&lt;/div&gt;

&lt;h1 id=&#34;多类分类问题:d35c8128725ed53542064246ed42d2c0&#34;&gt;多类分类问题&lt;/h1&gt;

&lt;p&gt;输出用向量来表示多类分类问题：&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2 \newline
\cdots \newline
x_n
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(2)} \newline
a_1^{(2)} \newline
a_2^{(2)} \newline
\cdots
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(3)} \newline
a_1^{(3)} \newline
a_2^{(3)} \newline
\cdots
\end{bmatrix} \rightarrow \cdots \rightarrow
\begin{bmatrix}
h_\Theta(x)_1 \newline
h_\Theta(x)_2 \newline
h_\Theta(x)_3 \newline
h_\Theta(x)_4 \newline
\end{bmatrix} \rightarrow
\end{align*}
$$
&lt;/div&gt;

&lt;p&gt;最后的结果：&lt;/p&gt;

&lt;div&gt;
$$h_\Theta(x) = 
\begin{bmatrix}
0 \newline
0 \newline
1 \newline
0 \newline
\end{bmatrix}$$
&lt;/div&gt;

&lt;p&gt;表示该样本属于第三类，$h_{\Theta}(x)_3$。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记4 正则化</title>
      <link>http://nanshu.wang/post/2015-02-17</link>
      <pubDate>Tue, 17 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-17</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;正则化-regularization:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化 Regularization&lt;/h1&gt;

&lt;p&gt;为了和正规方程(normal equation)里&amp;rdquo;正规&amp;rdquo;区分开来，这里Regularization都译作“正则化”，有些地方也用的是“正规化”。以下内容来自&lt;a href=&#34;http://en.wikipedia.org/w/index.php?title=Regularization_(mathematics&#34;&gt;wikipedia&lt;/a&gt;)：&lt;/p&gt;

&lt;p&gt;正则化是指通过引入额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。正则化的一种理论解释是它试图引入&lt;a href=&#34;http://en.wikipedia.org/wiki/Occam%27s_razor&#34;&gt;奥卡姆剃刀原则&lt;/a&gt;。而从贝叶斯的观点来看，正则化则是在模型参数上引入了某种先验的分布。&lt;/p&gt;

&lt;p&gt;机器学习中最常见的正则化是$L_1$和$L_2$正则化。正则化是在学习算法的损失(成本)函数$E(X,Y)$的基础上在加上一项正则化参数项：$E(X,Y)+\alpha|w|$，其中$w$是参数向量，$\alpha$是正则项的参数值，需要在实际训练中调整。正则化在许多模型中都适用，对于线性回归模型来说，采用$L_1$正则化的模型叫作lasso回归，采用$L_2$的叫作ridge回归。对于logistic回归，神经网络，支持向量机，随机条件场和一些矩阵分解方法，正则化也适用。在神经网络中，$L_2$正则化又叫作“权重衰减”(weight decay)。$L_1$正则化能产生稀疏模型，因此在特征选择中很有用，但是$L_1$范式不可微，所以需要在学习算法中修改，特别是基于梯度下降的算法。&lt;/p&gt;

&lt;h1 id=&#34;过拟合问题:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;过拟合问题&lt;/h1&gt;

&lt;p&gt;欠拟合(也叫做高偏差(high bias))是指不能很好地拟合数据，一般是因为模型函数太简单或者特征较少。过拟合问题是指过于完美拟合了训练集数据，而对新的样本失去了一般性，不能有效预测新样本，这个问题也叫做高方差(high variances)。造成过拟合的原因可能是特征量太多或者模型函数过于复杂。线性回归和logistic回归都存在欠拟合和过拟合的问题。&lt;/p&gt;

&lt;p&gt;要解决过拟合的问题，通常有两种方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;减少特征数量&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;手动筛选特征&lt;/li&gt;
&lt;li&gt;采用特征筛选算法&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正则化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保留所有的特征，但尽可能使参数$\theta_j$尽量小。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;正则化在很多特征变量对目标值只有很小影响的情况下非常有用。&lt;/p&gt;

&lt;h1 id=&#34;成本函数:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;成本函数&lt;/h1&gt;

&lt;p&gt;在原有的成本函数的基础上加上使参数$\theta_j$正则化的项：&lt;/p&gt;

&lt;div&gt;
$$\min \frac1{2m}(\sum_{i=1}^m (h_{\theta}(x^{(i)}-y^{(i)}) + \lambda \sum_{j=1}^n \theta_j^2))$$
&lt;/div&gt;

&lt;p&gt;其中$\lambda$叫做正则化参数，决定了参数正则化项的影响大小。引入正则化参数项，可以避免模型过拟合的问题。如果$\lambda$的值设置过大，可能会使模型函数出现欠拟合的问题。&lt;/p&gt;

&lt;h1 id=&#34;正则化线性回归:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化线性回归&lt;/h1&gt;

&lt;h2 id=&#34;梯度下降法:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;梯度下降法&lt;/h2&gt;

&lt;p&gt;常数项$\theta_0$不用正则化，因此更新策略为：&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; \text{Repeat}\ \lbrace \newline
&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
&amp; \rbrace
\end{align*}
&lt;/div&gt;

&lt;p&gt;上面的式子可以写成：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j(1 - \alpha \frac{\lambda}m)-\alpha (\frac1m \sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}) $$
&lt;/div&gt;

&lt;p&gt;注意到$(1 - \alpha \frac{\lambda}m) &amp;lt; 1$，直观上可以理解为将式子中第一项$\theta_j$值减小了一点，第二项还是和无正则化的更新策略的第二项一致。&lt;/p&gt;

&lt;h2 id=&#34;正规方程:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正规方程&lt;/h2&gt;

&lt;p&gt;正则化正规方程求解$\theta$为：&lt;/p&gt;

&lt;div&gt;
\begin{align*}
&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline
&amp; \text{where}\ \ L = 
\begin{bmatrix}
 0 &amp; &amp; &amp; &amp; \newline
 &amp; 1 &amp; &amp; &amp; \newline
 &amp; &amp; 1 &amp; &amp; \newline
 &amp; &amp; &amp; \ddots &amp; \newline
 &amp; &amp; &amp; &amp; 1 \newline
\end{bmatrix}
\end{align*}
&lt;/div&gt;

&lt;p&gt;$L$是(n+1)*(n+1)的矩阵，除了右上角的值为0外，对角线上其他值都为1。直观上理解，不包括右上角$X_0$项，$L$是一个单位矩阵。&lt;/p&gt;

&lt;p&gt;当$m\leq n$时，$X^TX$不可逆，但加入$\lambda L$后，$X^TX$也变得可逆了。&lt;/p&gt;

&lt;h1 id=&#34;正则化logistic回归:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化logistic回归&lt;/h1&gt;

&lt;h2 id=&#34;成本函数-1:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;成本函数&lt;/h2&gt;

&lt;p&gt;加上正则化参数$\theta$项：&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$
&lt;/div&gt;

&lt;p&gt;注意$\sum_{j=1}^n \theta_j^2$中参数的下标时从1到n，没有包括常数项$\theta_0$&lt;/p&gt;

&lt;h2 id=&#34;梯度下降:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;梯度下降&lt;/h2&gt;

&lt;p&gt;和线性回归一样，$\theta_0$和其他参数要分开更新：&lt;/p&gt;

&lt;div&gt;
    $$\begin{align*}
&amp; \text{Repeat}\ \lbrace \newline
&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
&amp; \rbrace
\end{align*}$$
&lt;/div&gt;

&lt;p&gt;这和正则化线性回归的更新策略是一样的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记3 有监督学习 分类 logistic回归</title>
      <link>http://nanshu.wang/post/2015-02-12</link>
      <pubDate>Thu, 12 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-12</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;分类问题:662363920bb15b8d466d06a6774c21df&#34;&gt;分类问题&lt;/h1&gt;

&lt;p&gt;分类问题和回归问题不同的是，分类问题的预测值$y$只能取离散值，而非连续值。首先来看一个二类分类问题，预测值$y$只能取0或1。0又被称作负例(negative class)，1被称作正例(positive class)。通常也用&amp;rdquo;-&amp;ldquo;,&amp;rdquo;+&amp;ldquo;符号来表示。对于一个样本集输入$x^{(i)}$，对应的目标值$y^{(i)}$也被为标注(lable)。&lt;/p&gt;

&lt;h2 id=&#34;logistic回归:662363920bb15b8d466d06a6774c21df&#34;&gt;logistic回归&lt;/h2&gt;

&lt;p&gt;也可以用线性回归的方法运用到分类问题上，但是这样做很容易得到不好的结果。稍微改变一下我们的假设函数$h_\theta(x)$，使其的取值在{0,1}范围内：&lt;/p&gt;

&lt;div&gt;
$$h_\theta(x) = g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$$
$$g(z)=\frac1{1+e^{-z}}$$
&lt;/div&gt;

&lt;p&gt;$g(z)$叫做logistic函数，也叫做sigmoid函数。$g(z)$的函数图像如下：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/logistic-function.png&#34; alt=&#34;logistic-function&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;当$z\rightarrow \infty$时，$g(z)$趋近于1；当$z\rightarrow -\infty$时，$g(z)$趋近于0。因此$h(x)$的取值在0到1范围内。&lt;/p&gt;

&lt;p&gt;求$g(z)$的导数可得：&lt;/p&gt;

&lt;div&gt;
$$g&#39;(z)=g(z)(1-g(z))$$
&lt;/div&gt;

&lt;p&gt;下面是对分类问题作出的一些假设，预测函数$h_\theta(x)$将给出样本目标值分类为1的概率：&lt;/p&gt;

&lt;div&gt;
$$P(y=1|x;\theta) = h_{\theta}(x)$$
$$P(y=0|x;\theta) = 1-h_{\theta}(x)$$
$$p(y|x;\theta) = (h_{\theta}(x)^y(1-h_{\theta}(x)^{1-y}))$$
&lt;/div&gt;

&lt;p&gt;那么$\theta$的似然函数为：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
L(\theta) =&amp; p(\overrightarrow y|X;\theta)\\
=&amp; \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=&amp; \Pi_{i=1}^m (h_\theta(x^{(i)})^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}})
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;求log似然函数:&lt;/p&gt;

&lt;div&gt;
$$l(\theta)=\log L(\theta)=\sum_{i=1}^m y^{(i)}\log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))$$
&lt;/div&gt;

&lt;p&gt;求最大似然估计，同样可以采用梯度下降的方法，更新$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta+\alpha \nabla_\theta l(\theta)$$
&lt;/div&gt;

&lt;p&gt;这里是求最大值，因此更新$\theta$是加上$l(\theta)$的偏导。&lt;/p&gt;

&lt;p&gt;解之得到：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j+\alpha (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$$
&lt;/div&gt;

&lt;p&gt;这和之前在线性回归模型得到的LMS更新策略一样，这并不是巧合，而是因为线性回归和logistic回归都属于广义线性模型(GLM models)。&lt;/p&gt;

&lt;h2 id=&#34;perceptron学习算法:662363920bb15b8d466d06a6774c21df&#34;&gt;perceptron学习算法&lt;/h2&gt;

&lt;p&gt;有趣的是，如果这里不采用logistic函数，而是采用一种简单粗暴的只考虑阈值的函数g(z)：&lt;/p&gt;

&lt;div&gt;
$$g(z) = \begin{cases}1,if z\geq 0\\0,if z &lt; 0\end{cases}$$
&lt;/div&gt;

&lt;p&gt;我们得到的更新$\theta$的策略和采用logistic函数得到的策略是一致。这种算法叫做感知器(perceptron)学习算法，感知器原指一种用来刻画大脑神经元的粗糙模型。虽然表面上看这种简单粗暴的方式和其他算法得到的结果是一样的，但是这是一种和logistic回归以及最小二乘线性回归非常不同的一类算法，它不能推导出有意义的概率解释，也不能通过极大似然估计得到。&lt;/p&gt;

&lt;h2 id=&#34;牛顿法:662363920bb15b8d466d06a6774c21df&#34;&gt;牛顿法&lt;/h2&gt;

&lt;p&gt;为了求$f(\theta)=0$时$\theta$的取值，牛顿法每次更新$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-\frac{f(\theta)}{f&#39;(\theta)}$$
&lt;/div&gt;

&lt;p&gt;要最大化似然函数$l(\theta)$的值，使其导数$l&amp;rsquo;(\theta)＝0$。更新策略为：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-\frac{l&#39;(\theta)}{l&#39;&#39;(\theta)}$$
&lt;/div&gt;

&lt;p&gt;当$\theta$为向量时，推广更一般的牛顿法，这种方法也叫做牛顿－拉普森法(Newton-Raphson method)：&lt;/p&gt;

&lt;div&gt;
$$\theta:=\theta-H^{-1} \nabla_\theta l(\theta)$$
&lt;/div&gt;

&lt;p&gt;$\nabla_\theta l(\theta)$是$l(\theta)$对于$\theta$的偏导。$H$是$(n+1)*(n+1)$的矩阵，叫做Hessian：&lt;/p&gt;

&lt;div&gt;
$$H_{ij}=\frac{\delta^2 l(\theta)}{\delta \theta_i \delta \theta_j}$$
&lt;/div&gt;

&lt;p&gt;牛顿法收敛的速度通常比批量梯度下降要快，但是牛顿法每次迭代的计算量更大，每次迭代重新计算Hessian矩阵，需要$O(n^2)$的时间复杂度。但在n没有很大的情况下，牛顿法是更有效率的。将牛顿法用于logistic回归的log似然函数$l(\theta)$得到的方法也被称为Fisher scoring。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释</title>
      <link>http://nanshu.wang/post/2015-02-11</link>
      <pubDate>Wed, 11 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-11</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:a41c550b5852f048f913986db76cdf89&#34;&gt;有监督学习&lt;/h1&gt;

&lt;h2 id=&#34;局部加权线性回归-locally-weighted-linear-regression:a41c550b5852f048f913986db76cdf89&#34;&gt;局部加权线性回归(Locally weighted linear regression)&lt;/h2&gt;

&lt;p&gt;参数学习算法(parametric learning algorithm)：参数个数固定&lt;/p&gt;

&lt;p&gt;非参数学习算法(non-parametric learning algorithm)：参数个数随样本增加&lt;/p&gt;

&lt;p&gt;特征选择对参数学习算法非常重要，否则会出现下面的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欠拟合(underfitting)：特征过少，模型过于简单，高偏差(high bias)，不能很好拟合训练集&lt;/li&gt;
&lt;li&gt;过拟合(overfitting)：特征过多，模型过于复杂，高方差(high variance)，过于拟合训练集，不能很好预测新样本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于非参数学习算法来说，并不需要进行精心的特征选择，局部加权线性回归就是这样。&lt;/p&gt;

&lt;p&gt;局部加权回归又叫做Loess，其成本函数为：&lt;/p&gt;

&lt;div&gt;
$$\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$$
&lt;/div&gt;

&lt;p&gt;$w^{(i)}$是非负权重，通常定义$w^{(i)}$如下：&lt;/p&gt;

&lt;div&gt;
$$w^{(i)}=exp(- \frac{(x^{(i)}-x)^2}{2\tau^2})$$
&lt;/div&gt;

&lt;p&gt;权值取决于用于预测的输入变量$x$的值：离$x$越近的样本，权值越接近1；越远的样本，越接近0。$\tau$被称为带宽(bandwidth)参数，决定了以$x$为中心，样本权重递减的速度。$\tau$越大，递减速度越慢；$\tau$越小，递减速度越快。&lt;/p&gt;

&lt;p&gt;参数学习算法的参数是固定的，一经学习得到不再改变。而非参数学习算法的参数并不固定，每次预测都要重新学习一组新的参数，并且要一直保留完整的训练样本集。当样本集很大时，局部加权回归的计算开销会很大，Andrew Moore提出的KD-tree方法可以在大数据集上的计算更高效。&lt;/p&gt;

&lt;h2 id=&#34;回归模型的概率解释:a41c550b5852f048f913986db76cdf89&#34;&gt;回归模型的概率解释&lt;/h2&gt;

&lt;p&gt;遇到一个回归问题，为什么采用线性回归，又为什么采用最小二乘作为成本函数？其实最小二乘回归中蕴含了非常自然的概率假设。假设目标值和输入值之间满足以下关系：&lt;/p&gt;

&lt;div&gt;
$$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$$
&lt;/div&gt;

&lt;p&gt;其中，$\epsilon^{(i)}$是误差项，包含了模型未考虑到的影响目标值的因素和随机噪声。假设误差项独立同分布，服从均值为0，方差为$\sigma^2$的高斯分布（正态分布），即$\epsilon^{(i)}\sim N(0,\sigma^2)$。$\epsilon^{(i)}$的密度函数为：&lt;/p&gt;

&lt;div&gt;
$$p(\epsilon^{(i)})=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$
&lt;/div&gt;

&lt;p&gt;也就是说：&lt;/p&gt;

&lt;div&gt;
$$p(y^{(i)}|x^{(i)};\theta)=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
&lt;/div&gt;

&lt;p&gt;或者也可以这样写：$(y^{(i)}|x^{(i)};\theta) \sim N(\theta^Tx^{(i)},\sigma^2)$。&lt;/p&gt;

&lt;p&gt;为什么采用正态分布？一种原因是数学上处理起来比较便利，二是因为根据中心极限定理，独立的随机变量的和，即多种随机误差的累积，其总的影响是接近正态分布的。&lt;/p&gt;

&lt;p&gt;$p(y^{(i)}|x^{(i)};\theta)$的含义是给定条件$x^{(i)}$，参数设定为$\theta$时，$y^{(i)}$的分布值。不能将$\theta$看作是概率条件，因为$\theta$不是随机变量，而是实际存在的真实值，虽然我们不知道真实值到底是多少。因此在以上的式子中，用了分号而不是逗号来区分$x^{(i)}$和$\theta$。&lt;/p&gt;

&lt;p&gt;定义$\theta$的似然(likelihood)函数：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
L(\theta)=&amp; p(y|X;\theta)\\
=&amp; \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=&amp; \Pi_{i=1}^m \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;似然(likelihood)和概率(probability)实际上是一个东西，但是似然函数是对参数$\theta$定义的，为了加以区分，使用了似然这一术语。我们可以说参数的似然，数据的概率，但不能说数据的似然，参数的概率。&lt;/p&gt;

&lt;p&gt;极大似然估计的含义是选择参数$\theta$，使参数的似然函数最大化，也就是说选择参数使得已有样本数据出现的概率最大。&lt;/p&gt;

&lt;p&gt;为方便求解，再定义函数$l(\theta)$：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
l(\theta) =&amp; \log L(\theta) \\
=&amp; \sum_{i=1}^m \log \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\
=&amp; m\log (\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2)
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;可得，要最大化似然函数$L(\theta)$，也就是最大化$l(\theta)$，也就是最小化：&lt;/p&gt;

&lt;div&gt;
$$\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$$
&lt;/div&gt;

&lt;p&gt;这个式子刚好是最小二乘法中定义的成本函数$J(\theta)$。总结一下，最小二乘回归模型刚好就是在假设了误差独立同服从正态分布后，得到的最大似然估计。注意到，正态分布中的方差$\sigma^2$的取值对模型并没有影响。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记1 有监督学习 线性回归 LMS算法 正规方程</title>
      <link>http://nanshu.wang/post/2015-02-10</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-02-10</guid>
      <description>

&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:b33d274e87081502d65882ed2d51cd57&#34;&gt;有监督学习&lt;/h1&gt;

&lt;p&gt;先理清几个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$x^{(i)}$表示&amp;rdquo;输入&amp;rdquo;变量(&amp;ldquo;input&amp;rdquo; variables)，也称为特征值(features)。&lt;/li&gt;
&lt;li&gt;$y^{(i)}$表示&amp;rdquo;输出&amp;rdquo;变量(&amp;ldquo;output&amp;rdquo; variables)，也称为目标值(target)。&lt;/li&gt;
&lt;li&gt;一对$(x^{(i)},y^{(i)})$称为一个训练样本(training example)，用作训练的数据集就是就是一组$m$个训练样本${(x^{(i)},y^{(i)});i=1,&amp;hellip;,m}$，被称为训练集(training set)。&lt;/li&gt;
&lt;li&gt;$X$表示输入变量的取值空间，$Y$表示输出变量的取值空间。那么$h:X \rightarrow Y$是训练得到的映射函数，对于每个取值空间X的取值，都能给出取值空间Y上的一个预测值。函数$h$的含义为假设(hypothesis)。&lt;/li&gt;
&lt;li&gt;图形化表示整个过程：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/supervised-learning.png&#34; alt=&#34;supervised-learning&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当预测值y为连续值时，则有监督学习问题是回归(regression)问题；预测值y为离散值时，则为分类(classification)问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;线性回归-linear-regression:b33d274e87081502d65882ed2d51cd57&#34;&gt;线性回归(Linear Regression)&lt;/h2&gt;

&lt;p&gt;先简单将y表示为x的线性函数：&lt;/p&gt;

&lt;div&gt;
$$h(x) = \sum_{i=0}^{n}\theta _ix_i=\theta^Tx$$
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;$\theta$ 称为参数(parameters)，也叫做权重(weights)，参数决定了$X$到$Y$的射映空间。&lt;/li&gt;
&lt;li&gt;用$x_0=1$来表示截距项(intercept term)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有了训练集，如果通过学习得到参数$\theta$？&lt;/p&gt;

&lt;p&gt;一种方法是，让预测值$h(x)$尽量接近真实值y，定义成本函数(cost function):&lt;/p&gt;

&lt;div&gt;
$$J(\theta) = \frac12\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)^2$$
&lt;/div&gt;

&lt;p&gt;这实际上就是最小二乘成本函数，我们把这个回归模型叫做普通最小二乘回归模型(ordinary least squares regression model)。&lt;/p&gt;

&lt;h2 id=&#34;lms算法:b33d274e87081502d65882ed2d51cd57&#34;&gt;LMS算法&lt;/h2&gt;

&lt;p&gt;为了找到使成本函数$J(\theta)$最小的参数$\theta$，采用搜索算法：给定一个$\theta$的初值，然后不断改进，每次改进都使$J(\theta)$更小，直到最小化$J(\theta)$的$\theta$的值收敛。&lt;/p&gt;

&lt;p&gt;考虑梯度下降(gradient descent)算法：从初始$\theta$开始，不断更新：&lt;/p&gt;

&lt;p&gt;$$\theta_j:=\theta_j-\alpha \frac{\delta}{\delta\theta_j}J(\theta)$$&lt;/p&gt;

&lt;p&gt;注意，更新是同时对所有$j=0,&amp;hellip;,n$的$\theta_j$值进行。$\alpha$被称作学习率(learning rate)，也是梯度下降的长度，若$\alpha$取值较小，则收敛的时间较长；相反，若$\alpha$取值较大，则可能错过最优值。&lt;/p&gt;

&lt;p&gt;假设我们只有一个训练样本$(x,y)$，此时$J(\theta) = \frac12(h_{\theta}(x)-y)^2$，求偏导项得到：&lt;/p&gt;

&lt;div&gt;
$$\begin{equation}
\begin{split}
\frac{\delta}{\delta\theta_j}J(\theta) =&amp; \frac{\delta}{\delta\theta_j}\frac12(h_{\theta}(x)-y)^2\\
=&amp; (h_{\theta}(x)-y)*\frac{\delta}{\delta\theta_j}(h_{\theta}(x)-y)\\
=&amp; ((h_{\theta}(x)-y))*\frac{\delta}{\delta\theta_j}(\sum_{i=0}^{n}\theta_ix_i-y)\\
=&amp; (h_{\theta}(x)-y)*x_j
\end{split}
\end{equation}$$
&lt;/div&gt;

&lt;p&gt;每次按照以下式子更新$\theta_j$的值：&lt;/p&gt;

&lt;div&gt;
$$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))*x_j^{(i)}$$
&lt;/div&gt;

&lt;p&gt;这种更新方法叫做LMS更新策略(Least Mean Squares update rule)，也叫做Widrow-Hoff 学习策略。&lt;/p&gt;

&lt;p&gt;采用LMS方法，参数更新的次数和误差项$(y^{(i)}-h_{\theta}(x^{(i)}))$成正比。也就是说，如果预测值与真实值的误差项较小，则参数调整改变不会很大，相反，如果误差项较大，参数进行的调整更大。&lt;/p&gt;

&lt;p&gt;如果训练集不只一个训练样本，可以采用以下方法更新参数：&lt;/p&gt;

&lt;p&gt;Repeat until convergence{&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$\theta_j:=\theta_j+\alpha\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)})x_j^{(i)})$&lt;/code&gt; (for every j)&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;实际上，这里的求和刚好是$\frac{\delta J(\theta)}{\delta\theta_j}$的值。这种方法每一步更新都会遍历每所有的训练样本，因此被称作批量梯度下降(batch gradient descent)。&lt;/p&gt;

&lt;p&gt;梯度下降法通常容易受局部最优值的影响，但这里的最优问题只有一个全局最优值，没有局部最优值。因此梯度下降总是收敛到全局最优解（学习率$\alpha$不能取太大，否则错过最优值）。&lt;/p&gt;

&lt;p&gt;除了批量梯度下降，还有一种方法叫做随机梯度下降(stochastic gradient descent)，也叫做增量梯度下降(incremental gradient descent)。其更新策略为：&lt;/p&gt;

&lt;p&gt;Loop{&lt;/p&gt;

&lt;p&gt;for i=1 to m,{&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}$&lt;/code&gt; (for every j).&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;随机梯度下降和批量梯度下降不同点在于，批量梯度下降每一步更新$\theta$值，都需要遍历全部的训练样本，而随机梯度下降在遇到每个训练样本时，更新$\theta$之后继续处理下一个样本，每个样本只遍历一次，算法的学习时间比批量梯度下降快很多。但是，随机梯度下降可能永远不会收敛到全局最优值，而是在成本函数$J(\theta)$最优值周围附近摇摆。但是在实际问题中，接近最优值的参数值可能已经是足够好的结果了，特别是对于数据量非常大的训练集来说，随机梯度下降是比批量梯度下降更好的选择。&lt;/p&gt;

&lt;p&gt;在实际使用梯度下降算法时，将输入变量归一到同一取值范围，能够减少算法的迭代次数。这是因为$\theta$在小的取值范围内会下降很快，但在大的取值范围内会下降较慢。并且当输入变量取值范围不够均衡时，$\theta$更容易在最优值周围波动。采用特征缩放(feature scaling)和均值归一化(mean normalization)可以避免这些问题。选择学习率$\alpha$的值，要观察$J(\theta)$值在每次迭代后的变化。已经证明了如果$\alpha$的取值足够小，则$J(\theta)$每次迭代后的值都会减少。如果$J(\theta)$在某次迭代后反而增加了，说明学习率$\alpha$的值应该减小，因为错过了最优值。Andrew Ng推荐的一个经验是每次将$\alpha$减少3倍。&lt;/p&gt;

&lt;h2 id=&#34;正规方程-the-normal-equations:b33d274e87081502d65882ed2d51cd57&#34;&gt;正规方程(The normal equations)&lt;/h2&gt;

&lt;p&gt;梯度下降是最小化$J(\theta)$的一种方式，正规方程是另一种求解参数$\theta$的方法，这种方法可以直接求出最优值参数结果，不需要迭代更新，也不需要事先对数据进行归一化预处理。这种方法实际上是直接求出$J(\theta)$的导数，并令其为0。&lt;/p&gt;

&lt;div&gt;
$$J(\theta)=\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)}))^2=\frac12(X\theta-\overrightarrow y)^T(X\theta-\overrightarrow y)$$

$$\nabla_{\theta}J(\theta)=0$$ 
&lt;/div&gt;

&lt;p&gt;解之，&lt;/p&gt;

&lt;div&gt;
$$\nabla_{\theta}J(\theta) = X^TX\theta-X^T\overrightarrow y=0$$
&lt;/div&gt;

&lt;p&gt;得到正规方程：&lt;/p&gt;

&lt;div&gt;
$$X^TX\theta=X^T\overrightarrow y$$
&lt;/div&gt;

&lt;p&gt;求解$\theta$：&lt;/p&gt;

&lt;div&gt;
$$\theta=(X^TX)^{-1}X^T\overrightarrow y$$
&lt;/div&gt;

&lt;p&gt;正规方程求解$\theta$的时间复杂度为$O(n^3)$，n是特征数量。当特征数量很大时，正规方程求解会很慢。Andrew Ng给出的一个经验参考是：当n&amp;gt;10,000时，采用梯度下降比正规方程更好。&lt;/p&gt;

&lt;p&gt;比较一下正规方程和梯度下降：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;正规方程&lt;/th&gt;
&lt;th&gt;梯度下降&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;不需要调整参数&lt;/td&gt;
&lt;td&gt;需要调整参数$\alpha$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;不需要迭代&lt;/td&gt;
&lt;td&gt;需要迭代更新$\theta$值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n较大时效率低&lt;/td&gt;
&lt;td&gt;n较大时效率也不错&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;正规方程还会遇到$X^TX$不可逆的情况，通常这是因为输入变量中存在线性相关的变量或者是因为特征太多($n\geq m$)。解决方法是去掉线性相关的冗余变量，或者删掉一些特征。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hugo静态网站生成器中文教程</title>
      <link>http://nanshu.wang/post/2015-01-31</link>
      <pubDate>Sat, 31 Jan 2015 00:30:03 CST</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2015-01-31</guid>
      <description>

&lt;h1 id=&#34;前言:d605f9890f3528aea462ac7515ece633&#34;&gt;前言&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://gohugo.io&#34;&gt;Hugo&lt;/a&gt;是什么？官方文档是这样介绍它的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hugo is a general-purpose website framework. Technically speaking, Hugo is a static site generator.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hugo是一种通用的网站框架。严格来说，Hugo应该被称作静态网站生成器。&lt;/p&gt;

&lt;p&gt;静态网站生成器从字面上来理解，就是将你的内容生成静态网站。所谓“静态”的含义其实反映在网站页面的生成的时间。一般的web服务器（WordPress, Ghost, Drupal等等）在收到页面请求时，需要调用数据库生成页面（也就是HTML代码），再返回给用户请求。而静态网站则不需要在收到请求后生成页面，而是在整个网站建立起之前就将所有的页面全部生成完成，页面一经生成便称为静态文件，访问时直接返回现成的静态页面，不需要数据库的参与。&lt;/p&gt;

&lt;p&gt;采用静态网站的维护也相当简单，实际上你根本不需要什么维护，完全不用考虑复杂的运行时间，依赖和数据库的问题。再有也不用担心安全性的问题，没有数据库，网站注入什么的也无从下手。&lt;/p&gt;

&lt;p&gt;静态网站最大好处就是访问快速，不用每次重新生成页面。当然，一旦网站有任何更改，静态网站生成器需要重新生成所有的与更改相关的页面。然而对于小型的个人网站，项目主页等等，网站规模很小，重新生成整个网站也是非常快的。Hugo在速度方面做得非常好，Dan Hersam在他这个&lt;a href=&#34;https://www.udemy.com/build-static-sites-in-seconds-with-hugo/&#34;&gt;Hugo教程&lt;/a&gt;里提到，5000篇文章的博客，Hugo生成整个网站只花了6秒，而很多其他的静态网站生成器则需要几分钟的时间。我的博客目前文章只有几十篇，用Hugo生成整个网站只需要0.1秒。官方文档提供的数据是每篇页面的生成时间不到1ms。&lt;/p&gt;

&lt;p&gt;我认为对于个人博客来说，应该将时间花在内容上而不是各种折腾网站。Hugo会将Markdown格式的内容和设置好模版一起，生成漂亮干净的页面。挑选折腾好一个喜爱的模版，在Sublime Text里用Markdown写博客，再敲一行命令生成同步到服务器就OK了。整个体验是不是非常优雅简单还有点geek的味道呢？&lt;/p&gt;

&lt;p&gt;Hugo是用&lt;a href=&#34;http://golang.org/&#34;&gt;Go语言&lt;/a&gt;写的，为什么使用Go，作者&lt;a href=&#34;http://spf13.com&#34;&gt;Steve Francia&lt;/a&gt;的原话是：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I looked at existing static site generators like Jekyll, Middleman and nanoc. All had complicated dependencies to install and took far longer to render my blog with hundreds of posts than I felt was acceptable. I wanted a framework to be able to get rapid feedback while making changes to the templates, and the 5+-minute render times was just too slow. In general, they were also very blog minded and didn’t have the ability to have different content types and flexible URLs.&lt;/p&gt;

&lt;p&gt;I wanted to develop a fast and full-featured website framework without dependencies. The Go language seemed to have all of the features I needed in a language. I began developing Hugo in Go and fell in love with the language. I hope you will enjoy using (and contributing to) Hugo as much as I have writing it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;总结他的一下大意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;吐槽脸：Jekyll以及那一堆静态网站生成器安装麻烦（依赖多），速度又慢，内容类型单一，url死板&lt;/li&gt;
&lt;li&gt;挽袖子状：Go挺萌的符合我对语言的一切幻想，就用它重写一个吧&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我为啥用Hugo？除了以上提到的原因，很重要的一点是&lt;a href=&#34;http://gohugo.io&#34;&gt;Hugo主页&lt;/a&gt;很漂亮，看了一圈静态网站生成器的主页，一眼就被Hugo的美到了，首页的照片里的那个格子小本子应该是&lt;a href=&#34;http://www.paperthinks.com&#34;&gt;Paperthinks&lt;/a&gt;，我正好也在用，有种刚好看到自己桌面的感觉。&lt;/p&gt;

&lt;h1 id=&#34;安装:d605f9890f3528aea462ac7515ece633&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;如果说速度快是Hugo的第一大优点，那么安装简单应该就是Hugo的第二大优点。对于Mac用户，没有brew的话先安装brew，在命令行里敲：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后再敲一行安装Hugo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew new Hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然你也可以在&lt;a href=&#34;https://github.com/spf13/hugo/releases&#34;&gt;这里&lt;/a&gt;直接下载对应系统的binary文件，解压就行了。&lt;/p&gt;

&lt;h1 id=&#34;了解hugo:d605f9890f3528aea462ac7515ece633&#34;&gt;了解Hugo&lt;/h1&gt;

&lt;p&gt;首先建立自己的网站，mysite是网站的路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo new site mysite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后进入该路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd mysite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在该目录下你可以看到以下几个目录和&lt;code&gt;config.toml&lt;/code&gt;文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ▸ archetypes/ 
 ▸ content/
 ▸ layouts/
 ▸ static/
   config.toml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;config.toml&lt;/code&gt;是网站的配置文件，包括&lt;code&gt;baseurl&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;copyright&lt;/code&gt;等等网站参数。&lt;/p&gt;

&lt;p&gt;这几个文件夹的作用分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;archetypes：包括内容类型，在创建新内容时自动生成内容的配置&lt;/li&gt;
&lt;li&gt;content：包括网站内容，全部使用markdown格式&lt;/li&gt;
&lt;li&gt;layouts：包括了网站的模版，决定内容如何呈现&lt;/li&gt;
&lt;li&gt;static：包括了css, js, fonts, media等，决定网站的外观&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hugo提供了一些完整的主题可以使用，下载这些主题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone --recursive https://github.com/spf13/hugoThemes themes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时现成的主题存放在&lt;code&gt;themes/&lt;/code&gt;文件夹中。&lt;/p&gt;

&lt;p&gt;现在我们先熟悉一下Hugo，创建新页面：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo new about.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入&lt;code&gt;content/&lt;/code&gt;文件夹可以看到，此时多了一个markdown格式的文件&lt;code&gt;about.md&lt;/code&gt;，打开文件可以看到时间和文件名等信息已经自动加到文件开头，包括创建时间，页面名，是否为草稿等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
+++
date = &amp;quot;2015-02-01T18:19:54+08:00&amp;quot;
draft = true
title = &amp;quot;about&amp;quot;

+++

# 关于我
- 2010  HR@RUC
- 2014  CS@ICT, CAS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我在页面中加入了一些内容，然后运行Hugo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo server -t hyde --buildDrafts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;-t&lt;/code&gt;参数的意思是使用hyde主题渲染我们的页面，注意到&lt;code&gt;about.md&lt;/code&gt;目前是作为草稿，即&lt;code&gt;draft&lt;/code&gt;参数设置为&lt;code&gt;true&lt;/code&gt;，运行Hugo时要加上&lt;code&gt;--buildDrafts&lt;/code&gt;参数才会生成被标记为草稿的页面。
在浏览器输入localhost:1313，就可以看到我们刚刚创建的页面。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/hugo-server-1.png&#34; alt=&#34;hugo-server-1&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;注意观察当前目录下多了一个文件夹&lt;code&gt;public/&lt;/code&gt;，这里面是Hugo生成的整个静态网站，如果使用Github pages来作为博客的Host，你只需要将&lt;code&gt;public/&lt;/code&gt;里的文件上传就可以，这相当于是Hugo的输出。&lt;/p&gt;

&lt;h1 id=&#34;主题选择:d605f9890f3528aea462ac7515ece633&#34;&gt;主题选择&lt;/h1&gt;

&lt;p&gt;进入&lt;code&gt;themes/hyde&lt;/code&gt;文件夹，可以看到熟悉的文件夹名，和主题相关的文件主要是在&lt;code&gt;layouts/&lt;/code&gt;和&lt;code&gt;static/&lt;/code&gt;这两个文件内，选择好一个主题后，可以将&lt;code&gt;themes/&lt;/code&gt;中的文件夹直接复制到&lt;code&gt;mysite/&lt;/code&gt;目录下，覆盖原来的&lt;code&gt;layouts/&lt;/code&gt;, &lt;code&gt;static/&lt;/code&gt;文件夹，此时直接使用\$Hugo server就可以看到主题效果，修改主题也可以直接修改其中的css, js, html等文件。&lt;/p&gt;

&lt;p&gt;我的博客模版是在Hugo作者spf13的&lt;a href=&#34;http://spf13.com&#34;&gt;博客&lt;/a&gt;基础上修改的。第一步，先去他的博客网站源码&lt;a href=&#34;https://github.com/spf13/spf13.com&#34;&gt;主页&lt;/a&gt;把整个项目clone下来&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git@github.com:spf13/spf13.com.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把项目中的&lt;code&gt;static/&lt;/code&gt;和&lt;code&gt;layouts/&lt;/code&gt;文件复制到自己网站的目录下替换原来的文件夹。再次运行Hugo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo server --buildDrafts -w
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这次没有选择主题，如果选择了主题会将当前的主题覆盖掉。参数&lt;code&gt;-w&lt;/code&gt;意味监视watch，此时如果修改了网站内的信息，会直接显示在浏览器的页面上，不需要重新运行\$hugo server，方便我们进行修改。这是采用了spf13主题的页面：&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/hugo-server-2.png&#34; alt=&#34;hugo-server-2&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;我们尝试在他的主题基础上修改，找到&lt;code&gt;/layouts/partials/subheader.html&lt;/code&gt;文件:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;header id=&amp;quot;header&amp;quot;&amp;gt;
    &amp;lt;figure&amp;gt;
      &amp;lt;a href=&amp;quot;/&amp;quot; border=0 id=&amp;quot;logolink&amp;quot;&amp;gt;&amp;lt;div class=&amp;quot;icon-spf13-3&amp;quot; id=&amp;quot;logo&amp;quot;&amp;gt; &amp;lt;/div&amp;gt;&amp;lt;/a&amp;gt;
    &amp;lt;/figure&amp;gt;
    &amp;lt;div id=&amp;quot;byline&amp;quot;&amp;gt;by Steve Francia&amp;lt;/div&amp;gt;
    &amp;lt;nav id=&amp;quot;nav&amp;quot;&amp;gt;
    {{ partial &amp;quot;nav.html&amp;quot; . }}
    {{ partial &amp;quot;social.html&amp;quot; . }}
    &amp;lt;/nav&amp;gt;
&amp;lt;/header&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将by Steve Francia换成by myname，再次回到浏览器，可以看到左边侧栏已经发生变化了，你可以根据自己的需要修改对应的文件，当然得懂一点css, html。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/hugo-server-change.png&#34; alt=&#34;hugo-server-change&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;评论功能:d605f9890f3528aea462ac7515ece633&#34;&gt;评论功能&lt;/h1&gt;

&lt;p&gt;个人博客当然不能没有评论，Hugo默认支持&lt;a href=&#34;https://disqus.com/&#34;&gt;Disqus&lt;/a&gt;的评论，需要在模版中添加以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ template &amp;quot;_internal/disqus.html&amp;quot; . }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;spf13在&lt;code&gt;/layouts/partials/disqus.html&lt;/code&gt;中已经添加好了。&lt;/p&gt;

&lt;p&gt;只需要去Disqus注册一个账号，然后在&lt;code&gt;config.toml&lt;/code&gt;里加上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;disqusShortname = &amp;quot;yourdisqusShortname&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意&lt;code&gt;-w&lt;/code&gt;参数是不能监测&lt;code&gt;config.toml&lt;/code&gt;里参数变化的，因此需要重新运行Hugo，进入localhost:1313/about，可以看到评论功能。&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://nanshu.wang/media/comments.png&#34; alt=&#34;comments&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;/p&gt;

&lt;h1 id=&#34;代码高亮:d605f9890f3528aea462ac7515ece633&#34;&gt;代码高亮&lt;/h1&gt;

&lt;p&gt;作为码农，代码高亮对于写博客来说当然必不可少。有两种方法：第一种是在生成页面时就生成好代码高亮过的页面；第二种是使用js，用户加载页面时浏览器再进行渲染。&lt;/p&gt;

&lt;p&gt;第一种方法需要使用&lt;a href=&#34;http://pygments.org/&#34;&gt;Pygments&lt;/a&gt;，一个python写的工具。&lt;/p&gt;

&lt;p&gt;安装Pygments：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install Pygments
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;没有pip的先下载 &lt;a href=&#34;https://bootstrap.pypa.io/get-pip.py&#34;&gt;https://bootstrap.pypa.io/get-pip.py&lt;/a&gt; ，然后安装pip：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python get-pip.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pygments的调用采用shortcodes实现，spf13里也写好了，在&lt;code&gt;/layouts/shortcode/highlight.html&lt;/code&gt;里&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ $lang := index .Params 0 }}
{{ highlight .Inner $lang }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要使代码高亮，在你的代码外面加上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ % highlight python %}}
your code here.
{{ % /highlight %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里为了避免以上两行被识别为代码高亮的标识，在&lt;code&gt;{{&lt;/code&gt;和&lt;code&gt;%&lt;/code&gt;之间多加了一个空格，实际使用的时候需要把空格去掉。&lt;/p&gt;

&lt;p&gt;第二种方法比较简单，在&lt;code&gt;layouts/partials/header_includes.html&lt;/code&gt;中加上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script src=&amp;quot;https://yandex.st/highlightjs/8.0/highlight.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;https://yandex.st/highlightjs/8.0/styles/default.min.css&amp;quot;&amp;gt;
&amp;lt;script&amp;gt;hljs.initHighlightingOnLoad();&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里使用了&lt;a href=&#34;http://yandex.ru/&#34;&gt;Yandex&lt;/a&gt;的&lt;a href=&#34;http://highlightjs.org/&#34;&gt;Highlight.js&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;其他的可以实现代码高亮的js库还有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://highlightjs.org/&#34;&gt;Highlight.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://craig.is/making/rainbows&#34;&gt;Rainbow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://alexgorbatchev.com/SyntaxHighlighter/&#34;&gt;Syntax Highlighter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://code.google.com/p/google-code-prettify/&#34;&gt;Google Prettify&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;插入图片:d605f9890f3528aea462ac7515ece633&#34;&gt;插入图片&lt;/h1&gt;

&lt;p&gt;图片文件放在&lt;code&gt;static/media&lt;/code&gt;文件中，插入图片：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{ % img src=&amp;quot;/media/example.jpg&amp;quot; alt=&amp;quot;example&amp;quot; %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意这里的&lt;code&gt;{{&lt;/code&gt;和&lt;code&gt;%&lt;/code&gt;之间也加上了空格，避免这行代码起作用，实际使用也需要把空格去掉。&lt;/p&gt;

&lt;h1 id=&#34;使用mathjax:d605f9890f3528aea462ac7515ece633&#34;&gt;使用Mathjax&lt;/h1&gt;

&lt;p&gt;在需要渲染公式的页面加入以下代码，比如&lt;code&gt;layouts/_default/single.html&lt;/code&gt;文件，这个文件是对于所有post进行页面生成的模版，如果你希望所有页面都对公式渲染的话，可以加入&lt;code&gt;layouts/partials/footer.html&lt;/code&gt;文件里，保证所有生成的页面都有这几行代码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mathjax和Markdown会有冲突问题，&lt;a href=&#34;http://doswa.com/2011/07/20/mathjax-in-markdown.html&#34;&gt;这里&lt;/a&gt;提供了解决方案。&lt;/p&gt;

&lt;h1 id=&#34;用github-pages作为网站的host:d605f9890f3528aea462ac7515ece633&#34;&gt;用github pages作为网站的Host&lt;/h1&gt;

&lt;p&gt;Github pages分为两种：一种是项目主页，每个项目都可以有一个；另一种是用户主页，一个用户只能有一个。&lt;/p&gt;

&lt;p&gt;因为用户主页只能有一个，所以建议使用项目主页托管，不过我这里采用了用户主页，反正我也只用一个博客，使用个人主页作为Host也相对更简单一点。&lt;/p&gt;

&lt;p&gt;我们需要创建两个单独的repo，一个用于放Hugo的输入文件，即除了&lt;code&gt;public/&lt;/code&gt;文件夹之外的所有文件，另一个放我们生成的静态网站，也就是&lt;code&gt;public/&lt;/code&gt;的内容。&lt;/p&gt;

&lt;p&gt;步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在Github上创建repo &lt;code&gt;&amp;lt;your-project&amp;gt;-hugo&lt;/code&gt;，托管Hugo的输入文件。&lt;/li&gt;
&lt;li&gt;创建repo &lt;code&gt;&amp;lt;username&amp;gt;.github.io&lt;/code&gt;，用于托管&lt;code&gt;public/&lt;/code&gt;文件夹，注意这里的repo名字一定要用自己的用户名，才会被当作是个人主页。&lt;/li&gt;
&lt;li&gt;clone your-project&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ git clone &amp;lt;&amp;lt;your-project&amp;gt;-hugo-url&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;进入your-project 目录&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ cd &amp;lt;your-project&amp;gt;-hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;删掉public目录（这个目录每次运行Hugo都会再次生成，不用担心）&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ rm -rf public
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;把public/目录添加为submodule 与&lt;username&gt;.github.io同步&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ git submodule add git@github.com:&amp;lt;username&amp;gt;/&amp;lt;username&amp;gt;.github.io.git public
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;添加.gitignore文件，文件中写&lt;code&gt;public/&lt;/code&gt;，在同步&lt;code&gt;&amp;lt;your-project&amp;gt;-hugo&lt;/code&gt;时会忽略public文件夹&lt;/li&gt;
&lt;li&gt;下面是写好的一个script &lt;code&gt;deploy.sh&lt;/code&gt;，拷贝过去直接就能用，记得chmod +x deploy.sh加上运行权限。&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi

# Push Hugo content 
git add -A
git commit -m &amp;quot;$msg&amp;quot;
git push origin master


# Build the project. 
hugo # if using a theme, replace by `hugo -t &amp;lt;yourtheme&amp;gt;`

# Go To Public folder
cd public
# Add changes to git.
git add -A

# Commit changes.

git commit -m &amp;quot;$msg&amp;quot;

# Push source and build repos.
git push origin master

# Come Back
cd ..

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等一小会儿（10分钟左右），你就能在&lt;a href=&#34;http://username.github.io/&#34;&gt;http://username.github.io/&lt;/a&gt; 这个页面看到你的网站了！每次更新网站或者写了新文章，只需要运行./deploy.sh 发布就搞定了，简单吧？&lt;/p&gt;

&lt;p&gt;Github pages还支持域名绑定，三个步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在&lt;code&gt;&amp;lt;username&amp;gt;.github.io&lt;/code&gt; repo的跟目录下添加&lt;code&gt;CNAME&lt;/code&gt;文件，文件里写上你的域名，不用加http://的开头。&lt;/li&gt;
&lt;li&gt;记下&lt;a href=&#34;http://username.github.io/&#34;&gt;http://username.github.io/&lt;/a&gt; 的ip地址。&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ ping username.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;在你的域名管理中加上两条A记录，分别是www和@，记录指向&lt;a href=&#34;http://username.github.io/&#34;&gt;http://username.github.io/&lt;/a&gt; 的ip地址，也需要等一小会儿生效。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;更改字体服务商:d605f9890f3528aea462ac7515ece633&#34;&gt;更改字体服务商&lt;/h1&gt;

&lt;p&gt;我的博客模版里用的字体是从googleapis里获取的，国内访问会下载失败，把字体库改成360的。
找到&lt;code&gt;layouts/partials/head_includes.html&lt;/code&gt;文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;link href=&#39;http://fonts.googleapis.com/css?family=Fjalla+One|Open+Sans:300&#39; rel=&#39;stylesheet&#39; type=&#39;text/css&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将其中的googleapis替换为useso就行了。&lt;/p&gt;

&lt;p&gt;教程会根据我的博客遇到的问题继续更新。&lt;/p&gt;

&lt;h1 id=&#34;增加网站分析:d605f9890f3528aea462ac7515ece633&#34;&gt;增加网站分析&lt;/h1&gt;

&lt;p&gt;使用网站分析可以帮助我们更好地了解博客的读者和流量来源，我使用了&lt;a href=&#34;http://tongji.baidu.com&#34;&gt;百度统计&lt;/a&gt;和&lt;a href=&#34;http://www.google.cn/webmasters/&#34;&gt;谷歌统计&lt;/a&gt;，注册帐号后只需要按照提示在模板中加入相应的script代码就可以了。&lt;/p&gt;

&lt;h1 id=&#34;参考:d605f9890f3528aea462ac7515ece633&#34;&gt;参考&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://gohugo.io/overview/introduction/&#34;&gt;Hugo docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ipn.li/kernelpanic/3/&#34;&gt;《内核恐慌》静态网站生成器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/build-static-sites-in-seconds-with-hugo/&#34;&gt;Build Static Sites in Seconds with Hugo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/articles/setting-up-a-custom-domain-with-github-pages/&#34;&gt;Setting up a custom domain with GitHub Pages&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Scikit-Learn机器学习介绍（中文翻译）</title>
      <link>http://nanshu.wang/post/2014-12-02</link>
      <pubDate>Tue, 02 Dec 2014 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2014-12-02</guid>
      <description>

&lt;p&gt;翻译自：&lt;a href=&#34;http://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34;&gt;http://scikit-learn.org/stable/tutorial/basic/tutorial.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;以后可能会根据自己的学习慢慢翻译其他的章节，水平有限，不足之处请指正。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本章内容
在本章中，我们会介绍在使用scikit-learn中遇到的&lt;a href=&#34;http://en.wikipedia.org/wiki/Machine_learning&#34;&gt;机器学习&lt;/a&gt;(machine learning)术语，以及一个简单的机器学习例子。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;机器学习-问题设定:935e1a74f960fd04b26b502f6058f057&#34;&gt;机器学习：问题设定&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;一般来说，机器学习问题可以这样来理解：我们有n个&lt;a href=&#34;http://en.wikipedia.org/wiki/Sample_(statistics)&#34;&gt;样本&lt;/a&gt;(sample)的数据集，想要预测未知数据的属性。
如果描述每个样本的数字不只一个，比如一个多维的条目（也叫做&lt;a href=&#34;http://en.wikipedia.org/wiki/Multivariate_random_variable&#34;&gt;多变量数据&lt;/a&gt;(multivariate data)），那么这个样本就有多个属性或者&lt;strong&gt;特征&lt;/strong&gt;。
我们可以将学习问题分为以下几类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Supervised_learning&#34;&gt;有监督学习&lt;/a&gt;(unsupervised learning)是指数据中包括了我们想预测的属性，有监督学习问题有以下两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Classification_in_machine_learning&#34;&gt;分类&lt;/a&gt;（classification）:样本属于两个或多个类别，我们希望通过从已标记类别的数据学习，来预测未标记数据的分类。例如，识别手写数字就是一个分类问题，其目标是将每个输入向量对应到有穷的数字类别。从另一种角度来思考，分类是一种有监督学习的离散（相对于连续）形式，对于n个样本，一方有对应的有限个类别数量，另一方则试图标记样本并分配到正确的类别。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Regression_analysis&#34;&gt;回归&lt;/a&gt;(regression):如果希望的输出是一个或多个连续的变量，那么这项任务被称作*回归*，比如用年龄和体重的函数来预测三文鱼的长度。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Unsupervised_learning&#34;&gt;无监督学习&lt;/a&gt;(unsupervised learning)的训练数据包括了输入向量X的集合，但没有相对应的目标变量。这类问题的目标可以是发掘数据中相似样本的分组，被称作&lt;a href=&#34;http://en.wikipedia.org/wiki/Cluster_analysis&#34;&gt;聚类&lt;/a&gt;(Clustering)；也可以是确定输入样本空间中的数据分布，被称作&lt;a href=&#34;http://en.wikipedia.org/wiki/Density_estimation&#34;&gt;密度估计&lt;/a&gt;（density estimation）;还可以是将数据从高维空间投射到两维或三维空间，以便进行数据可视化。&lt;a href=&#34;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&#34;&gt;这里&lt;/a&gt;是Scikit-Learn的无监督学习主页。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;训练集和测试集
机器学习是关于如何从数据学习到一些属性并且用于新的数据集。这也是为什么机器学习中评估算法的一个习惯做法是将手头已有的数据集分成两部分：一部分我们称作&lt;strong&gt;训练集&lt;/strong&gt;（training set），用来学习数据的属性；另一部分叫做&lt;strong&gt;测试集&lt;/strong&gt;（testing set），用来测试这些属性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;加载样例数据集:935e1a74f960fd04b26b502f6058f057&#34;&gt;加载样例数据集&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;scikit-learn有一些标准数据集，比如用于分类的&lt;a href=&#34;http://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;iris&lt;/a&gt;和&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits&#34;&gt;digits&lt;/a&gt;数据集，和用于回归的&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Housing&#34;&gt;波士顿房价&lt;/a&gt;(boston house prices)数据集。
下面，我们会用shell里的Python解释器来加载&lt;code&gt;iris&lt;/code&gt;和&lt;code&gt;digits&lt;/code&gt;数据集。&lt;code&gt;$&lt;/code&gt;表示shell提示符，&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;表示Python解释器提示符：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ python
  &amp;gt;&amp;gt;&amp;gt; from sklearn import datasets
  &amp;gt;&amp;gt;&amp;gt; iris = datasets.load_iris()
  &amp;gt;&amp;gt;&amp;gt; digits = datasets.load_digits()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据集类似字典对象，包括了所有的数据和关于数据的元数据（metadata）。数据被存储在&lt;code&gt;.data&lt;/code&gt;成员内，是一个&lt;code&gt;n_samples*n_features&lt;/code&gt;的数组。在有监督问题的情形下，一个或多个因变量（response variables）被储存在&lt;code&gt;.target&lt;/code&gt;成员中。有关不同数据集的更多细节可以在&lt;a href=&#34;http://scikit-learn.org/stable/datasets/index.html#datasets&#34;&gt;这里&lt;/a&gt;被找到。
例如，在digits数据集中，&lt;code&gt;digits.data&lt;/code&gt;是可以用来分类数字样本的特征：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; print(digits.data)  # doctest: +NORMALIZE_WHITESPACE
  [[  0.   0.   5. ...,   0.   0.   0.]
   [  0.   0.   0. ...,  10.   0.   0.]
   [  0.   0.   0. ...,  16.   9.   0.]
   ...,
   [  0.   0.   1. ...,   6.   0.   0.]
   [  0.   0.   2. ...,  12.   0.   0.]
   [  0.   0.  10. ...,  12.   1.   0.]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;digits.target&lt;/code&gt;给出了digits数据集的真实值，即每个数字图案对应的我们想预测的真实数字：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; digits.target
  array([0, 1, 2, ..., 8, 9, 8])
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;数据数组的形式
数据是一个2维&lt;code&gt;n_samples*n_features&lt;/code&gt;的数组，尽管原始数据集可能会有不同的形式。在digits数据集中，每个原始样本是一个8*8的数组，可以用以下方式访问：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;    &amp;gt;&amp;gt;&amp;gt; digits.images[0]
    array([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],
         [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],
         [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],
         [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],
         [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],
         [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],
         [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],
         [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/plot_digits_classification.html#example-plot-digits-classification-py&#34;&gt;这个简单的例子&lt;/a&gt;说明了如何从原始问题里将数据形式化，以便scikit-learn使用。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;学习和预测:935e1a74f960fd04b26b502f6058f057&#34;&gt;学习和预测&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;在digits数据集中，我们的任务是给定一个图案，预测其表示的数字是什么。我们的样本有10个可能的分类（数字0到9)，我们将拟合一个&lt;a href=&#34;http://en.wikipedia.org/wiki/Estimator&#34;&gt;预测器&lt;/a&gt;(estimator)来&lt;strong&gt;预测&lt;/strong&gt;(predict)未知样本所属的分类。
在scikit-learn中，分类的预测器是一个Python对象，来实现&lt;code&gt;fit(X, y)&lt;/code&gt;和 &lt;code&gt;predict(T)&lt;/code&gt;方法。
下面这个预测器的例子是class&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;，实现了&lt;a href=&#34;http://en.wikipedia.org/wiki/Support_vector_machine&#34;&gt;支持向量机分类&lt;/a&gt;。创建分类器需要模型参数，但现在，我们暂时先将预测器看作是一个黑盒：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; from sklearn import svm
  &amp;gt;&amp;gt;&amp;gt; clf = svm.SVC(gamma=0.001, C=100.)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;选择模型参数
在这个例子里我们手动设置了&lt;code&gt;gamma&lt;/code&gt;值。可以通过这些工具例如&lt;a href=&#34;http://scikit-learn.org/stable/modules/grid_search.html#grid-search&#34;&gt;网格搜索&lt;/a&gt;（grid search）和&lt;a href=&#34;http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation&#34;&gt;交叉验证&lt;/a&gt;（cross validation）来自动找到参数的最佳取值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;给预测器取个名字叫做&lt;code&gt;clf&lt;/code&gt;（claasifier）。现在预测器必须来&lt;strong&gt;拟合&lt;/strong&gt;（fit）模型，也就是说，它必须从模型中&lt;strong&gt;学习&lt;/strong&gt;（learn）。这个过程是通过将训练集传递给&lt;code&gt;fit&lt;/code&gt;方法来实现的。我们将除了最后一个样本的数据全部作为训练集。通过Python语法&lt;code&gt;[:-1]&lt;/code&gt;来选择训练集，这会生成一个新的数组，包含了除最后一个条目的&lt;code&gt;digits.data&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; clf.fit(digits.data[:-1], digits.target[:-1])  # doctest: +NORMALIZE_WHITESPACE
  SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
    gamma=0.001, kernel=&#39;rbf&#39;, max_iter=-1, probability=False,
    random_state=None, shrinking=True, tol=0.001, verbose=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在你可以预测新值了，具体来说，我们可以询问分类器，&lt;code&gt;digits&lt;/code&gt;数据集里最后一个图案所代表的数字是什么，我们并没有用最后一个数据来训练分类器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; clf.predict(digits.data[-1])
  array([8])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最一个图案如下：
&lt;img src=&#34;http://scikit-learn.org/stable/_images/plot_digits_last_image_0011.png&#34; alt=&#34;此处输入图片的描述&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;如你所见，这项任务很具有挑战性：这个图案的分辨率很差。你能和分类器得到一致结果吗？
一个更复杂的分类问题的例子在这里:&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/plot_digits_classification.html#example-plot-digits-classification-py&#34;&gt;识别手写数字&lt;/a&gt;（Recognizing hand-written digits），供学习参考。&lt;/p&gt;

&lt;h2 id=&#34;模型持久性-model-persistence:935e1a74f960fd04b26b502f6058f057&#34;&gt;模型持久性（Model persistence）&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;可以采用Python内建的持久性模型&lt;a href=&#34;http://docs.python.org/library/pickle.html&#34;&gt;pickle&lt;/a&gt;来保存scikit的模型:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; from sklearn import svm
  &amp;gt;&amp;gt;&amp;gt; from sklearn import datasets
  &amp;gt;&amp;gt;&amp;gt; clf = svm.SVC()
  &amp;gt;&amp;gt;&amp;gt; iris = datasets.load_iris()
  &amp;gt;&amp;gt;&amp;gt; X, y = iris.data, iris.target
  &amp;gt;&amp;gt;&amp;gt; clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
    kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None,
    shrinking=True, tol=0.001, verbose=False)

  &amp;gt;&amp;gt;&amp;gt; import pickle
  &amp;gt;&amp;gt;&amp;gt; s = pickle.dumps(clf)
  &amp;gt;&amp;gt;&amp;gt; clf2 = pickle.loads(s)
  &amp;gt;&amp;gt;&amp;gt; clf2.predict(X[0])
  array([0])
  &amp;gt;&amp;gt;&amp;gt; y[0]
  0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在scikit的特定情形下，用joblib&amp;rsquo;s来代替pickle（&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;）会更吸引人，在大数据下效率更高，但只能pickle到磁盘而不是字符串：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; from sklearn.externals import joblib
  &amp;gt;&amp;gt;&amp;gt; joblib.dump(clf, &#39;filename.pkl&#39;) # doctest: +SKIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以在之后重新加载pickled模型（可以在另一个Python程序里）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;gt;&amp;gt;&amp;gt; clf = joblib.load(&#39;filename.pkl&#39;) # doctest:+SKIP
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;注意：
joblib.dump返回一个文件名列表。每个包含在&lt;code&gt;clf&lt;/code&gt;对象中独立的numpy数组是在文件系统中是按顺序排列的一个独立文件。当用joblib.load重新加载模型时，所有文件必须在同一个目录下。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;注意pickle有一些安全性和维护性问题。请参考&lt;a href=&#34;http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence&#34;&gt;模型持久性&lt;/a&gt;章节获得更多关于scikit-learn模型持久性的信息。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>软件自定义网络Software-Definded Networking</title>
      <link>http://nanshu.wang/post/2013-10-22</link>
      <pubDate>Tue, 22 Oct 2013 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2013-10-22</guid>
      <description>

&lt;p&gt;译自：&lt;a href=&#34;https://en.wikipedia.org/wiki/Software-defined_networking&#34;&gt;https://en.wikipedia.org/wiki/Software-defined_networking&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;软件定义网络（Software-Definded Networking, SDN）最初来源于2008年左右在加州大学伯克利分校和斯坦福大学的研究工作，而后演化成为一种新型的计算机网络。&lt;a href=&#34;https://en.wikipedia.org/wiki/Software-defined_networking#cite_note-1&#34;&gt;1&lt;/a&gt; SDN允许网络管理员通过对低层功能的抽象来管理网络服务。具体实现是通过将决定流量从如何发送的系统（控制平面）从底层转发流量到指定目的的系统（数据平面）分离开来。这项技术的发明者们和系统的供应商们认为SDN可以达到简化网络的目的。&lt;a href=&#34;https://en.wikipedia.org/wiki/Software-defined_networking#cite_note-white-2&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;SDN需要引入一些方法来实现控制平面与数据平面的通信。OpenFlow的作为其中的一种实现机制 ，经常被人误解为等同于SDN，实际上其他机制也可以实现SDN的概念。开放网络基金会的创立促进了SDN和OpenFlow市场的发展，云计算这一概念也开始流行。&lt;/p&gt;

&lt;h2 id=&#34;背景:9256170d87d1564859db31c49448ee70&#34;&gt;背景：&lt;/h2&gt;

&lt;p&gt;控制器充当的物理网络和SDN层的之间的界面基于互联网协议 （IP）的网络最初是建立在自治系统 （Autonomous Systems, AS）的概念基础上的。这一概念允许网络进行扩展和延伸，连接节点知道部分必须的路由信息，根据这些信息找到合理的下一跳，并转发数据包到下一跳。这种网络连接方法是简单的，并已被证明是有弹性的和可扩展的。考虑到报文递交的服务，AS的原则不允许指定转发目的在不改变自己标识的情况下移动。转发目的之间的拓扑位置正是它们所连接的网络接口，即它们的标识。此外，只使用基本的AS是很难得到其他标识的质量，比如逻辑分组，访问控制， 服务质量 ，中间的网络处理，也难以确定涉及到成为网络流或网络对话的数据包序列方面的内容。&lt;/p&gt;

&lt;p&gt;互联网工程任务组 (Internet Engineering Task Force,IETF)实行了许多补充标准来扩大标识特定化的需求，如虚拟局域网和虚拟专用网络。这些增加的标准提高了网络元件规格和网络供应商配置接口的复杂度。&lt;/p&gt;

&lt;p&gt;随着弹性的云架构和动态资源分配的发展，以及移动电脑操作系统和虚拟机使用的增长，软件定义网络（SDN）这一额外的分层需求出现了。这一额外分层允许网络运营商指定网络服务，而不用去耦合网络接口的规格。这使得实体之间可以在不改变标识也不违反规格的情况下移动。它也可以简化网络操作，因为每个标识的全局定义不必与每一个接口位置相匹配。这样一层也可以通过将标识和流量特征控制逻辑从基本的基于拓扑转发、桥接及路由中分离出来，从而重建网络元素内部的复杂设计。&lt;/p&gt;

&lt;p&gt;全局软件定义控制还会基于源和目标识别来跟踪特定内容的流量。关于驱动网络硬件的一种机制已经被网络设备制造商采纳，目的是共享在软件定义边缘和供应商特定的桥接及路由之间运行的边缘。OpenFlow协议定义了一组关于转发的开放命令。OpenFlow协议运用全球感知软件控制器（集中式或分布式）来驱动网络边缘硬件，以便创造一个易编程的、基于标识的、覆盖在传统的IP核心上的网络。&lt;a href=&#34;https://en.wikipedia.org/wiki/Software-defined_networking#cite_note-white-2&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;SDN是实现可编程化 主动互联网进程中的一步。SDN允许网络管理员通过控制器来实现对网络流量的可编程中央控制，而不需要访问物理层的网络交换机 。&lt;a href=&#34;https://en.wikipedia.org/wiki/Software-defined_networking#cite_note-4&#34;&gt;3&lt;/a&gt; SDN的配置可以创建起一个逻辑网络控制平面，其中硬件与数据转发平台硬件在物理上是分离的，即网络交换机可以转发数据包并且一个单独的服务器可以这个运行网络控制平面。 这种分离允许控制平面与数据平面在实现中使用不同的分布模型。控制平面的开发和运行环境的任务可以在不同的平台上运行（而不是硬件交换机和路由器上的低功率管理处理器）。&lt;/p&gt;

&lt;p&gt;##SDN部署模型&lt;/p&gt;

&lt;p&gt;###对称与非对称&lt;/p&gt;

&lt;p&gt;在非对称模型中，SDN全局信息尽可能地集中化，边缘驱动时尽可能地分布化。这样做背后的考虑是清晰的，集中化使得全局整合轻松了许多，和分布化降低了SDN流量汇聚-封装的压力。然而，针对不同类型的SDN元素之间的确切关系，这种模型又产生了一系列问题，包括一致性，向外扩展的简化，多位置的高可用性，而在传统的基于AS的网络模型中，这些问题是不存在的。在对称分布的SDN模式中，需要增加全球的信息发布能力，提高SDN汇聚性能，使SDN元素基本上是一种类型的组件。只要在任意元素的子集中网络是可达的，在这样一组元素就可以在之上形成一个SDN覆盖网络。&lt;/p&gt;

&lt;p&gt;###非泛洪与泛洪&lt;/p&gt;

&lt;p&gt;在泛洪模型中，全局信息共享的相当一部分是通过广播和组播机制实现。这可以帮助SDN模型更加对称，它利用现有的透明桥接动态封装的原则，以实现全局意识和标识学习。这种方法的缺点之一是，随着越来越多的位置的加入，每个位置相应负载增加，从而降低了可扩展性。在非泛洪模型中，所有的转发是基于对全局的精确匹配，这通常采用分布式哈希和分布式缓存SDN查找表来实现。&lt;/p&gt;

&lt;p&gt;###基于主机与网络中心化&lt;/p&gt;

&lt;p&gt;在基于主机的模型中，关于SDN的数据中心中，假设很多的虚拟机通过可以移动来便增加弹性。在这个假设下，主机的虚拟机监视器代表本地虚拟机完成SDN封装。这种设计减少了SDN边缘的流量压力，并且根据每个主机上的空闲核心能力来“自由”处理。在网络中心化的设计中，一个更清晰的划分是在网络边缘和终端之间。这种SDN边缘与机柜设备顶端(TopOfRack)接口相连，并在主机端点之外。这是一种更传统的方法，网络互联并不依靠终端去执行任何路由的功能。&lt;/p&gt;

&lt;p&gt;这些设计模型之间的界限划分并不明确。例如，在数据中心使用具有很多CPU“大型”主机计算结构有很多的CPU插件同样执行一些TopOfRack的接入功能，并可以代替一个机箱中所有的CPU插件，集中SDN边缘功能。这既是基于主机的设计，也是网络中心化的设计。也有可能是这些设计变量之间存在依赖关系，例如基于主机的实现通常会委托一个非对称集中查找或业务流程服务来协助组织庞大的分布。对称和非泛洪的模型通常会委托网络SDN汇聚来实现合理边缘点数量的查找分布。这样的集中度依赖于本地的OpenFlow的接口，以维持流量封装压力。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>经过检验的应用软件The examined software</title>
      <link>http://nanshu.wang/post/2013-01-04</link>
      <pubDate>Fri, 04 Jan 2013 00:00:00 UTC</pubDate>
      <author>Nanshu Wang</author>
      <guid>http://nanshu.wang/post/2013-01-04</guid>
      <description>&lt;p&gt;在图书馆看关于苏格拉底的一篇文章《The examined life》，越看越觉得人生不值得过。不过对于应用软件来说，好像道理也是一样，套用苏格拉底的话来说就是 The unexaminedsoftware is not worth applicating.未经检验的软件是不值得应用的软件。按下win键发现还是有好些软件在电脑里的存活时间挺长了，也可以算是经过检验了，心血来潮数了数。（排名不分先后）&lt;/p&gt;

&lt;p&gt;1.Ginger
英文写作纠错工具，语法检查还行，比较好的是能够根据上下文给出用词的建议。&lt;/p&gt;

&lt;p&gt;2.ManicTime
可以统计电脑使用的时间及应用分类，比如这个月花了多少时间写paper，花了多少时间上网，花了多少时间打游戏。除了对application的统计，还会根据不同的file统计，也就是说不同文件以及不同域名的网站也会区别，可以看到多少时间刷豆瓣，多少时间逛淘宝，某篇论文写了多少时间。&lt;/p&gt;

&lt;p&gt;3.Bing词典
简单好用，单词本、例句搜索和近义词的功能都做得很好，缺点是例句不能直接链接到原文。（update：最新版2.1已经有这个功能了）&lt;/p&gt;

&lt;p&gt;4.OneNote
Office的笔记软件，分类十分方便，习惯office操作的话很容易上手，我觉着这个比evernote好用。OneNote有个强大的功能是可以用ORC技术直接把图片里的文字复制出来，支持中英文，对于有些复制不了的网站很是有用。缺点是同步不好使基本只能在一个电脑上用。&lt;/p&gt;

&lt;p&gt;5.MovieMaker
视频制作软件，看着其实这个软件我能用到的功能基本都有，而且速度很快不耗内存，导出的质量也蛮高，配合着PowerPoint生成的动画其实可以出很好的效果，做个小视频送人做生日礼物什么的效率很高。&lt;/p&gt;

&lt;p&gt;6.曦力音视频转换专家
听名字就知道是干嘛使的了，干净且好用，拿来扒视频的音频超快。&lt;/p&gt;

&lt;p&gt;7.腾讯TM2009
无广告无花里胡哨的功能，内存占用小，蓝色的图标很萌，这个软件是支撑我用QQ的动力。&lt;/p&gt;

&lt;p&gt;8.Everything
搜索文件的工具，Windows搜索巨巨巨巨巨慢，这个会快一点。&lt;/p&gt;

&lt;p&gt;9.SublimeText2
写代码的，其实我只用这个来写过HTML，弱爆了。&lt;/p&gt;

&lt;p&gt;差不多就这些，数完了好用的软件，又想起了些不得不吐槽的unexamined软件：
人人桌面（垃圾）、人人极速相册（一插U盘就怂恿传图是什么劲啊）、360安全卫士（不是360杀毒）、所有IE内核浏览器（IE6开什么网页的样式都一团糟）、SPSS（太慢了不好看）、绘声绘影（……）、暴风影音（……）、阿里旺旺（……）&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
